str->list     Convert XML to URL List.     From Biligrab.
Downloads Dailymotion videos by URL.
Downloads Sina videos by URL.
Format text with color or other effects into ANSI escaped string.
Print a log message to standard error.
Print an error log message.
What a Terrible Failure!
Detect operating system.
str->None
str->dict     Information for CKPlayer API content.
Splicing URLs according to video ID to get video details
str->list of str         Give you the real URLs.
Converts a string to a valid filename.
Downloads CBS videos by URL.
Override the original one         Ugly ugly dirty hack
str, str, str, bool, bool ->None      Download Acfun video by vid.      Call Acfun API, decide which site to use, and pass the job to its     extractor.
Scans through a string for substrings matched some patterns.      Args:         text: A string to be scanned.         patterns: a list of regex pattern.      Returns:         a list if matched. empty if not.
Parses the query string of a URL and returns the value of a parameter.      Args:         url: A URL.         param: A string representing the name of the parameter.      Returns:         The value of the parameter.
Gets the content of a URL via sending a HTTP GET request.      Args:         url: A URL.         headers: Request headers used by the client.         decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.      Returns:         The content as a string.
Post the content of a URL via sending a HTTP POST request.      Args:         url: A URL.         headers: Request headers used by the client.         decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.      Returns:         The content as a string.
Parses host name and port number from a string.
str->str
JSON, int, int, int->str          Get a proper title with courseid+topicID+partID.
int->None          Download a WHOLE course.     Reuse the API call to save time.
int, int, int->None          Download ONE PART of the course.
Checks if a task is either queued or running in this executor          :param task_instance: TaskInstance         :return: True if the task is known to this executor
Returns and flush the event buffer. In case dag_ids is specified         it will only return and flush events for the given dag_ids. Otherwise         it returns and flushes all          :param dag_ids: to dag_ids to return events for, if None returns all         :return: a dict of events
Returns a snowflake.connection object
returns aws_access_key_id, aws_secret_access_key         from extra          intended to be used by external import and export statements
Fetches a field from extras, and returns it. This is some Airflow         magic. The grpc hook type adds custom UI elements         to the hook page, which allow admins to specify scopes, credential pem files, etc.         They get formatted as shown below.
Executes SQL using psycopg2 copy_expert method.         Necessary to execute COPY command without access to a superuser.          Note: if this method is called with a "COPY FROM" statement and         the specified input file does not exist, it creates an empty         file and no data is loaded, but the operation succeeds.         So if users want to be aware when the input file does not exist,         they have to check its existence by themselves.
Dumps a database table into a tab-delimited file
Uploads the file to Google cloud storage
Gets the max partition for a table.      :param schema: The hive schema the table lives in     :type schema: str     :param table: The hive table you are interested in, supports the dot         notation as in "my_database.my_table", if a dot is found,         the schema param is disregarded     :type table: str     :param metastore_conn_id: The hive connection you are interested in.         If your default is set you don't need to use this parameter.     :type metastore_conn_id: str     :param filter_map: partition_key:partition_value map used for partition filtering,                        e.g. {'key1': 'value1', 'key2': 'value2'}.                        Only partitions matching all partition_key:partition_value                        pairs will be considered as candidates of max partition.     :type filter_map: map     :param field: the field to get the max value from. If there's only         one partition field, this will be inferred     :type field: str      >>> max_partition('airflow.static_babynames_partitioned')     '2015-01-01'
Returns a mysql connection object
Returns the state of a TaskInstance at the command line.     >>> airflow task_state tutorial sleep 2015-01-01     success
Runs forever, monitoring the child processes of @gunicorn_master_proc and     restarting workers occasionally.     Each iteration of the loop traverses one edge of this state transition     diagram, where each state (node) represents     [ num_ready_workers_running / num_workers_running ]. We expect most time to     be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.     The horizontal transition at ? happens after the new worker parses all the     dags (so it could take a while!)        V ────────────────────────────────────────────────────────────────────────┐     [n / n] ──TTIN──> [ [n, n+bs) / n + bs ]  ────?───> [n + bs / n + bs] ──TTOU─┘        ^                          ^───────────────┘        │        │      ┌────────────────v        └──────┴────── [ [0, n) / n ] <─── start     We change the number of workers by sending TTIN and TTOU to the gunicorn     master process, which increases and decreases the number of child workers     respectively. Gunicorn guarantees that on TTOU workers are terminated     gracefully and that the oldest worker is terminated.
Retrieves connection to Cloud Translate          :return: Google Cloud Translate client object.         :rtype: Client
Translate a string or list of strings.          See https://cloud.google.com/translate/docs/translating-text          :type values: str or list         :param values: String or list of strings to translate.          :type target_language: str         :param target_language: The language to translate results into. This                                 is required by the API and defaults to                                 the target language of the current instance.          :type format_: str         :param format_: (Optional) One of ``text`` or ``html``, to specify                         if the input text is plain text or HTML.          :type source_language: str or None         :param source_language: (Optional) The language of the text to                                 be translated.          :type model: str or None         :param model: (Optional) The model used to translate the text, such                       as ``'base'`` or ``'nmt'``.          :rtype: str or list         :returns: A list of dictionaries for each queried value. Each                   dictionary typically contains three keys (though not                   all will be present in all cases)                    * ``detectedSourceLanguage``: The detected language (as an                     ISO 639-1 language code) of the text.                   * ``translatedText``: The translation of the text into the                     target language.                   * ``input``: The corresponding input value.                   * ``model``: The model used to translate the text.                    If only a single value is passed, then only a single                   dictionary will be returned.         :raises: :class:`~exceptions.ValueError` if the number of                  values and translations differ.
Retrieves a resource containing information about a Cloud SQL instance.          :param instance: Database instance ID. This does not include the project ID.         :type instance: str         :param project_id: Project ID of the project that contains the instance. If set             to None or missing, the default project_id from the GCP connection is used.         :type project_id: str         :return: A Cloud SQL instance resource.         :rtype: dict
Creates a new Cloud SQL instance.          :param body: Body required by the Cloud SQL insert API, as described in             https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.         :type body: dict         :param project_id: Project ID of the project that contains the instance. If set             to None or missing, the default project_id from the GCP connection is used.         :type project_id: str         :return: None
Updates settings of a Cloud SQL instance.          Caution: This is not a partial update, so you must include values for         all the settings that you want to retain.          :param body: Body required by the Cloud SQL patch API, as described in             https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.         :type body: dict         :param instance: Cloud SQL instance ID. This does not include the project ID.         :type instance: str         :param project_id: Project ID of the project that contains the instance. If set             to None or missing, the default project_id from the GCP connection is used.         :type project_id: str         :return: None
Deletes a Cloud SQL instance.          :param project_id: Project ID of the project that contains the instance. If set             to None or missing, the default project_id from the GCP connection is used.         :type project_id: str         :param instance: Cloud SQL instance ID. This does not include the project ID.         :type instance: str         :return: None
Retrieves a database resource from a Cloud SQL instance.          :param instance: Database instance ID. This does not include the project ID.         :type instance: str         :param database: Name of the database in the instance.         :type database: str         :param project_id: Project ID of the project that contains the instance. If set             to None or missing, the default project_id from the GCP connection is used.         :type project_id: str         :return: A Cloud SQL database resource, as described in             https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.         :rtype: dict
Creates a new database inside a Cloud SQL instance.          :param instance: Database instance ID. This does not include the project ID.         :type instance: str         :param body: The request body, as described in             https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.         :type body: dict         :param project_id: Project ID of the project that contains the instance. If set             to None or missing, the default project_id from the GCP connection is used.         :type project_id: str         :return: None
Updates a database resource inside a Cloud SQL instance.          This method supports patch semantics.         See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.          :param instance: Database instance ID. This does not include the project ID.         :type instance: str         :param database: Name of the database to be updated in the instance.         :type database: str         :param body: The request body, as described in             https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.         :type body: dict         :param project_id: Project ID of the project that contains the instance. If set             to None or missing, the default project_id from the GCP connection is used.         :type project_id: str         :return: None
Deletes a database from a Cloud SQL instance.          :param instance: Database instance ID. This does not include the project ID.         :type instance: str         :param database: Name of the database to be deleted in the instance.         :type database: str         :param project_id: Project ID of the project that contains the instance. If set             to None or missing, the default project_id from the GCP connection is used.         :type project_id: str         :return: None
Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump         or CSV file.          :param instance: Database instance ID of the Cloud SQL instance. This does not include the             project ID.         :type instance: str         :param body: The request body, as described in             https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body         :type body: dict         :param project_id: Project ID of the project that contains the instance. If set             to None or missing, the default project_id from the GCP connection is used.         :type project_id: str         :return: None
Starts Cloud SQL Proxy.          You have to remember to stop the proxy if you started it!
Stops running proxy.          You should stop the proxy after you stop using it.
Returns version of the Cloud SQL Proxy.
Create connection in the Connection table, according to whether it uses         proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.          :param session: Session of the SQL Alchemy ORM (automatically generated with                         decorator).
Retrieves the dynamically created connection from the Connection table.          :param session: Session of the SQL Alchemy ORM (automatically generated with                         decorator).
Delete the dynamically created connection from the Connection table.          :param session: Session of the SQL Alchemy ORM (automatically generated with                         decorator).
Retrieve Cloud SQL Proxy runner. It is used to manage the proxy         lifecycle per task.          :return: The Cloud SQL Proxy runner.         :rtype: CloudSqlProxyRunner
Retrieve database hook. This is the actual Postgres or MySQL database hook         that uses proxy or connects directly to the Google Cloud SQL database.
Clean up database hook after it was used.
Reserve free TCP port to be used by Cloud SQL Proxy
Replaces invalid MLEngine job_id characters with '_'.      This also adds a leading 'z' in case job_id starts with an invalid     character.      Args:         job_id: A job_id str that may have invalid characters.      Returns:         A valid job_id representation.
Extract error code from ftp exception
Remove any existing DAG runs for the perf test DAGs.
Remove any existing task instances for the perf test DAGs.
Toggle the pause state of the DAGs in the test.
Print operational metrics for the scheduler test.
Override the scheduler heartbeat to determine when the test is complete
Invoke Lambda Function
Creates Operators needed for model evaluation and returns.      It gets prediction over inputs via Cloud ML Engine BatchPrediction API by     calling MLEngineBatchPredictionOperator, then summarize and validate     the result via Cloud Dataflow using DataFlowPythonOperator.      For details and pricing about Batch prediction, please refer to the website     https://cloud.google.com/ml-engine/docs/how-tos/batch-predict     and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/      It returns three chained operators for prediction, summary, and validation,     named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,     respectively.     (<prefix> should contain only alphanumeric characters or hyphen.)      The upstream and downstream can be set accordingly like:       pred, _, val = create_evaluate_ops(...)       pred.set_upstream(upstream_op)       ...       downstream_op.set_upstream(val)      Callers will provide two python callables, metric_fn and validate_fn, in     order to customize the evaluation behavior as they wish.     - metric_fn receives a dictionary per instance derived from json in the       batch prediction result. The keys might vary depending on the model.       It should return a tuple of metrics.     - validation_fn receives a dictionary of the averaged metrics that metric_fn       generated over all instances.       The key/value of the dictionary matches to what's given by       metric_fn_and_keys arg.       The dictionary contains an additional metric, 'count' to represent the       total number of instances received for evaluation.       The function would raise an exception to mark the task as failed, in a       case the validation result is not okay to proceed (i.e. to set the trained       version as default).      Typical examples are like this:      def get_metric_fn_and_keys():         import math  # imports should be outside of the metric_fn below.         def error_and_squared_error(inst):             label = float(inst['input_label'])             classes = float(inst['classes'])  # 0 or 1             err = abs(classes-label)             squared_err = math.pow(classes-label, 2)             return (err, squared_err)  # returns a tuple.         return error_and_squared_error, ['err', 'mse']  # key order must match.      def validate_err_and_count(summary):         if summary['err'] > 0.2:             raise ValueError('Too high err>0.2; summary=%s' % summary)         if summary['mse'] > 0.05:             raise ValueError('Too high mse>0.05; summary=%s' % summary)         if summary['count'] < 1000:             raise ValueError('Too few instances<1000; summary=%s' % summary)         return summary      For the details on the other BatchPrediction-related arguments (project_id,     job_id, region, data_format, input_paths, prediction_path, model_uri),     please refer to MLEngineBatchPredictionOperator too.      :param task_prefix: a prefix for the tasks. Only alphanumeric characters and         hyphen are allowed (no underscores), since this will be used as dataflow         job name, which doesn't allow other characters.     :type task_prefix: str      :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'     :type data_format: str      :param input_paths: a list of input paths to be sent to BatchPrediction.     :type input_paths: list[str]      :param prediction_path: GCS path to put the prediction results in.     :type prediction_path: str      :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:         - metric_fn is a function that accepts a dictionary (for an instance),           and returns a tuple of metric(s) that it calculates.         - metric_keys is a list of strings to denote the key of each metric.     :type metric_fn_and_keys: tuple of a function and a list[str]      :param validate_fn: a function to validate whether the averaged metric(s) is         good enough to push the model.     :type validate_fn: function      :param batch_prediction_job_id: the id to use for the Cloud ML Batch         prediction job. Passed directly to the MLEngineBatchPredictionOperator as         the job_id argument.     :type batch_prediction_job_id: str      :param project_id: the Google Cloud Platform project id in which to execute         Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s         `default_args['project_id']` will be used.     :type project_id: str      :param region: the Google Cloud Platform region in which to execute Cloud ML         Batch Prediction and Dataflow jobs. If None, then the `dag`'s         `default_args['region']` will be used.     :type region: str      :param dataflow_options: options to run Dataflow jobs. If None, then the         `dag`'s `default_args['dataflow_default_options']` will be used.     :type dataflow_options: dictionary      :param model_uri: GCS path of the model exported by Tensorflow using         tensorflow.estimator.export_savedmodel(). It cannot be used with         model_name or version_name below. See MLEngineBatchPredictionOperator for         more detail.     :type model_uri: str      :param model_name: Used to indicate a model to use for prediction. Can be         used in combination with version_name, but cannot be used together with         model_uri. See MLEngineBatchPredictionOperator for more detail. If None,         then the `dag`'s `default_args['model_name']` will be used.     :type model_name: str      :param version_name: Used to indicate a model version to use for prediction,         in combination with model_name. Cannot be used together with model_uri.         See MLEngineBatchPredictionOperator for more detail. If None, then the         `dag`'s `default_args['version_name']` will be used.     :type version_name: str      :param dag: The `DAG` to use for all Operators.     :type dag: airflow.models.DAG      :returns: a tuple of three operators, (prediction, summary, validation)     :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,                   PythonOperator)
Creates the directory specified by path, creating intermediate directories     as necessary. If directory already exists, this is a no-op.      :param path: The directory to create     :type path: str     :param mode: The mode to give to the directory e.g. 0o755, ignores umask     :type mode: int
A small helper function to convert a string to a numeric value     if appropriate      :param s: the string to be converted     :type s: str
Make a naive datetime.datetime in a given time zone aware.      :param value: datetime     :param timezone: timezone     :return: localized datetime in settings.TIMEZONE or timezone
Make an aware datetime.datetime naive in a given time zone.      :param value: datetime     :param timezone: timezone     :return: naive datetime
Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified      :return: datetime.datetime
Establish a connection to druid broker.
Returns http session for use with requests          :param headers: additional headers to be passed through as a dictionary         :type headers: dict
Performs the request          :param endpoint: the endpoint to be called i.e. resource/v1/query?         :type endpoint: str         :param data: payload to be uploaded or request parameters         :type data: dict         :param headers: additional headers to be passed through as a dictionary         :type headers: dict         :param extra_options: additional options to be used when executing the request             i.e. {'check_response': False} to avoid checking raising exceptions on non             2XX or 3XX status codes         :type extra_options: dict
Checks the status code and raise an AirflowException exception on non 2XX or 3XX         status codes          :param response: A requests response object         :type response: requests.response
Grabs extra options like timeout and actually runs the request,         checking for the result          :param session: the session to be used to execute the request         :type session: requests.Session         :param prepped_request: the prepared request generated in run()         :type prepped_request: session.prepare_request         :param extra_options: additional options to be used when executing the request             i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX             or 3XX status codes         :type extra_options: dict
Contextmanager that will create and teardown a session.
Function decorator that provides a session if it isn't provided.     If you want to reuse a session or run the function as part of a     database transaction, you pass it to the function, if not this wrapper     will create one and close it for you.
Clear out the database
Parses some DatabaseError to provide a better error message
Get a set of records from Presto
Get a pandas dataframe from a sql query.
Execute the statement against Presto. Can be used to create views.
A generic way to insert a set of tuples into a table.          :param table: Name of the target table         :type table: str         :param rows: The rows to insert into the table         :type rows: iterable of tuples         :param target_fields: The names of the columns to fill in the table         :type target_fields: iterable of strings
Return a cosmos db client.
Checks if a collection exists in CosmosDB.
Creates a new collection in the CosmosDB database.
Checks if a database exists in CosmosDB.
Creates a new database in CosmosDB.
Deletes an existing database in CosmosDB.
Deletes an existing collection in the CosmosDB database.
Insert a list of new documents into an existing collection in the CosmosDB database.
Delete an existing document out of a collection in the CosmosDB database.
Get a document from an existing collection in the CosmosDB database.
Get a list of documents from an existing collection in the CosmosDB database via SQL query.
Returns the Cloud Function with the given name.          :param name: Name of the function.         :type name: str         :return: A Cloud Functions object representing the function.         :rtype: dict
Creates a new function in Cloud Function in the location specified in the body.          :param location: The location of the function.         :type location: str         :param body: The body required by the Cloud Functions insert API.         :type body: dict         :param project_id: Optional, Google Cloud Project project_id where the function belongs.             If set to None or missing, the default project_id from the GCP connection is used.         :type project_id: str         :return: None
Updates Cloud Functions according to the specified update mask.          :param name: The name of the function.         :type name: str         :param body: The body required by the cloud function patch API.         :type body: dict         :param update_mask: The update mask - array of fields that should be patched.         :type update_mask: [str]         :return: None
Uploads zip file with sources.          :param location: The location where the function is created.         :type location: str         :param zip_path: The path of the valid .zip file to upload.         :type zip_path: str         :param project_id: Optional, Google Cloud Project project_id where the function belongs.             If set to None or missing, the default project_id from the GCP connection is used.         :type project_id: str         :return: The upload URL that was returned by generateUploadUrl method.
Deletes the specified Cloud Function.          :param name: The name of the function.         :type name: str         :return: None
Wrapper around the private _get_dep_statuses method that contains some global         checks for all dependencies.          :param ti: the task instance to get the dependency status for         :type ti: airflow.models.TaskInstance         :param session: database session         :type session: sqlalchemy.orm.session.Session         :param dep_context: the context for which this dependency should be evaluated for         :type dep_context: DepContext
Returns whether or not this dependency is met for a given task instance. A         dependency is considered met if all of the dependency statuses it reports are         passing.          :param ti: the task instance to see if this dependency is met for         :type ti: airflow.models.TaskInstance         :param session: database session         :type session: sqlalchemy.orm.session.Session         :param dep_context: The context this dependency is being checked under that stores             state that can be used by this dependency.         :type dep_context: BaseDepContext
Returns an iterable of strings that explain why this dependency wasn't met.          :param ti: the task instance to see if this dependency is met for         :type ti: airflow.models.TaskInstance         :param session: database session         :type session: sqlalchemy.orm.session.Session         :param dep_context: The context this dependency is being checked under that stores             state that can be used by this dependency.         :type dep_context: BaseDepContext
Parses a config file for s3 credentials. Can currently     parse boto, s3cmd.conf and AWS SDK config formats      :param config_file_name: path to the config file     :type config_file_name: str     :param config_format: config type. One of "boto", "s3cmd" or "aws".         Defaults to "boto"     :type config_format: str     :param profile: profile name in AWS type config file     :type profile: str
Get the underlying `botocore.Credentials` object.          This contains the following authentication attributes: access_key, secret_key and token.
Returns verticaql connection object
Ensure all logging output has been flushed
If the path contains a folder with a .zip suffix, then     the folder is treated as a zip archive and path to zip is returned.
Traverse a directory and look for Python files.      :param directory: the directory to traverse     :type directory: unicode     :param safe_mode: whether to use a heuristic to determine whether a file         contains Airflow DAG definitions     :return: a list of paths to Python files in the specified directory     :rtype: list[unicode]
Construct a TaskInstance from the database based on the primary key          :param session: DB session.         :param lock_for_update: if True, indicates that the database should             lock the TaskInstance (issuing a FOR UPDATE clause) until the             session is committed.
Launch DagFileProcessorManager processor and start DAG parsing loop in manager.
Send termination signal to DAG parsing processor manager         and expect it to terminate all DAG file processors.
Helper method to clean up DAG file processors to avoid leaving orphan processes.
Use multiple processes to parse and generate tasks for the         DAGs in parallel. By processing them in separate processes,         we can get parallelism and isolation from potentially harmful         user code.
Parse DAG files repeatedly in a standalone loop.
Parse DAG files in a loop controlled by DagParsingSignal.         Actual DAG parsing loop will run once upon receiving one         agent heartbeat message and will report done when finished the loop.
Refresh file paths from dag dir if we haven't done it for too long.
Occasionally print out stats about how fast the files are getting processed
Clears import errors for files that no longer exist.          :param session: session for ORM operations         :type session: sqlalchemy.orm.session.Session
Print out stats about how files are getting processed.          :param known_file_paths: a list of file paths that may contain Airflow             DAG definitions         :type known_file_paths: list[unicode]         :return: None
Update this with a new set of paths to DAG definition files.          :param new_file_paths: list of paths to DAG definition files         :type new_file_paths: list[unicode]         :return: None
Sleeps until all the processors are done.
This should be periodically called by the manager loop. This method will         kick off new processes to process DAG definition files and read the         results from the finished processors.          :return: a list of SimpleDags that were produced by processors that             have finished since the last time this was called         :rtype: list[airflow.utils.dag_processing.SimpleDag]
Kill all child processes on exit since we don't want to leave         them as orphaned.
Opens a ssh connection to the remote host.          :rtype: paramiko.client.SSHClient
Creates a transfer job that runs periodically.          :param body: (Required) A request body, as described in             https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body         :type body: dict         :return: transfer job.             See:             https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob         :rtype: dict
Gets the latest state of a long-running operation in Google Storage         Transfer Service.          :param job_name: (Required) Name of the job to be fetched         :type job_name: str         :param project_id: (Optional) the ID of the project that owns the Transfer             Job. If set to None or missing, the default project_id from the GCP             connection is used.         :type project_id: str         :return: Transfer Job         :rtype: dict
Lists long-running operations in Google Storage Transfer         Service that match the specified filter.          :param filter: (Required) A request filter, as described in             https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.filter         :type filter: dict         :return: List of Transfer Jobs         :rtype: list[dict]
Updates a transfer job that runs periodically.          :param job_name: (Required) Name of the job to be updated         :type job_name: str         :param body: A request body, as described in             https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body         :type body: dict         :return: If successful, TransferJob.         :rtype: dict
Deletes a transfer job. This is a soft delete. After a transfer job is         deleted, the job and all the transfer executions are subject to garbage         collection. Transfer jobs become eligible for garbage collection         30 days after soft delete.          :param job_name: (Required) Name of the job to be deleted         :type job_name: str         :param project_id: (Optional) the ID of the project that owns the Transfer             Job. If set to None or missing, the default project_id from the GCP             connection is used.         :type project_id: str         :rtype: None
Cancels an transfer operation in Google Storage Transfer Service.          :param operation_name: Name of the transfer operation.         :type operation_name: str         :rtype: None
Pauses an transfer operation in Google Storage Transfer Service.          :param operation_name: (Required) Name of the transfer operation.         :type operation_name: str         :rtype: None
Resumes an transfer operation in Google Storage Transfer Service.          :param operation_name: (Required) Name of the transfer operation.         :type operation_name: str         :rtype: None
Waits until the job reaches the expected state.          :param job: Transfer job             See:             https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob         :type job: dict         :param expected_statuses: State that is expected             See:             https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status         :type expected_statuses: set[str]         :param timeout:         :type timeout: time in which the operation must end in seconds         :rtype: None
Returns all task reschedules for the task instance and try number,         in ascending order.          :param task_instance: the task instance to find task reschedules for         :type task_instance: airflow.models.TaskInstance
Returns the number of slots open at the moment
Runs command and returns stdout
Remove an option if it exists in config from a file or         default config. If both of config have the same option, this removes         the option in both configs unless remove_default=False.
Returns the section as a dict. Values are converted to int, float, bool         as required.          :param section: section from the config         :rtype: dict
Allocate IDs for incomplete keys.          .. seealso::             https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds          :param partial_keys: a list of partial keys.         :type partial_keys: list         :return: a list of full keys.         :rtype: list
Begins a new transaction.          .. seealso::             https://cloud.google.com/datastore/docs/reference/rest/v1/projects/beginTransaction          :return: a transaction handle.         :rtype: str
Commit a transaction, optionally creating, deleting or modifying some entities.          .. seealso::             https://cloud.google.com/datastore/docs/reference/rest/v1/projects/commit          :param body: the body of the commit request.         :type body: dict         :return: the response body of the commit request.         :rtype: dict
Lookup some entities by key.          .. seealso::             https://cloud.google.com/datastore/docs/reference/rest/v1/projects/lookup          :param keys: the keys to lookup.         :type keys: list         :param read_consistency: the read consistency to use. default, strong or eventual.                                  Cannot be used with a transaction.         :type read_consistency: str         :param transaction: the transaction to use, if any.         :type transaction: str         :return: the response body of the lookup request.         :rtype: dict
Roll back a transaction.          .. seealso::             https://cloud.google.com/datastore/docs/reference/rest/v1/projects/rollback          :param transaction: the transaction to roll back.         :type transaction: str
Run a query for entities.          .. seealso::             https://cloud.google.com/datastore/docs/reference/rest/v1/projects/runQuery          :param body: the body of the query request.         :type body: dict         :return: the batch of query results.         :rtype: dict
Gets the latest state of a long-running operation.          .. seealso::             https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/get          :param name: the name of the operation resource.         :type name: str         :return: a resource operation instance.         :rtype: dict
Deletes the long-running operation.          .. seealso::             https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/delete          :param name: the name of the operation resource.         :type name: str         :return: none if successful.         :rtype: dict
Poll backup operation state until it's completed.          :param name: the name of the operation resource         :type name: str         :param polling_interval_in_seconds: The number of seconds to wait before calling another request.         :type polling_interval_in_seconds: int         :return: a resource operation instance.         :rtype: dict
Export entities from Cloud Datastore to Cloud Storage for backup.          .. note::             Keep in mind that this requests the Admin API not the Data API.          .. seealso::             https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/export          :param bucket: The name of the Cloud Storage bucket.         :type bucket: str         :param namespace: The Cloud Storage namespace path.         :type namespace: str         :param entity_filter: Description of what data from the project is included in the export.         :type entity_filter: dict         :param labels: Client-assigned labels.         :type labels: dict of str         :return: a resource operation instance.         :rtype: dict
Import a backup from Cloud Storage to Cloud Datastore.          .. note::             Keep in mind that this requests the Admin API not the Data API.          .. seealso::             https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/import          :param bucket: The name of the Cloud Storage bucket.         :type bucket: str         :param file: the metadata file written by the projects.export operation.         :type file: str         :param namespace: The Cloud Storage namespace path.         :type namespace: str         :param entity_filter: specify which kinds/namespaces are to be imported.         :type entity_filter: dict         :param labels: Client-assigned labels.         :type labels: dict of str         :return: a resource operation instance.         :rtype: dict
Publish a message to a topic or an endpoint.          :param target_arn: either a TopicArn or an EndpointArn         :type target_arn: str         :param message: the default message you want to send         :param message: str
Fetch the hostname using the callable from the config or using     `socket.getfqdn` as a fallback.
Retrieves connection to Cloud Natural Language service.          :return: Cloud Natural Language service object         :rtype: google.cloud.language_v1.LanguageServiceClient
Finds named entities in the text along with entity types,         salience, mentions for each entity, and other properties.          :param document: Input document.             If a dict is provided, it must be of the same form as the protobuf message Document         :type document: dict or class google.cloud.language_v1.types.Document         :param encoding_type: The encoding type used by the API to calculate offsets.         :type encoding_type: google.cloud.language_v1.types.EncodingType         :param retry: A retry object used to retry requests. If None is specified, requests will not be             retried.         :type retry: google.api_core.retry.Retry         :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if             retry is specified, the timeout applies to each individual attempt.         :type timeout: float         :param metadata: Additional metadata that is provided to the method.         :type metadata: sequence[tuple[str, str]]]         :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse
A convenience method that provides all the features that analyzeSentiment,         analyzeEntities, and analyzeSyntax provide in one call.          :param document: Input document.             If a dict is provided, it must be of the same form as the protobuf message Document         :type document: dict or google.cloud.language_v1.types.Document         :param features: The enabled features.             If a dict is provided, it must be of the same form as the protobuf message Features         :type features: dict or google.cloud.language_v1.enums.Features         :param encoding_type: The encoding type used by the API to calculate offsets.         :type encoding_type: google.cloud.language_v1.types.EncodingType         :param retry: A retry object used to retry requests. If None is specified, requests will not be             retried.         :type retry: google.api_core.retry.Retry         :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if             retry is specified, the timeout applies to each individual attempt.         :type timeout: float         :param metadata: Additional metadata that is provided to the method.         :type metadata: sequence[tuple[str, str]]]         :rtype: google.cloud.language_v1.types.AnnotateTextResponse
Classifies a document into categories.          :param document: Input document.             If a dict is provided, it must be of the same form as the protobuf message Document         :type document: dict or class google.cloud.language_v1.types.Document         :param retry: A retry object used to retry requests. If None is specified, requests will not be             retried.         :type retry: google.api_core.retry.Retry         :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if             retry is specified, the timeout applies to each individual attempt.         :type timeout: float         :param metadata: Additional metadata that is provided to the method.         :type metadata: sequence[tuple[str, str]]]         :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse
Gets template fields for specific operator class.      :param fullname: Full path to operator class.         For example: ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetCreateOperator``     :return: List of template field     :rtype: list[str]
A role that allows you to include a list of template fields in the middle of the text. This is especially     useful when writing guides describing how to use the operator.     The result is a list of fields where each field is shorted in the literal block.      Sample usage::      :template-fields:`airflow.contrib.operators.gcp_natural_language_operator.CloudLanguageAnalyzeSentimentOperator`      For further information look at:      * [http://docutils.sourceforge.net/docs/howto/rst-roles.html](Creating reStructuredText Interpreted       Text Roles)
Properly close pooled database connections
Ensures that certain subfolders of AIRFLOW_HOME are on the classpath
Gets the returned Celery result from the Airflow task         ID provided to the sensor, and returns True if the         celery result has been finished execution.          :param context: Airflow's execution context         :type context: dict         :return: True if task has been executed, otherwise False         :rtype: bool
Return true if the ticket cache contains "conf" information as is found     in ticket caches of Kerberos 1.8.1 or later. This is incompatible with the     Sun Java Krb5LoginModule in Java6, so we need to take an action to work     around it.
Transforms a SQLAlchemy model instance into a dictionary
Yield successive chunks of a given size from a list of items
Reduce the given list of items by splitting it into chunks     of the given size and passing each chunk through the reducer
Given a number of tasks, builds a dependency chain.      chain(task_1, task_2, task_3, task_4)      is equivalent to      task_1.set_downstream(task_2)     task_2.set_downstream(task_3)     task_3.set_downstream(task_4)
Returns a pretty ascii table from tuples      If namedtuple are used, the table will have headers
Given task instance, try_number, filename_template, return the rendered log     filename      :param ti: task instance     :param try_number: try_number of the task     :param filename_template: filename template, which can be jinja template or         python string template
Returns a Google Cloud Dataproc service object.
Awaits for Google Cloud Dataproc Operation to complete.
Coerces content or all values of content if it is a dict to a string. The     function will throw if content contains non-string or non-numeric types.      The reason why we have this function is because the ``self.json`` field must be a     dict with only string values. This is because ``render_template`` will fail     for numerical values.
Handles the Airflow + Databricks lifecycle logic for a Databricks operator      :param operator: Databricks operator being handled     :param context: Airflow context
Run an pig script using the pig cli          >>> ph = PigCliHook()         >>> result = ph.run_cli("ls /;")         >>> ("hdfs://" in result)         True
Fetch and return the state of the given celery task. The scope of this function is     global so that it can be called by subprocesses in the pool.      :param celery_task: a tuple of the Celery task key and the async Celery object used         to fetch the task's state     :type celery_task: tuple(str, celery.result.AsyncResult)     :return: a tuple of the Celery task key and the Celery state of the task     :rtype: tuple[str, str]
How many Celery tasks should each worker process send.          :return: Number of tasks that should be sent per process         :rtype: int
How many Celery tasks should be sent to each worker process.          :return: Number of tasks that should be used per process         :rtype: int
Like a Python builtin dict object, setdefault returns the current value         for a key, and if it isn't there, stores the default value and returns it.          :param key: Dict key for this Variable         :type key: str         :param default: Default value to set and return if the variable             isn't already in the DB         :type default: Mixed         :param deserialize_json: Store this as a JSON encoded value in the DB             and un-encode it when retrieving a value         :return: Mixed
Returns a Google MLEngine service object.
Launches a MLEngine job and wait for it to reach a terminal state.          :param project_id: The Google Cloud project id within which MLEngine             job will be launched.         :type project_id: str          :param job: MLEngine Job object that should be provided to the MLEngine             API, such as: ::                  {                   'jobId': 'my_job_id',                   'trainingInput': {                     'scaleTier': 'STANDARD_1',                     ...                   }                 }          :type job: dict          :param use_existing_job_fn: In case that a MLEngine job with the same             job_id already exist, this method (if provided) will decide whether             we should use this existing job, continue waiting for it to finish             and returning the job object. It should accepts a MLEngine job             object, and returns a boolean value indicating whether it is OK to             reuse the existing job. If 'use_existing_job_fn' is not provided,             we by default reuse the existing MLEngine job.         :type use_existing_job_fn: function          :return: The MLEngine job object if the job successfully reach a             terminal state (which might be FAILED or CANCELLED state).         :rtype: dict
Gets a MLEngine job based on the job name.          :return: MLEngine job object if succeed.         :rtype: dict          Raises:             googleapiclient.errors.HttpError: if HTTP error is returned from server
Waits for the Job to reach a terminal state.          This method will periodically check the job state until the job reach         a terminal state.          Raises:             googleapiclient.errors.HttpError: if HTTP error is returned when getting             the job
Creates the Version on Google Cloud ML Engine.          Returns the operation if the version was created successfully and         raises an error otherwise.
Sets a version to be the default. Blocks until finished.
Lists all available versions of a model. Blocks until finished.
Deletes the given version of a model. Blocks until finished.
Create a Model. Blocks until finished.
Gets a Model. Blocks until finished.
Write batch items to dynamodb table with provisioned throughout capacity.
Integrate plugins to the context.
Creates a new instance of the configured executor if none exists and returns it
Creates a new instance of the named executor.     In case the executor name is not know in airflow,     look for it in the plugins
Handles error callbacks when using Segment with segment_debug_mode set to True
Returns a mssql connection object
Trigger a new dag run for a Dag with an execution date of now unless     specified in the data.
Delete all DB records related to the specified Dag.
Returns a JSON with a task's public instance variables.
Get all pools.
Create a pool.
Delete pool.
Create a new container group          :param resource_group: the name of the resource group         :type resource_group: str         :param name: the name of the container group         :type name: str         :param container_group: the properties of the container group         :type container_group: azure.mgmt.containerinstance.models.ContainerGroup
Get the state and exitcode of a container group          :param resource_group: the name of the resource group         :type resource_group: str         :param name: the name of the container group         :type name: str         :return: A tuple with the state, exitcode, and details.             If the exitcode is unknown 0 is returned.         :rtype: tuple(state,exitcode,details)
Get the messages of a container group          :param resource_group: the name of the resource group         :type resource_group: str         :param name: the name of the container group         :type name: str         :return: A list of the event messages         :rtype: list[str]
Get the tail from logs of a container group          :param resource_group: the name of the resource group         :type resource_group: str         :param name: the name of the container group         :type name: str         :param tail: the size of the tail         :type tail: int         :return: A list of log messages         :rtype: list[str]
Delete a container group          :param resource_group: the name of the resource group         :type resource_group: str         :param name: the name of the container group         :type name: str
Test if a container group exists          :param resource_group: the name of the resource group         :type resource_group: str         :param name: the name of the container group         :type name: str
Function decorator that Looks for an argument named "default_args", and     fills the unspecified arguments from it.      Since python2.* isn't clear about which arguments are missing when     calling a function, and that this can be quite confusing with multi-level     inheritance and argument defaults, this decorator also alerts with     specific information about the missing arguments.
Builds an ingest query for an HDFS TSV load.          :param static_path: The path on hdfs where the data is         :type static_path: str         :param columns: List of all the columns that are available         :type columns: list
Check for message on subscribed channels and write to xcom the message with key ``message``          An example of message ``{'type': 'message', 'pattern': None, 'channel': b'test', 'data': b'hello'}``          :param context: the context object         :type context: dict         :return: ``True`` if message (with type 'message') is available or ``False`` if not
Returns a set of dag runs for the given search criteria.          :param dag_id: the dag_id to find dag runs for         :type dag_id: int, list         :param run_id: defines the the run id for this dag run         :type run_id: str         :param execution_date: the execution date         :type execution_date: datetime.datetime         :param state: the state of the dag run         :type state: airflow.utils.state.State         :param external_trigger: whether this dag run is externally triggered         :type external_trigger: bool         :param no_backfills: return no backfills (True), return all (False).             Defaults to False         :type no_backfills: bool         :param session: database session         :type session: sqlalchemy.orm.session.Session
Returns the task instances for this dag run
Returns the task instance specified by task_id for this dag run          :param task_id: the task id
The previous DagRun, if there is one
The previous, SCHEDULED DagRun, if there is one
Determines the overall state of the DagRun based on the state         of its TaskInstances.          :return: State
Verifies the DagRun by checking for removed tasks or tasks that are not in the         database yet. It will set state to removed or add the task if required.
We need to get the headers in addition to the body answer     to get the location from them     This function uses jenkins_request method from python-jenkins library     with just the return call changed      :param jenkins_server: The server to query     :param req: The request to execute     :return: Dict containing the response body (key body)         and the headers coming along (headers)
Given a context, this function provides a dictionary of values that can be used to     externally reconstruct relations between dags, dag_runs, tasks and task_instances.     Default to abc.def.ghi format and can be made to ABC_DEF_GHI format if     in_env_var_format is set to True.      :param context: The context for the task_instance of interest.     :type context: dict     :param in_env_var_format: If returned vars should be in ABC_DEF_GHI format.     :type in_env_var_format: bool     :return: task_instance context as dict.
This function decides whether or not to Trigger the remote DAG
Sends a single datapoint metric to DataDog          :param metric_name: The name of the metric         :type metric_name: str         :param datapoint: A single integer or float related to the metric         :type datapoint: int or float         :param tags: A list of tags associated with the metric         :type tags: list         :param type_: Type of your metric: gauge, rate, or count         :type type_: str         :param interval: If the type of the metric is rate or count, define the corresponding interval         :type interval: int
Queries datadog for a specific metric, potentially with some         function applied to it and returns the results.          :param query: The datadog query to execute (see datadog docs)         :type query: str         :param from_seconds_ago: How many seconds ago to start querying for.         :type from_seconds_ago: int         :param to_seconds_ago: Up to how many seconds ago to query for.         :type to_seconds_ago: int
Gets the DAG out of the dictionary, and refreshes it if expired
Fail given zombie tasks, which are tasks that haven't         had a heartbeat for too long, in the current DagBag.          :param zombies: zombie task instances to kill.         :type zombies: airflow.utils.dag_processing.SimpleTaskInstance         :param session: DB session.         :type session: sqlalchemy.orm.session.Session
Adds the DAG into the bag, recurses into sub dags.         Throws AirflowDagCycleException if a cycle is detected in this dag or its subdags
Given a file path or a folder, this method looks for python modules,         imports them and adds them to the dagbag collection.          Note that if a ``.airflowignore`` file is found while processing         the directory, it will behave much like a ``.gitignore``,         ignoring files that match any of the regex patterns specified         in the file.          **Note**: The patterns in .airflowignore are treated as         un-anchored regexes, not shell-like glob patterns.
Prints a report around DagBag loading stats
Add or subtract days from a YYYY-MM-DD      :param ds: anchor date in ``YYYY-MM-DD`` format to add to     :type ds: str     :param days: number of days to add to the ds, you can use negative values     :type days: int      >>> ds_add('2015-01-01', 5)     '2015-01-06'     >>> ds_add('2015-01-06', -5)     '2015-01-01'
Takes an input string and outputs another string     as specified in the output format      :param ds: input string which contains a date     :type ds: str     :param input_format: input string format. E.g. %Y-%m-%d     :type input_format: str     :param output_format: output string format  E.g. %Y-%m-%d     :type output_format: str      >>> ds_format('2015-01-01', "%Y-%m-%d", "%m-%d-%y")     '01-01-15'     >>> ds_format('1/5/2015', "%m/%d/%Y",  "%Y-%m-%d")     '2015-01-05'
poke matching files in a directory with self.regex          :return: Bool depending on the search criteria
poke for a non empty directory          :return: Bool depending on the search criteria
Clears a set of task instances, but makes sure the running ones     get killed.      :param tis: a list of task instances     :param session: current session     :param activate_dag_runs: flag to check for active dag run     :param dag: DAG object
Return the try number that this task number will be when it is actually         run.          If the TI is currently running, this will match the column in the         databse, in all othercases this will be incremenetd
Generates the shell command required to execute this task instance.          :param dag_id: DAG ID         :type dag_id: unicode         :param task_id: Task ID         :type task_id: unicode         :param execution_date: Execution date for the task         :type execution_date: datetime         :param mark_success: Whether to mark the task as successful         :type mark_success: bool         :param ignore_all_deps: Ignore all ignorable dependencies.             Overrides the other ignore_* parameters.         :type ignore_all_deps: bool         :param ignore_depends_on_past: Ignore depends_on_past parameter of DAGs             (e.g. for Backfills)         :type ignore_depends_on_past: bool         :param ignore_task_deps: Ignore task-specific dependencies such as depends_on_past             and trigger rule         :type ignore_task_deps: bool         :param ignore_ti_state: Ignore the task instance's previous failure/success         :type ignore_ti_state: bool         :param local: Whether to run the task locally         :type local: bool         :param pickle_id: If the DAG was serialized to the DB, the ID             associated with the pickled DAG         :type pickle_id: unicode         :param file_path: path to the file containing the DAG definition         :param raw: raw mode (needs more details)         :param job_id: job ID (needs more details)         :param pool: the Airflow pool that the task should run in         :type pool: unicode         :param cfg_path: the Path to the configuration file         :type cfg_path: basestring         :return: shell command that can be used to run the task instance
Get the very latest state from the database, if a session is passed,         we use and looking up the state becomes part of the session, otherwise         a new session is used.
Forces the task instance's state to FAILED in the database.
Refreshes the task instance from the database based on the primary key          :param lock_for_update: if True, indicates that the database should             lock the TaskInstance (issuing a FOR UPDATE clause) until the             session is committed.
Clears all XCom data from the database for the task instance
Returns a tuple that identifies the task instance uniquely
Checks whether the dependents of this task instance have all succeeded.         This is meant to be used by wait_for_downstream.          This is useful when you do not want to start processing the next         schedule of a task until the dependents are done. For instance,         if the task DROPs and recreates a table.
Get datetime of the next retry if the task instance fails. For exponential         backoff, retry_delay is used as base and will be converted to seconds.
Checks on whether the task instance is in the right state and timeframe         to be retried.
Returns a boolean as to whether the slot pool has room for this         task to run
Returns the DagRun for this TaskInstance          :param session:         :return: DagRun
Make an XCom available for tasks to pull.          :param key: A key for the XCom         :type key: str         :param value: A value for the XCom. The value is pickled and stored             in the database.         :type value: any pickleable object         :param execution_date: if provided, the XCom will not be visible until             this date. This can be used, for example, to send a message to a             task on a future date without it being immediately visible.         :type execution_date: datetime
Pull XComs that optionally meet certain criteria.          The default value for `key` limits the search to XComs         that were returned by other tasks (as opposed to those that were pushed         manually). To remove this filter, pass key=None (or any desired value).          If a single task_id string is provided, the result is the value of the         most recent matching XCom from that task_id. If multiple task_ids are         provided, a tuple of matching values is returned. None is returned         whenever no matches are found.          :param key: A key for the XCom. If provided, only XComs with matching             keys will be returned. The default key is 'return_value', also             available as a constant XCOM_RETURN_KEY. This key is automatically             given to XComs returned by tasks (as opposed to being pushed             manually). To remove the filter, pass key=None.         :type key: str         :param task_ids: Only XComs from tasks with matching ids will be             pulled. Can pass None to remove the filter.         :type task_ids: str or iterable of strings (representing task_ids)         :param dag_id: If provided, only pulls XComs from this DAG.             If None (default), the DAG of the calling task is used.         :type dag_id: str         :param include_prior_dates: If False, only XComs from the current             execution_date are returned. If True, XComs from previous dates             are returned as well.         :type include_prior_dates: bool
Sets the log context.
Close and upload local log file to remote storage Wasb.
Retrieves connection to Google Compute Engine.          :return: Google Compute Engine services object         :rtype: dict
Starts an existing instance defined by project_id, zone and resource_id.         Must be called with keyword arguments rather than positional.          :param zone: Google Cloud Platform zone where the instance exists         :type zone: str         :param resource_id: Name of the Compute Engine instance resource         :type resource_id: str         :param project_id: Optional, Google Cloud Platform project ID where the             Compute Engine Instance exists. If set to None or missing,             the default project_id from the GCP connection is used.         :type project_id: str         :return: None
Sets machine type of an instance defined by project_id, zone and resource_id.         Must be called with keyword arguments rather than positional.          :param zone: Google Cloud Platform zone where the instance exists.         :type zone: str         :param resource_id: Name of the Compute Engine instance resource         :type resource_id: str         :param body: Body required by the Compute Engine setMachineType API,             as described in             https://cloud.google.com/compute/docs/reference/rest/v1/instances/setMachineType         :type body: dict         :param project_id: Optional, Google Cloud Platform project ID where the             Compute Engine Instance exists. If set to None or missing,             the default project_id from the GCP connection is used.         :type project_id: str         :return: None
Retrieves instance template by project_id and resource_id.         Must be called with keyword arguments rather than positional.          :param resource_id: Name of the instance template         :type resource_id: str         :param project_id: Optional, Google Cloud Platform project ID where the             Compute Engine Instance exists. If set to None or missing,             the default project_id from the GCP connection is used.         :type project_id: str         :return: Instance template representation as object according to             https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates         :rtype: dict
Inserts instance template using body specified         Must be called with keyword arguments rather than positional.          :param body: Instance template representation as object according to             https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates         :type body: dict         :param request_id: Optional, unique request_id that you might add to achieve             full idempotence (for example when client call times out repeating the request             with the same request id will not create a new instance template again)             It should be in UUID format as defined in RFC 4122         :type request_id: str         :param project_id: Optional, Google Cloud Platform project ID where the             Compute Engine Instance exists. If set to None or missing,             the default project_id from the GCP connection is used.         :type project_id: str         :return: None
Retrieves Instance Group Manager by project_id, zone and resource_id.         Must be called with keyword arguments rather than positional.          :param zone: Google Cloud Platform zone where the Instance Group Manager exists         :type zone: str         :param resource_id: Name of the Instance Group Manager         :type resource_id: str         :param project_id: Optional, Google Cloud Platform project ID where the             Compute Engine Instance exists. If set to None or missing,             the default project_id from the GCP connection is used.         :type project_id: str         :return: Instance group manager representation as object according to             https://cloud.google.com/compute/docs/reference/rest/beta/instanceGroupManagers         :rtype: dict
Patches Instance Group Manager with the specified body.         Must be called with keyword arguments rather than positional.          :param zone: Google Cloud Platform zone where the Instance Group Manager exists         :type zone: str         :param resource_id: Name of the Instance Group Manager         :type resource_id: str         :param body: Instance Group Manager representation as json-merge-patch object             according to             https://cloud.google.com/compute/docs/reference/rest/beta/instanceTemplates/patch         :type body: dict         :param request_id: Optional, unique request_id that you might add to achieve             full idempotence (for example when client call times out repeating the request             with the same request id will not create a new instance template again).             It should be in UUID format as defined in RFC 4122         :type request_id: str         :param project_id: Optional, Google Cloud Platform project ID where the             Compute Engine Instance exists. If set to None or missing,             the default project_id from the GCP connection is used.         :type project_id: str         :return: None
Waits for the named operation to complete - checks status of the async call.          :param operation_name: name of the operation         :type operation_name: str         :param zone: optional region of the request (might be None for global operations)         :type zone: str         :return: None
Check if bucket_name exists.          :param bucket_name: the name of the bucket         :type bucket_name: str
Creates an Amazon S3 bucket.          :param bucket_name: The name of the bucket         :type bucket_name: str         :param region_name: The name of the aws region in which to create the bucket.         :type region_name: str
Checks that a prefix exists in a bucket          :param bucket_name: the name of the bucket         :type bucket_name: str         :param prefix: a key prefix         :type prefix: str         :param delimiter: the delimiter marks key hierarchy.         :type delimiter: str
Lists prefixes in a bucket under prefix          :param bucket_name: the name of the bucket         :type bucket_name: str         :param prefix: a key prefix         :type prefix: str         :param delimiter: the delimiter marks key hierarchy.         :type delimiter: str         :param page_size: pagination size         :type page_size: int         :param max_items: maximum items to return         :type max_items: int
Lists keys in a bucket under prefix and not containing delimiter          :param bucket_name: the name of the bucket         :type bucket_name: str         :param prefix: a key prefix         :type prefix: str         :param delimiter: the delimiter marks key hierarchy.         :type delimiter: str         :param page_size: pagination size         :type page_size: int         :param max_items: maximum items to return         :type max_items: int
Checks if a key exists in a bucket          :param key: S3 key that will point to the file         :type key: str         :param bucket_name: Name of the bucket in which the file is stored         :type bucket_name: str
Returns a boto3.s3.Object          :param key: the path to the key         :type key: str         :param bucket_name: the name of the bucket         :type bucket_name: str
Reads a key from S3          :param key: S3 key that will point to the file         :type key: str         :param bucket_name: Name of the bucket in which the file is stored         :type bucket_name: str
Reads a key with S3 Select.          :param key: S3 key that will point to the file         :type key: str         :param bucket_name: Name of the bucket in which the file is stored         :type bucket_name: str         :param expression: S3 Select expression         :type expression: str         :param expression_type: S3 Select expression type         :type expression_type: str         :param input_serialization: S3 Select input data serialization format         :type input_serialization: dict         :param output_serialization: S3 Select output data serialization format         :type output_serialization: dict         :return: retrieved subset of original data by S3 Select         :rtype: str          .. seealso::             For more details about S3 Select parameters:             http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.select_object_content
Checks that a key matching a wildcard expression exists in a bucket          :param wildcard_key: the path to the key         :type wildcard_key: str         :param bucket_name: the name of the bucket         :type bucket_name: str         :param delimiter: the delimiter marks key hierarchy         :type delimiter: str
Returns a boto3.s3.Object object matching the wildcard expression          :param wildcard_key: the path to the key         :type wildcard_key: str         :param bucket_name: the name of the bucket         :type bucket_name: str         :param delimiter: the delimiter marks key hierarchy         :type delimiter: str
Loads a local file to S3          :param filename: name of the file to load.         :type filename: str         :param key: S3 key that will point to the file         :type key: str         :param bucket_name: Name of the bucket in which to store the file         :type bucket_name: str         :param replace: A flag to decide whether or not to overwrite the key             if it already exists. If replace is False and the key exists, an             error will be raised.         :type replace: bool         :param encrypt: If True, the file will be encrypted on the server-side             by S3 and will be stored in an encrypted form while at rest in S3.         :type encrypt: bool
Loads a string to S3          This is provided as a convenience to drop a string in S3. It uses the         boto infrastructure to ship a file to s3.          :param string_data: str to set as content for the key.         :type string_data: str         :param key: S3 key that will point to the file         :type key: str         :param bucket_name: Name of the bucket in which to store the file         :type bucket_name: str         :param replace: A flag to decide whether or not to overwrite the key             if it already exists         :type replace: bool         :param encrypt: If True, the file will be encrypted on the server-side             by S3 and will be stored in an encrypted form while at rest in S3.         :type encrypt: bool
Loads bytes to S3          This is provided as a convenience to drop a string in S3. It uses the         boto infrastructure to ship a file to s3.          :param bytes_data: bytes to set as content for the key.         :type bytes_data: bytes         :param key: S3 key that will point to the file         :type key: str         :param bucket_name: Name of the bucket in which to store the file         :type bucket_name: str         :param replace: A flag to decide whether or not to overwrite the key             if it already exists         :type replace: bool         :param encrypt: If True, the file will be encrypted on the server-side             by S3 and will be stored in an encrypted form while at rest in S3.         :type encrypt: bool
Loads a file object to S3          :param file_obj: The file-like object to set as the content for the S3 key.         :type file_obj: file-like object         :param key: S3 key that will point to the file         :type key: str         :param bucket_name: Name of the bucket in which to store the file         :type bucket_name: str         :param replace: A flag that indicates whether to overwrite the key             if it already exists.         :type replace: bool         :param encrypt: If True, S3 encrypts the file on the server,             and the file is stored in encrypted form at rest in S3.         :type encrypt: bool
Creates a copy of an object that is already stored in S3.          Note: the S3 connection used here needs to have access to both         source and destination bucket/key.          :param source_bucket_key: The key of the source object.              It can be either full s3:// style url or relative path from root level.              When it's specified as a full s3:// url, please omit source_bucket_name.         :type source_bucket_key: str         :param dest_bucket_key: The key of the object to copy to.              The convention to specify `dest_bucket_key` is the same             as `source_bucket_key`.         :type dest_bucket_key: str         :param source_bucket_name: Name of the S3 bucket where the source object is in.              It should be omitted when `source_bucket_key` is provided as a full s3:// url.         :type source_bucket_name: str         :param dest_bucket_name: Name of the S3 bucket to where the object is copied.              It should be omitted when `dest_bucket_key` is provided as a full s3:// url.         :type dest_bucket_name: str         :param source_version_id: Version ID of the source object (OPTIONAL)         :type source_version_id: str
Queries cassandra and returns a cursor to the results.
Converts a user type to RECORD that contains n fields, where n is the         number of attributes. Each element in the user type class will be converted to its         corresponding data type in BQ.
Send an email with html content using sendgrid.      To use this plugin:     0. include sendgrid subpackage as part of your Airflow installation, e.g.,     pip install 'apache-airflow[sendgrid]'     1. update [email] backend in airflow.cfg, i.e.,     [email]     email_backend = airflow.contrib.utils.sendgrid.send_email     2. configure Sendgrid specific environment variables at all Airflow instances:     SENDGRID_MAIL_FROM={your-mail-from}     SENDGRID_API_KEY={your-sendgrid-api-key}.
Retrieves connection to Cloud Speech.          :return: Google Cloud Speech client object.         :rtype: google.cloud.speech_v1.SpeechClient
Recognizes audio input          :param config: information to the recognizer that specifies how to process the request.             https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionConfig         :type config: dict or google.cloud.speech_v1.types.RecognitionConfig         :param audio: audio data to be recognized             https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionAudio         :type audio: dict or google.cloud.speech_v1.types.RecognitionAudio         :param retry: (Optional) A retry object used to retry requests. If None is specified,             requests will not be retried.         :type retry: google.api_core.retry.Retry         :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.             Note that if retry is specified, the timeout applies to each individual attempt.         :type timeout: float
Call the SparkSqlHook to run the provided sql query
Load AirflowPlugin subclasses from the entrypoints     provided. The entry_point group should be 'airflow.plugins'.      :param entry_points: A collection of entrypoints to search for plugins     :type entry_points: Generator[setuptools.EntryPoint, None, None]     :param airflow_plugins: A collection of existing airflow plugins to         ensure we don't load duplicates     :type airflow_plugins: list[type[airflow.plugins_manager.AirflowPlugin]]     :rtype: list[airflow.plugins_manager.AirflowPlugin]
Check whether a potential object is a subclass of     the AirflowPlugin class.      :param plugin_obj: potential subclass of AirflowPlugin     :param existing_plugins: Existing list of AirflowPlugin subclasses     :return: Whether or not the obj is a valid subclass of         AirflowPlugin
Sets tasks instances to skipped from the same dag run.          :param dag_run: the DagRun for which to set the tasks to skipped         :param execution_date: execution_date         :param tasks: tasks to skip (not task_ids)         :param session: db session to use
Return a AzureDLFileSystem object.
Check if a file exists on Azure Data Lake.          :param file_path: Path and name of the file.         :type file_path: str         :return: True if the file exists, False otherwise.         :rtype: bool
Upload a file to Azure Data Lake.          :param local_path: local path. Can be single file, directory (in which case,             upload recursively) or glob pattern. Recursive glob patterns using `**`             are not supported.         :type local_path: str         :param remote_path: Remote path to upload to; if multiple files, this is the             directory root to write within.         :type remote_path: str         :param nthreads: Number of threads to use. If None, uses the number of cores.         :type nthreads: int         :param overwrite: Whether to forcibly overwrite existing files/directories.             If False and remote path is a directory, will quit regardless if any files             would be overwritten or not. If True, only matching filenames are actually             overwritten.         :type overwrite: bool         :param buffersize: int [2**22]             Number of bytes for internal buffer. This block cannot be bigger than             a chunk and cannot be smaller than a block.         :type buffersize: int         :param blocksize: int [2**22]             Number of bytes for a block. Within each chunk, we write a smaller             block for each API call. This block cannot be bigger than a chunk.         :type blocksize: int
List files in Azure Data Lake Storage          :param path: full path/globstring to use to list files in ADLS         :type path: str
Run Presto Query on Athena
Uncompress gz and bz2 files
Queries MSSQL and returns a cursor of results.          :return: mssql cursor
Decorates function to execute function at the same time submitting action_logging     but in CLI context. It will call action logger callbacks twice,     one for pre-execution and the other one for post-execution.      Action logger will be called with below keyword parameters:         sub_command : name of sub-command         start_datetime : start datetime instance by utc         end_datetime : end datetime instance by utc         full_command : full command line arguments         user : current user         log : airflow.models.log.Log ORM instance         dag_id : dag id (optional)         task_id : task_id (optional)         execution_date : execution date (optional)         error : exception instance if there's an exception      :param f: function instance     :return: wrapped function
Builds metrics dict from function args     It assumes that function arguments is from airflow.bin.cli module's function     and has Namespace instance where it optionally contains "dag_id", "task_id",     and "execution_date".      :param func_name: name of function     :param namespace: Namespace instance from argparse     :return: dict with metrics
Create the specified cgroup.          :param path: The path of the cgroup to create.         E.g. cpu/mygroup/mysubgroup         :return: the Node associated with the created cgroup.         :rtype: cgroupspy.nodes.Node
Delete the specified cgroup.          :param path: The path of the cgroup to delete.         E.g. cpu/mygroup/mysubgroup
The purpose of this function is to be robust to improper connections         settings provided by users, specifically in the host field.          For example -- when users supply ``https://xx.cloud.databricks.com`` as the         host, we must strip out the protocol to get the host.::              h = DatabricksHook()             assert h._parse_host('https://xx.cloud.databricks.com') == \                 'xx.cloud.databricks.com'          In the case where users supply the correct ``xx.cloud.databricks.com`` as the         host, this function is a no-op.::              assert h._parse_host('xx.cloud.databricks.com') == 'xx.cloud.databricks.com'
Utility function to perform an API call with retries          :param endpoint_info: Tuple of method and endpoint         :type endpoint_info: tuple[string, string]         :param json: Parameters for this API call.         :type json: dict         :return: If the api call returns a OK status code,             this function returns the response in JSON. Otherwise,             we throw an AirflowException.         :rtype: dict
Sign into Salesforce, only if we are not already signed in.
Make a query to Salesforce.          :param query: The query to make to Salesforce.         :type query: str         :return: The query result.         :rtype: dict
Get the description of an object from Salesforce.         This description is the object's schema and         some extra metadata that Salesforce stores for each object.          :param obj: The name of the Salesforce object that we are getting a description of.         :type obj: str         :return: the description of the Salesforce object.         :rtype: dict
Get a list of all available fields for an object.          :param obj: The name of the Salesforce object that we are getting a description of.         :type obj: str         :return: the names of the fields.         :rtype: list of str
Get all instances of the `object` from Salesforce.         For each model, only get the fields specified in fields.          All we really do underneath the hood is run:             SELECT <fields> FROM <obj>;          :param obj: The object name to get from Salesforce.         :type obj: str         :param fields: The fields to get from the object.         :type fields: iterable         :return: all instances of the object from Salesforce.         :rtype: dict
Convert a column of a dataframe to UNIX timestamps if applicable          :param column: A Series object representing a column of a dataframe.         :type column: pd.Series         :return: a new series that maintains the same index as the original         :rtype: pd.Series
Write query results to file.          Acceptable formats are:             - csv:                 comma-separated-values file. This is the default format.             - json:                 JSON array. Each element in the array is a different row.             - ndjson:                 JSON array but each element is new-line delimited instead of comma delimited like in `json`          This requires a significant amount of cleanup.         Pandas doesn't handle output to CSV and json in a uniform way.         This is especially painful for datetime types.         Pandas wants to write them as strings in CSV, but as millisecond Unix timestamps.          By default, this function will try and leave all values as they are represented in Salesforce.         You use the `coerce_to_timestamp` flag to force all datetimes to become Unix timestamps (UTC).         This is can be greatly beneficial as it will make all of your datetime fields look the same,         and makes it easier to work with in other database environments          :param query_results: the results from a SQL query         :type query_results: list of dict         :param filename: the name of the file where the data should be dumped to         :type filename: str         :param fmt: the format you want the output in. Default:  'csv'         :type fmt: str         :param coerce_to_timestamp: True if you want all datetime fields to be converted into Unix timestamps.             False if you want them to be left in the same format as they were in Salesforce.             Leaving the value as False will result in datetimes being strings. Default: False         :type coerce_to_timestamp: bool         :param record_time_added: True if you want to add a Unix timestamp field             to the resulting data that marks when the data was fetched from Salesforce. Default: False         :type record_time_added: bool         :return: the dataframe that gets written to the file.         :rtype: pd.Dataframe
Fetches PyMongo Client
Fetches a mongo collection object for querying.          Uses connection schema as DB unless specified.
Replaces many documents in a mongo collection.          Uses bulk_write with multiple ReplaceOne operations         https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.bulk_write          .. note::             If no ``filter_docs``are given, it is assumed that all             replacement documents contain the ``_id`` field which are then             used as filters.          :param mongo_collection: The name of the collection to update.         :type mongo_collection: str         :param docs: The new documents.         :type docs: list[dict]         :param filter_docs: A list of queries that match the documents to replace.             Can be omitted; then the _id fields from docs will be used.         :type filter_docs: list[dict]         :param mongo_db: The name of the database to use.             Can be omitted; then the database from the connection string is used.         :type mongo_db: str         :param upsert: If ``True``, perform an insert if no documents             match the filters for the replace operation.         :type upsert: bool         :param collation: An instance of             :class:`~pymongo.collation.Collation`. This option is only             supported on MongoDB 3.4 and above.         :type collation: pymongo.collation.Collation
Checks the mail folder for mails containing attachments with the given name.          :param name: The name of the attachment that will be searched for.         :type name: str         :param mail_folder: The mail folder where to look at.         :type mail_folder: str         :param check_regex: Checks the name for a regular expression.         :type check_regex: bool         :returns: True if there is an attachment with the given name and False if not.         :rtype: bool
Retrieves mail's attachments in the mail folder by its name.          :param name: The name of the attachment that will be downloaded.         :type name: str         :param mail_folder: The mail folder where to look at.         :type mail_folder: str         :param check_regex: Checks the name for a regular expression.         :type check_regex: bool         :param latest_only: If set to True it will only retrieve                             the first matched attachment.         :type latest_only: bool         :param not_found_mode: Specify what should happen if no attachment has been found.                                Supported values are 'raise', 'warn' and 'ignore'.                                If it is set to 'raise' it will raise an exception,                                if set to 'warn' it will only print a warning and                                if set to 'ignore' it won't notify you at all.         :type not_found_mode: str         :returns: a list of tuple each containing the attachment filename and its payload.         :rtype: a list of tuple
Downloads mail's attachments in the mail folder by its name to the local directory.          :param name: The name of the attachment that will be downloaded.         :type name: str         :param local_output_directory: The output directory on the local machine                                        where the files will be downloaded to.         :type local_output_directory: str         :param mail_folder: The mail folder where to look at.         :type mail_folder: str         :param check_regex: Checks the name for a regular expression.         :type check_regex: bool         :param latest_only: If set to True it will only download                             the first matched attachment.         :type latest_only: bool         :param not_found_mode: Specify what should happen if no attachment has been found.                                Supported values are 'raise', 'warn' and 'ignore'.                                If it is set to 'raise' it will raise an exception,                                if set to 'warn' it will only print a warning and                                if set to 'ignore' it won't notify you at all.         :type not_found_mode: str
Gets all attachments by name for the mail.          :param name: The name of the attachment to look for.         :type name: str         :param check_regex: Checks the name for a regular expression.         :type check_regex: bool         :param find_first: If set to True it will only find the first match and then quit.         :type find_first: bool         :returns: a list of tuples each containing name and payload                   where the attachments name matches the given name.         :rtype: list of tuple
Gets the file including name and payload.          :returns: the part's name and payload.         :rtype: tuple
Write batch records to Kinesis Firehose
Determines whether a task is ready to be rescheduled. Only tasks in         NONE state with at least one row in task_reschedule table are         handled by this dependency class, otherwise this dependency is         considered as passed. This dependency fails if the latest reschedule         request's reschedule date is still in future.
Send email using backend specified in EMAIL_BACKEND.
Send an email with html content      >>> send_email('test@example.com', 'foo', '<b>Foo</b> bar', ['/dev/null'], dryrun=True)
Processes DateTimes from the DB making sure it is always         returning UTC. Not using timezone.convert_to_utc as that         converts to configured TIMEZONE while the DB might be         running with some other setting. We assume UTC datetimes         in the database.
Check if a blob exists on Azure Blob Storage.          :param container_name: Name of the container.         :type container_name: str         :param blob_name: Name of the blob.         :type blob_name: str         :param kwargs: Optional keyword arguments that             `BlockBlobService.exists()` takes.         :type kwargs: object         :return: True if the blob exists, False otherwise.         :rtype: bool
Check if a prefix exists on Azure Blob storage.          :param container_name: Name of the container.         :type container_name: str         :param prefix: Prefix of the blob.         :type prefix: str         :param kwargs: Optional keyword arguments that             `BlockBlobService.list_blobs()` takes.         :type kwargs: object         :return: True if blobs matching the prefix exist, False otherwise.         :rtype: bool
Upload a string to Azure Blob Storage.          :param string_data: String to load.         :type string_data: str         :param container_name: Name of the container.         :type container_name: str         :param blob_name: Name of the blob.         :type blob_name: str         :param kwargs: Optional keyword arguments that             `BlockBlobService.create_blob_from_text()` takes.         :type kwargs: object
Read a file from Azure Blob Storage and return as a string.          :param container_name: Name of the container.         :type container_name: str         :param blob_name: Name of the blob.         :type blob_name: str         :param kwargs: Optional keyword arguments that             `BlockBlobService.create_blob_from_path()` takes.         :type kwargs: object
Delete a file from Azure Blob Storage.          :param container_name: Name of the container.         :type container_name: str         :param blob_name: Name of the blob.         :type blob_name: str         :param is_prefix: If blob_name is a prefix, delete all matching files         :type is_prefix: bool         :param ignore_if_missing: if True, then return success even if the             blob does not exist.         :type ignore_if_missing: bool         :param kwargs: Optional keyword arguments that             `BlockBlobService.create_blob_from_path()` takes.         :type kwargs: object
BACKPORT FROM PYTHON3 FTPLIB.      List a directory in a standardized format by using MLSD     command (RFC-3659). If path is omitted the current directory     is assumed. "facts" is a list of strings representing the type     of information desired (e.g. ["type", "size", "perm"]).      Return a generator object yielding a tuple of two elements     for every file found in path.     First element is the file name, the second one is a dictionary     including a variable number of "facts" depending on the server     and whether "facts" argument has been provided.
Returns a FTP connection object
Returns a list of files on the remote system.          :param path: full path to the remote directory to list         :type path: str
Transfers the remote file to a local location.          If local_full_path_or_buffer is a string path, the file will be put         at that location; if it is a file-like buffer, the file will         be written to the buffer but not closed.          :param remote_full_path: full path to the remote file         :type remote_full_path: str         :param local_full_path_or_buffer: full path to the local file or a             file-like buffer         :type local_full_path_or_buffer: str or file-like buffer         :param callback: callback which is called each time a block of data             is read. if you do not use a callback, these blocks will be written             to the file or buffer passed in. if you do pass in a callback, note             that writing to a file or buffer will need to be handled inside the             callback.             [default: output_handle.write()]         :type callback: callable          :Example::              hook = FTPHook(ftp_conn_id='my_conn')              remote_path = '/path/to/remote/file'             local_path = '/path/to/local/file'              # with a custom callback (in this case displaying progress on each read)             def print_progress(percent_progress):                 self.log.info('Percent Downloaded: %s%%' % percent_progress)              total_downloaded = 0             total_file_size = hook.get_size(remote_path)             output_handle = open(local_path, 'wb')             def write_to_file_with_progress(data):                 total_downloaded += len(data)                 output_handle.write(data)                 percent_progress = (total_downloaded / total_file_size) * 100                 print_progress(percent_progress)             hook.retrieve_file(remote_path, None, callback=write_to_file_with_progress)              # without a custom callback data is written to the local_path             hook.retrieve_file(remote_path, local_path)
Transfers a local file to the remote location.          If local_full_path_or_buffer is a string path, the file will be read         from that location; if it is a file-like buffer, the file will         be read from the buffer but not closed.          :param remote_full_path: full path to the remote file         :type remote_full_path: str         :param local_full_path_or_buffer: full path to the local file or a             file-like buffer         :type local_full_path_or_buffer: str or file-like buffer
Returns a datetime object representing the last time the file was modified          :param path: remote file path         :type path: string
Call the DiscordWebhookHook to post message
Return the FileService object.
Check if a directory exists on Azure File Share.          :param share_name: Name of the share.         :type share_name: str         :param directory_name: Name of the directory.         :type directory_name: str         :param kwargs: Optional keyword arguments that             `FileService.exists()` takes.         :type kwargs: object         :return: True if the file exists, False otherwise.         :rtype: bool
Check if a file exists on Azure File Share.          :param share_name: Name of the share.         :type share_name: str         :param directory_name: Name of the directory.         :type directory_name: str         :param file_name: Name of the file.         :type file_name: str         :param kwargs: Optional keyword arguments that             `FileService.exists()` takes.         :type kwargs: object         :return: True if the file exists, False otherwise.         :rtype: bool
Return the list of directories and files stored on a Azure File Share.          :param share_name: Name of the share.         :type share_name: str         :param directory_name: Name of the directory.         :type directory_name: str         :param kwargs: Optional keyword arguments that             `FileService.list_directories_and_files()` takes.         :type kwargs: object         :return: A list of files and directories         :rtype: list
Create a new directory on a Azure File Share.          :param share_name: Name of the share.         :type share_name: str         :param directory_name: Name of the directory.         :type directory_name: str         :param kwargs: Optional keyword arguments that             `FileService.create_directory()` takes.         :type kwargs: object         :return: A list of files and directories         :rtype: list
Upload a file to Azure File Share.          :param file_path: Path to the file to load.         :type file_path: str         :param share_name: Name of the share.         :type share_name: str         :param directory_name: Name of the directory.         :type directory_name: str         :param file_name: Name of the file.         :type file_name: str         :param kwargs: Optional keyword arguments that             `FileService.create_file_from_path()` takes.         :type kwargs: object
Upload a string to Azure File Share.          :param string_data: String to load.         :type string_data: str         :param share_name: Name of the share.         :type share_name: str         :param directory_name: Name of the directory.         :type directory_name: str         :param file_name: Name of the file.         :type file_name: str         :param kwargs: Optional keyword arguments that             `FileService.create_file_from_text()` takes.         :type kwargs: object
Upload a stream to Azure File Share.          :param stream: Opened file/stream to upload as the file content.         :type stream: file-like         :param share_name: Name of the share.         :type share_name: str         :param directory_name: Name of the directory.         :type directory_name: str         :param file_name: Name of the file.         :type file_name: str         :param count: Size of the stream in bytes         :type count: int         :param kwargs: Optional keyword arguments that             `FileService.create_file_from_stream()` takes.         :type kwargs: object
Returns a Google Cloud Storage service object.
Copies an object from a bucket to another, with renaming if requested.          destination_bucket or destination_object can be omitted, in which case         source bucket/object is used, but not both.          :param source_bucket: The bucket of the object to copy from.         :type source_bucket: str         :param source_object: The object to copy.         :type source_object: str         :param destination_bucket: The destination of the object to copied to.             Can be omitted; then the same bucket is used.         :type destination_bucket: str         :param destination_object: The (renamed) path of the object if given.             Can be omitted; then the same name is used.         :type destination_object: str
Get a file from Google Cloud Storage.          :param bucket_name: The bucket to fetch from.         :type bucket_name: str         :param object_name: The object to fetch.         :type object_name: str         :param filename: If set, a local file path where the file should be written to.         :type filename: str
Uploads a local file to Google Cloud Storage.          :param bucket_name: The bucket to upload to.         :type bucket_name: str         :param object_name: The object name to set when uploading the local file.         :type object_name: str         :param filename: The local file path to the file to be uploaded.         :type filename: str         :param mime_type: The MIME type to set when uploading the file.         :type mime_type: str         :param gzip: Option to compress file for upload         :type gzip: bool
Checks for the existence of a file in Google Cloud Storage.          :param bucket_name: The Google cloud storage bucket where the object is.         :type bucket_name: str         :param object_name: The name of the blob_name to check in the Google cloud             storage bucket.         :type object_name: str
Checks if an blob_name is updated in Google Cloud Storage.          :param bucket_name: The Google cloud storage bucket where the object is.         :type bucket_name: str         :param object_name: The name of the object to check in the Google cloud             storage bucket.         :type object_name: str         :param ts: The timestamp to check against.         :type ts: datetime.datetime
Deletes an object from the bucket.          :param bucket_name: name of the bucket, where the object resides         :type bucket_name: str         :param object_name: name of the object to delete         :type object_name: str
List all objects from the bucket with the give string prefix in name          :param bucket_name: bucket name         :type bucket_name: str         :param versions: if true, list all versions of the objects         :type versions: bool         :param max_results: max count of items to return in a single page of responses         :type max_results: int         :param prefix: prefix string which filters objects whose name begin with             this prefix         :type prefix: str         :param delimiter: filters objects based on the delimiter (for e.g '.csv')         :type delimiter: str         :return: a stream of object names matching the filtering criteria
Gets the size of a file in Google Cloud Storage.          :param bucket_name: The Google cloud storage bucket where the blob_name is.         :type bucket_name: str         :param object_name: The name of the object to check in the Google             cloud storage bucket_name.         :type object_name: str
Gets the CRC32c checksum of an object in Google Cloud Storage.          :param bucket_name: The Google cloud storage bucket where the blob_name is.         :type bucket_name: str         :param object_name: The name of the object to check in the Google cloud             storage bucket_name.         :type object_name: str
Gets the MD5 hash of an object in Google Cloud Storage.          :param bucket_name: The Google cloud storage bucket where the blob_name is.         :type bucket_name: str         :param object_name: The name of the object to check in the Google cloud             storage bucket_name.         :type object_name: str
Creates a new bucket. Google Cloud Storage uses a flat namespace, so         you can't create a bucket with a name that is already in use.          .. seealso::             For more information, see Bucket Naming Guidelines:             https://cloud.google.com/storage/docs/bucketnaming.html#requirements          :param bucket_name: The name of the bucket.         :type bucket_name: str         :param resource: An optional dict with parameters for creating the bucket.             For information on available parameters, see Cloud Storage API doc:             https://cloud.google.com/storage/docs/json_api/v1/buckets/insert         :type resource: dict         :param storage_class: This defines how objects in the bucket are stored             and determines the SLA and the cost of storage. Values include              - ``MULTI_REGIONAL``             - ``REGIONAL``             - ``STANDARD``             - ``NEARLINE``             - ``COLDLINE``.              If this value is not specified when the bucket is             created, it will default to STANDARD.         :type storage_class: str         :param location: The location of the bucket.             Object data for objects in the bucket resides in physical storage             within this region. Defaults to US.              .. seealso::                 https://developers.google.com/storage/docs/bucket-locations          :type location: str         :param project_id: The ID of the GCP Project.         :type project_id: str         :param labels: User-provided labels, in key/value pairs.         :type labels: dict         :return: If successful, it returns the ``id`` of the bucket.
Composes a list of existing object into a new object in the same storage bucket_name          Currently it only supports up to 32 objects that can be concatenated         in a single operation          https://cloud.google.com/storage/docs/json_api/v1/objects/compose          :param bucket_name: The name of the bucket containing the source objects.             This is also the same bucket to store the composed destination object.         :type bucket_name: str         :param source_objects: The list of source objects that will be composed             into a single object.         :type source_objects: list         :param destination_object: The path of the object if given.         :type destination_object: str
Returns true if training job's secondary status message has changed.      :param current_job_description: Current job description, returned from DescribeTrainingJob call.     :type current_job_description: dict     :param prev_job_description: Previous job description, returned from DescribeTrainingJob call.     :type prev_job_description: dict      :return: Whether the secondary status message of a training job changed or not.
Returns a string contains start time and the secondary training job status message.      :param job_description: Returned response from DescribeTrainingJob call     :type job_description: dict     :param prev_description: Previous job description from DescribeTrainingJob call     :type prev_description: dict      :return: Job status string to be printed.
Tar the local file or directory and upload to s3          :param path: local file or directory         :type path: str         :param key: s3 key         :type key: str         :param bucket: s3 bucket         :type bucket: str         :return: None
Extract the S3 operations from the configuration and execute them.          :param config: config of SageMaker operation         :type config: dict         :rtype: dict
Check if an S3 URL exists          :param s3url: S3 url         :type s3url: str         :rtype: bool
Establish an AWS connection for retrieving logs during training          :rtype: CloudWatchLogs.Client
Create a training job          :param config: the config for training         :type config: dict         :param wait_for_completion: if the program should keep running until job finishes         :type wait_for_completion: bool         :param check_interval: the time interval in seconds which the operator             will check the status of any SageMaker job         :type check_interval: int         :param max_ingestion_time: the maximum ingestion time in seconds. Any             SageMaker jobs that run longer than this will fail. Setting this to             None implies no timeout for any SageMaker job.         :type max_ingestion_time: int         :return: A response to training job creation
Create a tuning job          :param config: the config for tuning         :type config: dict         :param wait_for_completion: if the program should keep running until job finishes         :type wait_for_completion: bool         :param check_interval: the time interval in seconds which the operator             will check the status of any SageMaker job         :type check_interval: int         :param max_ingestion_time: the maximum ingestion time in seconds. Any             SageMaker jobs that run longer than this will fail. Setting this to             None implies no timeout for any SageMaker job.         :type max_ingestion_time: int         :return: A response to tuning job creation
Create a transform job          :param config: the config for transform job         :type config: dict         :param wait_for_completion: if the program should keep running until job finishes         :type wait_for_completion: bool         :param check_interval: the time interval in seconds which the operator             will check the status of any SageMaker job         :type check_interval: int         :param max_ingestion_time: the maximum ingestion time in seconds. Any             SageMaker jobs that run longer than this will fail. Setting this to             None implies no timeout for any SageMaker job.         :type max_ingestion_time: int         :return: A response to transform job creation
Create an endpoint          :param config: the config for endpoint         :type config: dict         :param wait_for_completion: if the program should keep running until job finishes         :type wait_for_completion: bool         :param check_interval: the time interval in seconds which the operator             will check the status of any SageMaker job         :type check_interval: int         :param max_ingestion_time: the maximum ingestion time in seconds. Any             SageMaker jobs that run longer than this will fail. Setting this to             None implies no timeout for any SageMaker job.         :type max_ingestion_time: int         :return: A response to endpoint creation
Return the training job info associated with job_name and print CloudWatch logs
Check status of a SageMaker job          :param job_name: name of the job to check status         :type job_name: str         :param key: the key of the response dict             that points to the state         :type key: str         :param describe_function: the function used to retrieve the status         :type describe_function: python callable         :param args: the arguments for the function         :param check_interval: the time interval in seconds which the operator             will check the status of any SageMaker job         :type check_interval: int         :param max_ingestion_time: the maximum ingestion time in seconds. Any             SageMaker jobs that run longer than this will fail. Setting this to             None implies no timeout for any SageMaker job.         :type max_ingestion_time: int         :param non_terminal_states: the set of nonterminal states         :type non_terminal_states: set         :return: response of describe call after job is done
Display the logs for a given training job, optionally tailing them until the         job is complete.          :param job_name: name of the training job to check status and display logs for         :type job_name: str         :param non_terminal_states: the set of non_terminal states         :type non_terminal_states: set         :param failed_states: the set of failed states         :type failed_states: set         :param wait_for_completion: Whether to keep looking for new log entries             until the job completes         :type wait_for_completion: bool         :param check_interval: The interval in seconds between polling for new log entries and job completion         :type check_interval: int         :param max_ingestion_time: the maximum ingestion time in seconds. Any             SageMaker jobs that run longer than this will fail. Setting this to             None implies no timeout for any SageMaker job.         :type max_ingestion_time: int         :return: None
Execute the python dataflow job.
Run migrations in 'offline' mode.      This configures the context with just a URL     and not an Engine, though an Engine is acceptable     here as well.  By skipping the Engine creation     we don't even need a DBAPI to be available.      Calls to context.execute() here emit the given string to the     script output.
Run migrations in 'online' mode.      In this scenario we need to create an Engine     and associate a connection with the context.
Deletes the specified Cloud Bigtable instance.         Raises google.api_core.exceptions.NotFound if the Cloud Bigtable instance does         not exist.          :param project_id: Optional, Google Cloud Platform project ID where the             BigTable exists. If set to None or missing,             the default project_id from the GCP connection is used.         :type project_id: str         :param instance_id: The ID of the Cloud Bigtable instance.         :type instance_id: str
Creates new instance.          :type instance_id: str         :param instance_id: The ID for the new instance.         :type main_cluster_id: str         :param main_cluster_id: The ID for main cluster for the new instance.         :type main_cluster_zone: str         :param main_cluster_zone: The zone for main cluster.             See https://cloud.google.com/bigtable/docs/locations for more details.         :type project_id: str         :param project_id: Optional, Google Cloud Platform project ID where the             BigTable exists. If set to None or missing,             the default project_id from the GCP connection is used.         :type replica_cluster_id: str         :param replica_cluster_id: (optional) The ID for replica cluster for the new             instance.         :type replica_cluster_zone: str         :param replica_cluster_zone: (optional)  The zone for replica cluster.         :type instance_type: enums.Instance.Type         :param instance_type: (optional) The type of the instance.         :type instance_display_name: str         :param instance_display_name: (optional) Human-readable name of the instance.                 Defaults to ``instance_id``.         :type instance_labels: dict         :param instance_labels: (optional) Dictionary of labels to associate with the             instance.         :type cluster_nodes: int         :param cluster_nodes: (optional) Number of nodes for cluster.         :type cluster_storage_type: enums.StorageType         :param cluster_storage_type: (optional) The type of storage.         :type timeout: int         :param timeout: (optional) timeout (in seconds) for instance creation.                         If None is not specified, Operator will wait indefinitely.
Creates the specified Cloud Bigtable table.         Raises ``google.api_core.exceptions.AlreadyExists`` if the table exists.          :type instance: Instance         :param instance: The Cloud Bigtable instance that owns the table.         :type table_id: str         :param table_id: The ID of the table to create in Cloud Bigtable.         :type initial_split_keys: list         :param initial_split_keys: (Optional) A list of row keys in bytes to use to             initially split the table.         :type column_families: dict         :param column_families: (Optional) A map of columns to create. The key is the             column_id str, and the value is a             :class:`google.cloud.bigtable.column_family.GarbageCollectionRule`.
Deletes the specified table in Cloud Bigtable.         Raises google.api_core.exceptions.NotFound if the table does not exist.          :type instance_id: str         :param instance_id: The ID of the Cloud Bigtable instance.         :type table_id: str         :param table_id: The ID of the table in Cloud Bigtable.         :type project_id: str         :param project_id: Optional, Google Cloud Platform project ID where the             BigTable exists. If set to None or missing,             the default project_id from the GCP connection is used.
Updates number of nodes in the specified Cloud Bigtable cluster.         Raises google.api_core.exceptions.NotFound if the cluster does not exist.          :type instance: Instance         :param instance: The Cloud Bigtable instance that owns the cluster.         :type cluster_id: str         :param cluster_id: The ID of the cluster.         :type nodes: int         :param nodes: The desired number of nodes.
This function creates the command list from available information
This function prepares a list of hiveconf params         from a dictionary of key value pairs.          :param d:         :type d: dict          >>> hh = HiveCliHook()         >>> hive_conf = {"hive.exec.dynamic.partition": "true",         ... "hive.exec.dynamic.partition.mode": "nonstrict"}         >>> hh._prepare_hiveconf(hive_conf)         ["-hiveconf", "hive.exec.dynamic.partition=true",\  "-hiveconf", "hive.exec.dynamic.partition.mode=nonstrict"]
Loads a pandas DataFrame into hive.          Hive data types will be inferred if not passed but column names will         not be sanitized.          :param df: DataFrame to load into a Hive table         :type df: pandas.DataFrame         :param table: target Hive table, use dot notation to target a             specific database         :type table: str         :param field_dict: mapping from column name to hive data type.             Note that it must be OrderedDict so as to keep columns' order.         :type field_dict: collections.OrderedDict         :param delimiter: field delimiter in the file         :type delimiter: str         :param encoding: str encoding to use when writing DataFrame to file         :type encoding: str         :param pandas_kwargs: passed to DataFrame.to_csv         :type pandas_kwargs: dict         :param kwargs: passed to self.load_file
Loads a local file into Hive          Note that the table generated in Hive uses ``STORED AS textfile``         which isn't the most efficient serialization format. If a         large amount of data is loaded and/or if the tables gets         queried considerably, you may want to use this operator only to         stage the data into a temporary table before loading it into its         final destination using a ``HiveOperator``.          :param filepath: local filepath of the file to load         :type filepath: str         :param table: target Hive table, use dot notation to target a             specific database         :type table: str         :param delimiter: field delimiter in the file         :type delimiter: str         :param field_dict: A dictionary of the fields name in the file             as keys and their Hive types as values.             Note that it must be OrderedDict so as to keep columns' order.         :type field_dict: collections.OrderedDict         :param create: whether to create the table if it doesn't exist         :type create: bool         :param overwrite: whether to overwrite the data in table or partition         :type overwrite: bool         :param partition: target partition as a dict of partition columns             and values         :type partition: dict         :param recreate: whether to drop and recreate the table at every             execution         :type recreate: bool         :param tblproperties: TBLPROPERTIES of the hive table being created         :type tblproperties: dict
Returns a Hive thrift client.
Checks whether a partition with a given name exists          :param schema: Name of hive schema (database) @table belongs to         :type schema: str         :param table: Name of hive table @partition belongs to         :type schema: str         :partition: Name of the partitions to check for (eg `a=b/c=d`)         :type schema: str         :rtype: bool          >>> hh = HiveMetastoreHook()         >>> t = 'static_babynames_partitioned'         >>> hh.check_for_named_partition('airflow', t, "ds=2015-01-01")         True         >>> hh.check_for_named_partition('airflow', t, "ds=xxx")         False
Check if table exists          >>> hh = HiveMetastoreHook()         >>> hh.table_exists(db='airflow', table_name='static_babynames')         True         >>> hh.table_exists(db='airflow', table_name='does_not_exist')         False
Returns a Hive connection object.
Get results of the provided hql in target schema.          :param hql: hql to be executed.         :type hql: str or list         :param schema: target schema, default to 'default'.         :type schema: str         :param fetch_size: max size of result to fetch.         :type fetch_size: int         :param hive_conf: hive_conf to execute alone with the hql.         :type hive_conf: dict         :return: results of hql execution, dict with data (list of results) and header         :rtype: dict
Execute hql in target schema and write results to a csv file.          :param hql: hql to be executed.         :type hql: str or list         :param csv_filepath: filepath of csv to write results into.         :type csv_filepath: str         :param schema: target schema, default to 'default'.         :type schema: str         :param delimiter: delimiter of the csv file, default to ','.         :type delimiter: str         :param lineterminator: lineterminator of the csv file.         :type lineterminator: str         :param output_header: header of the csv file, default to True.         :type output_header: bool         :param fetch_size: number of result rows to write into the csv file, default to 1000.         :type fetch_size: int         :param hive_conf: hive_conf to execute alone with the hql.         :type hive_conf: dict
Get a set of records from a Hive query.          :param hql: hql to be executed.         :type hql: str or list         :param schema: target schema, default to 'default'.         :type schema: str         :param hive_conf: hive_conf to execute alone with the hql.         :type hive_conf: dict         :return: result of hive execution         :rtype: list          >>> hh = HiveServer2Hook()         >>> sql = "SELECT * FROM airflow.static_babynames LIMIT 100"         >>> len(hh.get_records(sql))         100
Get a pandas dataframe from a Hive query          :param hql: hql to be executed.         :type hql: str or list         :param schema: target schema, default to 'default'.         :type schema: str         :return: result of hql execution         :rtype: DataFrame          >>> hh = HiveServer2Hook()         >>> sql = "SELECT * FROM airflow.static_babynames LIMIT 100"         >>> df = hh.get_pandas_df(sql)         >>> len(df.index)         100          :return: pandas.DateFrame
Retrieves connection to Cloud Vision.          :return: Google Cloud Vision client object.         :rtype: google.cloud.vision_v1.ProductSearchClient
Get Dingding endpoint for sending message.
Send Dingding message
Helper method that binds parameters to a SQL query.
Helper method that escapes parameters to a SQL query.
Helper method that casts a BigQuery row to the appropriate data types.     This is useful because BigQuery returns all fields as strings.
function to check expected type and raise     error if type is not correct
Returns a BigQuery PEP 249 connection object.
Returns a BigQuery service object.
Checks for the existence of a table in Google BigQuery.          :param project_id: The Google cloud project in which to look for the             table. The connection supplied to the hook must provide access to             the specified project.         :type project_id: str         :param dataset_id: The name of the dataset in which to look for the             table.         :type dataset_id: str         :param table_id: The name of the table to check the existence of.         :type table_id: str
Creates a new, empty table in the dataset.         To create a view, which is defined by a SQL query, parse a dictionary to 'view' kwarg          :param project_id: The project to create the table into.         :type project_id: str         :param dataset_id: The dataset to create the table into.         :type dataset_id: str         :param table_id: The Name of the table to be created.         :type table_id: str         :param schema_fields: If set, the schema field list as defined here:             https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.load.schema         :type schema_fields: list         :param labels: a dictionary containing labels for the table, passed to BigQuery         :type labels: dict          **Example**: ::              schema_fields=[{"name": "emp_name", "type": "STRING", "mode": "REQUIRED"},                            {"name": "salary", "type": "INTEGER", "mode": "NULLABLE"}]          :param time_partitioning: configure optional time partitioning fields i.e.             partition by field, type and expiration as per API specifications.              .. seealso::                 https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#timePartitioning         :type time_partitioning: dict         :param cluster_fields: [Optional] The fields used for clustering.             Must be specified with time_partitioning, data in the table will be first             partitioned and subsequently clustered.             https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#clustering.fields         :type cluster_fields: list         :param view: [Optional] A dictionary containing definition for the view.             If set, it will create a view instead of a table:             https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#view         :type view: dict          **Example**: ::              view = {                 "query": "SELECT * FROM `test-project-id.test_dataset_id.test_table_prefix*` LIMIT 1000",                 "useLegacySql": False             }          :return: None
Patch information in an existing table.         It only updates fileds that are provided in the request object.          Reference: https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/patch          :param dataset_id: The dataset containing the table to be patched.         :type dataset_id: str         :param table_id: The Name of the table to be patched.         :type table_id: str         :param project_id: The project containing the table to be patched.         :type project_id: str         :param description: [Optional] A user-friendly description of this table.         :type description: str         :param expiration_time: [Optional] The time when this table expires,             in milliseconds since the epoch.         :type expiration_time: int         :param external_data_configuration: [Optional] A dictionary containing             properties of a table stored outside of BigQuery.         :type external_data_configuration: dict         :param friendly_name: [Optional] A descriptive name for this table.         :type friendly_name: str         :param labels: [Optional] A dictionary containing labels associated with this table.         :type labels: dict         :param schema: [Optional] If set, the schema field list as defined here:             https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.load.schema             The supported schema modifications and unsupported schema modification are listed here:             https://cloud.google.com/bigquery/docs/managing-table-schemas             **Example**: ::                  schema=[{"name": "emp_name", "type": "STRING", "mode": "REQUIRED"},                                {"name": "salary", "type": "INTEGER", "mode": "NULLABLE"}]          :type schema: list         :param time_partitioning: [Optional] A dictionary containing time-based partitioning              definition for the table.         :type time_partitioning: dict         :param view: [Optional] A dictionary containing definition for the view.             If set, it will patch a view instead of a table:             https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#view             **Example**: ::                  view = {                     "query": "SELECT * FROM `test-project-id.test_dataset_id.test_table_prefix*` LIMIT 500",                     "useLegacySql": False                 }          :type view: dict         :param require_partition_filter: [Optional] If true, queries over the this table require a             partition filter. If false, queries over the table         :type require_partition_filter: bool
Cancel all started queries that have not yet completed
Delete an existing table from the dataset;         If the table does not exist, return an error unless ignore_if_missing         is set to True.          :param deletion_dataset_table: A dotted             ``(<project>.|<project>:)<dataset>.<table>`` that indicates which table             will be deleted.         :type deletion_dataset_table: str         :param ignore_if_missing: if True, then return success even if the             requested table does not exist.         :type ignore_if_missing: bool         :return:
creates a new, empty table in the dataset;         If the table already exists, update the existing table.         Since BigQuery does not natively allow table upserts, this is not an         atomic operation.          :param dataset_id: the dataset to upsert the table into.         :type dataset_id: str         :param table_resource: a table resource. see             https://cloud.google.com/bigquery/docs/reference/v2/tables#resource         :type table_resource: dict         :param project_id: the project to upsert the table into.  If None,             project will be self.project_id.         :return:
Grant authorized view access of a dataset to a view table.         If this view has already been granted access to the dataset, do nothing.         This method is not atomic.  Running it may clobber a simultaneous update.          :param source_dataset: the source dataset         :type source_dataset: str         :param view_dataset: the dataset that the view is in         :type view_dataset: str         :param view_table: the table of the view         :type view_table: str         :param source_project: the project of the source dataset. If None,             self.project_id will be used.         :type source_project: str         :param view_project: the project that the view is in. If None,             self.project_id will be used.         :type view_project: str         :return: the datasets resource of the source dataset.
Method returns dataset_resource if dataset exist         and raised 404 error if dataset does not exist          :param dataset_id: The BigQuery Dataset ID         :type dataset_id: str         :param project_id: The GCP Project ID         :type project_id: str         :return: dataset_resource              .. seealso::                 For more information, see Dataset Resource content:                 https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets#resource
Method returns full list of BigQuery datasets in the current project          .. seealso::             For more information, see:             https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/list          :param project_id: Google Cloud Project for which you             try to get all datasets         :type project_id: str         :return: datasets_list              Example of returned datasets_list: ::                     {                       "kind":"bigquery#dataset",                       "location":"US",                       "id":"your-project:dataset_2_test",                       "datasetReference":{                          "projectId":"your-project",                          "datasetId":"dataset_2_test"                       }                    },                    {                       "kind":"bigquery#dataset",                       "location":"US",                       "id":"your-project:dataset_1_test",                       "datasetReference":{                          "projectId":"your-project",                          "datasetId":"dataset_1_test"                       }                    }                 ]
Method to stream data into BigQuery one record at a time without needing         to run a load job          .. seealso::             For more information, see:             https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/insertAll          :param project_id: The name of the project where we have the table         :type project_id: str         :param dataset_id: The name of the dataset where we have the table         :type dataset_id: str         :param table_id: The name of the table         :type table_id: str         :param rows: the rows to insert         :type rows: list          **Example or rows**:             rows=[{"json": {"a_key": "a_value_0"}}, {"json": {"a_key": "a_value_1"}}]          :param ignore_unknown_values: [Optional] Accept rows that contain values             that do not match the schema. The unknown values are ignored.             The default value  is false, which treats unknown values as errors.         :type ignore_unknown_values: bool         :param skip_invalid_rows: [Optional] Insert all valid rows of a request,             even if invalid rows exist. The default value is false, which causes             the entire request to fail if any invalid rows exist.         :type skip_invalid_rows: bool         :param fail_on_error: [Optional] Force the task to fail if any errors occur.             The default value is false, which indicates the task should not fail             even if any insertion errors occur.         :type fail_on_error: bool
Executes a BigQuery query, and returns the job ID.          :param operation: The query to execute.         :type operation: str         :param parameters: Parameters to substitute into the query.         :type parameters: dict
Execute a BigQuery query multiple times with different parameters.          :param operation: The query to execute.         :type operation: str         :param seq_of_parameters: List of dictionary parameters to substitute into the             query.         :type seq_of_parameters: list
Helper method for fetchone, which returns the next row from a buffer.         If the buffer is empty, attempts to paginate through the result set for         the next page, and load it into the buffer.
Queries Postgres and returns a cursor to the results.
Create all the intermediate directories in a remote host      :param sftp_client: A Paramiko SFTP client.     :param remote_directory: Absolute Path of the directory containing the file     :return:
Create queue using connection object          :param queue_name: name of the queue.         :type queue_name: str         :param attributes: additional attributes for the queue (default: None)             For details of the attributes parameter see :py:meth:`botocore.client.SQS.create_queue`         :type attributes: dict          :return: dict with the information about the queue             For details of the returned value see :py:meth:`botocore.client.SQS.create_queue`         :rtype: dict
Send message to the queue          :param queue_url: queue url         :type queue_url: str         :param message_body: the contents of the message         :type message_body: str         :param delay_seconds: seconds to delay the message         :type delay_seconds: int         :param message_attributes: additional attributes for the message (default: None)             For details of the attributes parameter see :py:meth:`botocore.client.SQS.send_message`         :type message_attributes: dict          :return: dict with the information about the message sent             For details of the returned value see :py:meth:`botocore.client.SQS.send_message`         :rtype: dict
Run the task command.          :param run_with: list of tokens to run the task command with e.g. ``['bash', '-c']``         :type run_with: list         :param join_args: whether to concatenate the list of command tokens e.g. ``['airflow', 'run']`` vs             ``['airflow run']``         :param join_args: bool         :return: the process that was run         :rtype: subprocess.Popen
A callback that should be called when this is done running.
Parse options and process commands
generate HTML header content
generate HTML div
generate javascript code for the chart
Create X-axis
Create Y-axis
Returns a sqlite connection object
Decorator to log user actions
Decorator to make a view compressed
Returns the last dag run for a dag, None if there was none.     Last dag run can be any type of run eg. scheduled or backfilled.     Overridden DagRuns are ignored.
Creates a dag run from this dag including the tasks associated with this dag.         Returns the dag run.          :param run_id: defines the the run id for this dag run         :type run_id: str         :param execution_date: the execution date of this dag run         :type execution_date: datetime.datetime         :param state: the state of the dag run         :type state: airflow.utils.state.State         :param start_date: the date this dag run should be evaluated         :type start_date: datetime.datetime         :param external_trigger: whether this dag run is externally triggered         :type external_trigger: bool         :param session: database session         :type session: sqlalchemy.orm.session.Session
Publish the message to SQS queue          :param context: the context object         :type context: dict         :return: dict with information about the message sent             For details of the returned dict see :py:meth:`botocore.client.SQS.send_message`         :rtype: dict
returns a json response from a json serializable python object
Opens the given file. If the path contains a folder with a .zip suffix, then     the folder is treated as a zip archive, opening the file inside the archive.      :return: a file object, as in `open`, or as in `ZipFile.open`.
Used by cache to get a unique key per URL
Returns Gcp Video Intelligence Service client          :rtype: google.cloud.videointelligence_v1.VideoIntelligenceServiceClient
Performs video annotation.          :param input_uri: Input video location. Currently, only Google Cloud Storage URIs are supported,             which must be specified in the following format: ``gs://bucket-id/object-id``.         :type input_uri: str         :param input_content: The video data bytes.             If unset, the input video(s) should be specified via ``input_uri``.             If set, ``input_uri`` should be unset.         :type input_content: bytes         :param features: Requested video annotation features.         :type features: list[google.cloud.videointelligence_v1.VideoIntelligenceServiceClient.enums.Feature]         :param output_uri: Optional, location where the output (in JSON format) should be stored. Currently,             only Google Cloud Storage URIs are supported, which must be specified in the following format:             ``gs://bucket-id/object-id``.         :type output_uri: str         :param video_context: Optional, Additional video context and/or feature-specific parameters.         :type video_context: dict or google.cloud.videointelligence_v1.types.VideoContext         :param location: Optional, cloud region where annotation should take place. Supported cloud regions:             us-east1, us-west1, europe-west1, asia-east1.             If no region is specified, a region will be determined based on video file location.         :type location: str         :param retry: Retry object used to determine when/if to retry requests.             If None is specified, requests will not be retried.         :type retry: google.api_core.retry.Retry         :param timeout: Optional, The amount of time, in seconds, to wait for the request to complete.             Note that if retry is specified, the timeout applies to each individual attempt.         :type timeout: float         :param metadata: Optional, Additional metadata that is provided to the method.         :type metadata: seq[tuple[str, str]]
Get Opsgenie api_key for creating alert
Overwrite HttpHook get_conn because this hook just needs base_url         and headers, and does not need generic params          :param headers: additional headers to be passed through as a dictionary         :type headers: dict
Execute the Opsgenie Alert call          :param payload: Opsgenie API Create Alert payload values             See https://docs.opsgenie.com/docs/alert-api#section-create-alert         :type payload: dict
Construct the Opsgenie JSON payload. All relevant parameters are combined here         to a valid Opsgenie JSON payload.          :return: Opsgenie payload (dict) to send
Call the OpsgenieAlertHook to post message
check if aws conn exists already or create one and return it          :return: boto3 session
Run Presto query on athena with provided config and return submitted query_execution_id          :param query: Presto query to run         :type query: str         :param query_context: Context in which query need to be run         :type query_context: dict         :param result_configuration: Dict with path to store results in and config related to encryption         :type result_configuration: dict         :param client_request_token: Unique token created by user to avoid multiple executions of same query         :type client_request_token: str         :return: str
Fetch the status of submitted athena query. Returns None or one of valid query states.          :param query_execution_id: Id of submitted athena query         :type query_execution_id: str         :return: str
Poll the status of submitted athena query until query state reaches final state.         Returns one of the final states          :param query_execution_id: Id of submitted athena query         :type query_execution_id: str         :param max_tries: Number of times to poll for query state before function exits         :type max_tries: int         :return: str
Returns an SFTP connection object
Sleep for the time specified in the exception. If not specified, wait         for 60 seconds.
Call Zendesk API and return results          :param path: The Zendesk API to call         :param query: Query parameters         :param get_all_pages: Accumulate results over all pages before                returning. Due to strict rate limiting, this can often timeout.                Waits for recommended period between tries after a timeout.         :param side_loading: Retrieve related records as part of a single                request. In order to enable side-loading, add an 'include'                query parameter containing a comma-separated list of resources                to load. For more information on side-loading see                https://developer.zendesk.com/rest_api/docs/core/side_loading
Retrieves the partition values for a table.          :param database_name: The name of the catalog database where the partitions reside.         :type database_name: str         :param table_name: The name of the partitions' table.         :type table_name: str         :param expression: An expression filtering the partitions to be returned.             Please see official AWS documentation for further information.             https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-catalog-partitions.html#aws-glue-api-catalog-partitions-GetPartitions         :type expression: str         :param page_size: pagination size         :type page_size: int         :param max_items: maximum items to return         :type max_items: int         :return: set of partition values where each value is a tuple since             a partition may be composed of multiple columns. For example:             ``{('2018-01-01','1'), ('2018-01-01','2')}``
Get the information of the table          :param database_name: Name of hive database (schema) @table belongs to         :type database_name: str         :param table_name: Name of hive table         :type table_name: str         :rtype: dict          >>> hook = AwsGlueCatalogHook()         >>> r = hook.get_table('db', 'table_foo')         >>> r['Name'] = 'table_foo'
Get the physical location of the table          :param database_name: Name of hive database (schema) @table belongs to         :type database_name: str         :param table_name: Name of hive table         :type table_name: str         :return: str
Return status of a cluster          :param cluster_identifier: unique identifier of a cluster         :type cluster_identifier: str
Delete a cluster and optionally create a snapshot          :param cluster_identifier: unique identifier of a cluster         :type cluster_identifier: str         :param skip_final_cluster_snapshot: determines cluster snapshot creation         :type skip_final_cluster_snapshot: bool         :param final_cluster_snapshot_identifier: name of final cluster snapshot         :type final_cluster_snapshot_identifier: str
Gets a list of snapshots for a cluster          :param cluster_identifier: unique identifier of a cluster         :type cluster_identifier: str
Restores a cluster from its snapshot          :param cluster_identifier: unique identifier of a cluster         :type cluster_identifier: str         :param snapshot_identifier: unique identifier for a snapshot of a cluster         :type snapshot_identifier: str
Creates a snapshot of a cluster          :param snapshot_identifier: unique identifier for a snapshot of a cluster         :type snapshot_identifier: str         :param cluster_identifier: unique identifier of a cluster         :type cluster_identifier: str
SlackAPIOperator calls will not fail even if the call is not unsuccessful.         It should not prevent a DAG from completing in success
Creates a job flow using the config from the EMR connection.         Keys of the json extra hash may have the arguments of the boto3         run_job_flow method.         Overrides for this config may be passed as the job_flow_overrides.
Will test the filepath result and test if its size is at least self.filesize          :param result: a list of dicts returned by Snakebite ls         :param size: the file size in MB a file should be at least to trigger True         :return: (bool) depending on the matching criteria
Will filter if instructed to do so the result to remove matching criteria          :param result: list of dicts returned by Snakebite ls         :type result: list[dict]         :param ignored_ext: list of ignored extensions         :type ignored_ext: list         :param ignore_copying: shall we ignore ?         :type ignore_copying: bool         :return: list of dicts which were not removed         :rtype: list[dict]
Executed by task_instance at runtime
Get pool by a given name.
Create a pool with a given parameters.
Delete pool by a given name.
Converts a python dictionary to the proto supplied          :param py_dict: The dictionary to convert         :type py_dict: dict         :param proto: The proto object to merge with dictionary         :type proto: protobuf         :return: A parsed python dictionary in provided proto format         :raises:             ParseError: On JSON parsing problems.
Given an operation, continuously fetches the status from Google Cloud until either         completion or an error occurring          :param operation: The Operation to wait for         :type operation: google.cloud.container_V1.gapic.enums.Operation         :param project_id: Google Cloud Platform project ID         :type project_id: str         :return: A new, updated operation fetched from Google Cloud
Fetches the operation from Google Cloud          :param operation_name: Name of operation to fetch         :type operation_name: str         :param project_id: Google Cloud Platform project ID         :type project_id: str         :return: The new, updated operation from Google Cloud
Append labels to provided Cluster Protobuf          Labels must fit the regex ``[a-z]([-a-z0-9]*[a-z0-9])?`` (current          airflow version string follows semantic versioning spec: x.y.z).          :param cluster_proto: The proto to append resource_label airflow             version to         :type cluster_proto: google.cloud.container_v1.types.Cluster         :param key: The key label         :type key: str         :param val:         :type val: str         :return: The cluster proto updated with new label
Creates a cluster, consisting of the specified number and type of Google Compute         Engine instances.          :param cluster: A Cluster protobuf or dict. If dict is provided, it must             be of the same form as the protobuf message             :class:`google.cloud.container_v1.types.Cluster`         :type cluster: dict or google.cloud.container_v1.types.Cluster         :param project_id: Google Cloud Platform project ID         :type project_id: str         :param retry: A retry object (``google.api_core.retry.Retry``) used to             retry requests.             If None is specified, requests will not be retried.         :type retry: google.api_core.retry.Retry         :param timeout: The amount of time, in seconds, to wait for the request to             complete. Note that if retry is specified, the timeout applies to each             individual attempt.         :type timeout: float         :return: The full url to the new, or existing, cluster         :raises:             ParseError: On JSON parsing problems when trying to convert dict             AirflowException: cluster is not dict type nor Cluster proto type
Gets details of specified cluster          :param name: The name of the cluster to retrieve         :type name: str         :param project_id: Google Cloud Platform project ID         :type project_id: str         :param retry: A retry object used to retry requests. If None is specified,             requests will not be retried.         :type retry: google.api_core.retry.Retry         :param timeout: The amount of time, in seconds, to wait for the request to             complete. Note that if retry is specified, the timeout applies to each             individual attempt.         :type timeout: float         :return: google.cloud.container_v1.types.Cluster
Given a Discord http_conn_id, return the default webhook endpoint or override if a         webhook_endpoint is manually supplied.          :param http_conn_id: The provided connection ID         :param webhook_endpoint: The manually provided webhook endpoint         :return: Webhook endpoint (str) to use
Construct the Discord JSON payload. All relevant parameters are combined here         to a valid Discord JSON payload.          :return: Discord payload (str) to send
Execute the Discord webhook call
Encrypts a plaintext message using Google Cloud KMS.          :param key_name: The Resource Name for the key (or key version)                          to be used for encyption. Of the form                          ``projects/*/locations/*/keyRings/*/cryptoKeys/**``         :type key_name: str         :param plaintext: The message to be encrypted.         :type plaintext: bytes         :param authenticated_data: Optional additional authenticated data that                                    must also be provided to decrypt the message.         :type authenticated_data: bytes         :return: The base 64 encoded ciphertext of the original message.         :rtype: str
Imports table from remote location to target dir. Arguments are         copies of direct sqoop command line arguments          :param table: Table to read         :param target_dir: HDFS destination dir         :param append: Append data to an existing dataset in HDFS         :param file_type: "avro", "sequence", "text" or "parquet".             Imports data to into the specified format. Defaults to text.         :param columns: <col,col,col…> Columns to import from table         :param split_by: Column of the table used to split work units         :param where: WHERE clause to use during import         :param direct: Use direct connector if exists for the database         :param driver: Manually specify JDBC driver class to use         :param extra_import_options: Extra import options to pass as dict.             If a key doesn't have a value, just pass an empty string to it.             Don't include prefix of -- for sqoop options.
Imports a specific query from the rdbms to hdfs          :param query: Free format query to run         :param target_dir: HDFS destination dir         :param append: Append data to an existing dataset in HDFS         :param file_type: "avro", "sequence", "text" or "parquet"             Imports data to hdfs into the specified format. Defaults to text.         :param split_by: Column of the table used to split work units         :param direct: Use direct import fast path         :param driver: Manually specify JDBC driver class to use         :param extra_import_options: Extra import options to pass as dict.             If a key doesn't have a value, just pass an empty string to it.             Don't include prefix of -- for sqoop options.
Exports Hive table to remote location. Arguments are copies of direct         sqoop command line Arguments          :param table: Table remote destination         :param export_dir: Hive table to export         :param input_null_string: The string to be interpreted as null for             string columns         :param input_null_non_string: The string to be interpreted as null             for non-string columns         :param staging_table: The table in which data will be staged before             being inserted into the destination table         :param clear_staging_table: Indicate that any data present in the             staging table can be deleted         :param enclosed_by: Sets a required field enclosing character         :param escaped_by: Sets the escape character         :param input_fields_terminated_by: Sets the field separator character         :param input_lines_terminated_by: Sets the end-of-line character         :param input_optionally_enclosed_by: Sets a field enclosing character         :param batch: Use batch mode for underlying statement execution         :param relaxed_isolation: Transaction isolation to read uncommitted             for the mappers         :param extra_export_options: Extra export options to pass as dict.             If a key doesn't have a value, just pass an empty string to it.             Don't include prefix of -- for sqoop options.
Retrieves connection to Cloud Text to Speech.          :return: Google Cloud Text to Speech client object.         :rtype: google.cloud.texttospeech_v1.TextToSpeechClient
Synthesizes text input          :param input_data: text input to be synthesized. See more:             https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.SynthesisInput         :type input_data: dict or google.cloud.texttospeech_v1.types.SynthesisInput         :param voice: configuration of voice to be used in synthesis. See more:             https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.VoiceSelectionParams         :type voice: dict or google.cloud.texttospeech_v1.types.VoiceSelectionParams         :param audio_config: configuration of the synthesized audio. See more:             https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.AudioConfig         :type audio_config: dict or google.cloud.texttospeech_v1.types.AudioConfig         :return: SynthesizeSpeechResponse See more:             https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.SynthesizeSpeechResponse         :rtype: object         :param retry: (Optional) A retry object used to retry requests. If None is specified,                 requests will not be retried.         :type retry: google.api_core.retry.Retry         :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.             Note that if retry is specified, the timeout applies to each individual attempt.         :type timeout: float
Close and upload local log file to remote storage S3.
When using git to retrieve the DAGs, use the GitSync Init Container
Defines any necessary environment variables for the pod executor
Defines any necessary secrets for the pod executor
Defines the security context
Get link to qubole command result page.          :param operator: operator         :param dttm: datetime         :return: url link
Heartbeats update the job's entry in the database with a timestamp         for the latest_heartbeat and allows for the job to be killed         externally. This allows at the system level to monitor what is         actually active.          For instance, an old heartbeat for SchedulerJob would mean something         is wrong.          This also allows for any job to be killed externally, regardless         of who is running it or on which machine it is running.          Note that if your heartbeat is set to 60 seconds and you call this         method after 10 seconds of processing since the last heartbeat, it         will sleep 50 seconds to complete the 60 seconds and keep a steady         heart rate. If you go over 60 seconds before calling it, it won't         sleep at all.
Launch a process to process the given file.          :param result_queue: the queue to use for passing back the result         :type result_queue: multiprocessing.Queue         :param file_path: the file to process         :type file_path: unicode         :param pickle_dags: whether to pickle the DAGs found in the file and             save them to the DB         :type pickle_dags: bool         :param dag_id_white_list: if specified, only examine DAG ID's that are             in this list         :type dag_id_white_list: list[unicode]         :param thread_name: the name to use for the process that is launched         :type thread_name: unicode         :return: the process that was launched         :rtype: multiprocessing.Process         :param zombies: zombie task instances to kill         :type zombies: list[airflow.utils.dag_processing.SimpleTaskInstance]
Launch the process and start processing the DAG.
Check if the process launched to process this file is done.          :return: whether the process is finished running         :rtype: bool
Helper method to clean up processor_agent to avoid leaving orphan processes.
For the DAGs in the given DagBag, record any associated import errors and clears         errors for files that no longer have them. These are usually displayed through the         Airflow UI so that users know that there are issues parsing DAGs.          :param session: session for ORM operations         :type session: sqlalchemy.orm.session.Session         :param dagbag: DagBag containing DAGs with import errors         :type dagbag: airflow.models.DagBag
This method schedules the tasks for a single DAG by looking at the         active DAG runs and adding task instances that should run to the         queue.
For all DAG IDs in the SimpleDagBag, look for task instances in the         old_states and set them to new_state if the corresponding DagRun         does not exist or exists but is not in the running state. This         normally should not happen, but it can if the state of DagRuns are         changed manually.          :param old_states: examine TaskInstances in this state         :type old_state: list[airflow.utils.state.State]         :param new_state: set TaskInstances to this state         :type new_state: airflow.utils.state.State         :param simple_dag_bag: TaskInstances associated with DAGs in the             simple_dag_bag and with states in the old_state will be examined         :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag
Get the concurrency maps.          :param states: List of states to query for         :type states: list[airflow.utils.state.State]         :return: A map from (dag_id, task_id) to # of task instances and          a map from (dag_id, task_id) to # of task instances in the given state list         :rtype: dict[tuple[str, str], int]
Changes the state of task instances in the list with one of the given states         to QUEUED atomically, and returns the TIs changed in SimpleTaskInstance format.          :param task_instances: TaskInstances to change the state of         :type task_instances: list[airflow.models.TaskInstance]         :param acceptable_states: Filters the TaskInstances updated to be in these states         :type acceptable_states: Iterable[State]         :rtype: list[airflow.utils.dag_processing.SimpleTaskInstance]
Takes task_instances, which should have been set to queued, and enqueues them         with the executor.          :param simple_task_instances: TaskInstances to enqueue         :type simple_task_instances: list[SimpleTaskInstance]         :param simple_dag_bag: Should contains all of the task_instances' dags         :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag
Attempts to execute TaskInstances that should be executed by the scheduler.          There are three steps:         1. Pick TIs by priority with the constraint that they are in the expected states         and that we do exceed max_active_runs or pool limits.         2. Change the state for the TIs above atomically.         3. Enqueue the TIs in the executor.          :param simple_dag_bag: TaskInstances associated with DAGs in the             simple_dag_bag will be fetched from the DB and executed         :type simple_dag_bag: airflow.utils.dag_processing.SimpleDagBag         :param states: Execute TaskInstances in these states         :type states: tuple[airflow.utils.state.State]         :return: Number of task instance with state changed.
If there are tasks left over in the executor,         we set them back to SCHEDULED to avoid creating hanging tasks.          :param session: session for ORM operations
Respond to executor events.
Process a Python file containing Airflow DAGs.          This includes:          1. Execute the file and look for DAG objects in the namespace.         2. Pickle the DAG and save it to the DB (if necessary).         3. For each DAG, see what tasks should run and create appropriate task         instances in the DB.         4. Record any errors importing the file into ORM         5. Kill (in ORM) any task instances belonging to the DAGs that haven't         issued a heartbeat in a while.          Returns a list of SimpleDag objects that represent the DAGs found in         the file          :param file_path: the path to the Python file that should be executed         :type file_path: unicode         :param zombies: zombie task instances to kill.         :type zombies: list[airflow.utils.dag_processing.SimpleTaskInstance]         :param pickle_dags: whether serialize the DAGs found in the file and             save them to the db         :type pickle_dags: bool         :return: a list of SimpleDags made from the Dags found in the file         :rtype: list[airflow.utils.dag_processing.SimpleDagBag]
Updates the counters per state of the tasks that were running. Can re-add         to tasks to run in case required.          :param ti_status: the internal status of the backfill job tasks         :type ti_status: BackfillJob._DagRunTaskStatus
Checks if the executor agrees with the state of task instances         that are running          :param running: dict of key, task to verify
Returns a dag run for the given run date, which will be matched to an existing         dag run if available or create a new dag run otherwise. If the max_active_runs         limit is reached, this function will return None.          :param run_date: the execution date for the dag run         :type run_date: datetime.datetime         :param session: the database session object         :type session: sqlalchemy.orm.session.Session         :return: a DagRun in state RUNNING or None
Returns a map of task instance key to task instance object for the tasks to         run in the given dag run.          :param dag_run: the dag run to get the tasks from         :type dag_run: airflow.models.DagRun         :param session: the database session object         :type session: sqlalchemy.orm.session.Session
Computes the dag runs and their respective task instances for         the given run dates and executes the task instances.         Returns a list of execution dates of the dag runs that were executed.          :param run_dates: Execution dates for dag runs         :type run_dates: list         :param ti_status: internal BackfillJob status structure to tis track progress         :type ti_status: BackfillJob._DagRunTaskStatus         :param executor: the executor to use, it must be previously started         :type executor: BaseExecutor         :param pickle_id: numeric id of the pickled dag, None if not pickled         :type pickle_id: int         :param start_date: backfill start date         :type start_date: datetime.datetime         :param session: the current session object         :type session: sqlalchemy.orm.session.Session
Go through the dag_runs and update the state based on the task_instance state.         Then set DAG runs that are not finished to failed.          :param dag_runs: DAG runs         :param session: session         :return: None
Initializes all components required to run a dag for a specified date range and         calls helper method to execute the tasks.
Self destruct task if state has been moved away from running externally
Provides a client for interacting with the Cloud Spanner API.          :param project_id: The ID of the  GCP project.         :type project_id: str         :return: google.cloud.spanner_v1.client.Client         :rtype: object
Gets information about a particular instance.          :param project_id: Optional, The ID of the  GCP project that owns the Cloud Spanner             database.  If set to None or missing, the default project_id from the GCP connection is used.         :type project_id: str         :param instance_id: The ID of the Cloud Spanner instance.         :type instance_id: str         :return: google.cloud.spanner_v1.instance.Instance         :rtype: object
Invokes a method on a given instance by applying a specified Callable.          :param project_id: The ID of the  GCP project that owns the Cloud Spanner             database.         :type project_id: str         :param instance_id: The ID of the instance.         :type instance_id: str         :param configuration_name: Name of the instance configuration defining how the             instance will be created. Required for instances which do not yet exist.         :type configuration_name: str         :param node_count: (Optional) Number of nodes allocated to the instance.         :type node_count: int         :param display_name: (Optional) The display name for the instance in the Cloud             Console UI. (Must be between 4 and 30 characters.) If this value is not set             in the constructor, will fall back to the instance ID.         :type display_name: str         :param func: Method of the instance to be called.         :type func: Callable
Creates a new Cloud Spanner instance.          :param instance_id: The ID of the Cloud Spanner instance.         :type instance_id: str         :param configuration_name: The name of the instance configuration defining how the             instance will be created. Possible configuration values can be retrieved via             https://cloud.google.com/spanner/docs/reference/rest/v1/projects.instanceConfigs/list         :type configuration_name: str         :param node_count: (Optional) The number of nodes allocated to the Cloud Spanner             instance.         :type node_count: int         :param display_name: (Optional) The display name for the instance in the GCP             Console. Must be between 4 and 30 characters.  If this value is not set in             the constructor, the name falls back to the instance ID.         :type display_name: str         :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner             database. If set to None or missing, the default project_id from the GCP connection is used.         :type project_id: str         :return: None
Updates an existing Cloud Spanner instance.          :param instance_id: The ID of the Cloud Spanner instance.         :type instance_id: str         :param configuration_name: The name of the instance configuration defining how the             instance will be created. Possible configuration values can be retrieved via             https://cloud.google.com/spanner/docs/reference/rest/v1/projects.instanceConfigs/list         :type configuration_name: str         :param node_count: (Optional) The number of nodes allocated to the Cloud Spanner             instance.         :type node_count: int         :param display_name: (Optional) The display name for the instance in the GCP             Console. Must be between 4 and 30 characters. If this value is not set in             the constructor, the name falls back to the instance ID.         :type display_name: str         :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner             database. If set to None or missing, the default project_id from the GCP connection is used.         :type project_id: str         :return: None
Deletes an existing Cloud Spanner instance.          :param instance_id: The ID of the Cloud Spanner instance.         :type instance_id: str         :param project_id: Optional, the ID of the GCP project that owns the Cloud Spanner             database. If set to None or missing, the default project_id from the GCP connection is used.         :type project_id: str         :return: None
Retrieves a database in Cloud Spanner. If the database does not exist         in the specified instance, it returns None.          :param instance_id: The ID of the Cloud Spanner instance.         :type instance_id: str         :param database_id: The ID of the database in Cloud Spanner.         :type database_id: str         :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner             database. If set to None or missing, the default project_id from the GCP connection is used.         :type project_id: str         :return: Database object or None if database does not exist         :rtype: google.cloud.spanner_v1.database.Database or None
Creates a new database in Cloud Spanner.          :type project_id: str         :param instance_id: The ID of the Cloud Spanner instance.         :type instance_id: str         :param database_id: The ID of the database to create in Cloud Spanner.         :type database_id: str         :param ddl_statements: The string list containing DDL for the new database.         :type ddl_statements: list[str]         :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner             database. If set to None or missing, the default project_id from the GCP connection is used.         :return: None
Updates DDL of a database in Cloud Spanner.          :type project_id: str         :param instance_id: The ID of the Cloud Spanner instance.         :type instance_id: str         :param database_id: The ID of the database in Cloud Spanner.         :type database_id: str         :param ddl_statements: The string list containing DDL for the new database.         :type ddl_statements: list[str]         :param project_id: Optional, the ID of the GCP project that owns the Cloud Spanner             database. If set to None or missing, the default project_id from the GCP connection is used.         :param operation_id: (Optional) The unique per database operation ID that can be             specified to implement idempotency check.         :type operation_id: str         :return: None
Drops a database in Cloud Spanner.          :type project_id: str         :param instance_id: The ID of the Cloud Spanner instance.         :type instance_id: str         :param database_id: The ID of the database in Cloud Spanner.         :type database_id: str         :param project_id: Optional, the ID of the  GCP project that owns the Cloud Spanner             database. If set to None or missing, the default project_id from the GCP connection is used.         :return: True if everything succeeded         :rtype: bool
Pokes for a mail attachment on the mail server.          :param context: The context that is being provided when poking.         :type context: dict         :return: True if attachment with the given name is present and False if not.         :rtype: bool
Creates additional_properties parameter based on language_hints, web_detection_params and     additional_properties parameters specified by the user
Returns a cassandra Session object
Checks if a table exists in Cassandra          :param table: Target Cassandra table.                       Use dot notation to target a specific keyspace.         :type table: str
Checks if a record exists in Cassandra          :param table: Target Cassandra table.                       Use dot notation to target a specific keyspace.         :type table: str         :param keys: The keys and their values to check the existence.         :type keys: dict
Construct the command to poll the driver status.          :return: full command to be executed
Remote Popen to execute the spark-submit job          :param application: Submitted application, jar or py file         :type application: str         :param kwargs: extra arguments to Popen (see subprocess.Popen)
Processes the log files and extracts useful information out of it.          If the deploy-mode is 'client', log the output of the submit command as those         are the output logs of the Spark worker directly.          Remark: If the driver needs to be tracked for its status, the log-level of the         spark deploy needs to be at least INFO (log4j.logger.org.apache.spark.deploy=INFO)          :param itr: An iterator which iterates over the input of the subprocess
parses the logs of the spark driver status query process          :param itr: An iterator which iterates over the input of the subprocess
Get the task runner that can be used to run the given job.      :param local_task_job: The LocalTaskJob associated with the TaskInstance         that needs to be executed.     :type local_task_job: airflow.jobs.LocalTaskJob     :return: The task runner to use to run the task.     :rtype: airflow.task.task_runner.base_task_runner.BaseTaskRunner
Try to use a waiter from the below pull request              * https://github.com/boto/botocore/pull/1307          If the waiter is not available apply a exponential backoff              * docs.aws.amazon.com/general/latest/gr/api-retries.html
Queries mysql and returns a cursor to the results.
Configure a csv writer with the file_handle and write schema         as headers for the new file.
Takes a cursor, and writes the BigQuery schema in .json format for the         results to a local file system.          :return: A dictionary where key is a filename to be used as an object             name in GCS, and values are file handles to local files that             contains the BigQuery schema fields in .json format.
Return a dict of column name and column type based on self.schema if not None.
Helper function that maps from MySQL fields to BigQuery fields. Used         when a schema_filename is set.
Execute sqoop job
Saves the lineage to XCom and if configured to do so sends it     to the backend.
Returns the extra property by deserializing json.
Get a set of dates as a list based on a start, end and delta, delta     can be something that can be added to `datetime.datetime`     or a cron expression as a `str`      :Example::          date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta=timedelta(1))             [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),             datetime.datetime(2016, 1, 3, 0, 0)]         date_range(datetime(2016, 1, 1), datetime(2016, 1, 3), delta='0 0 * * *')             [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 1, 2, 0, 0),             datetime.datetime(2016, 1, 3, 0, 0)]         date_range(datetime(2016, 1, 1), datetime(2016, 3, 3), delta="0 0 0 * *")             [datetime.datetime(2016, 1, 1, 0, 0), datetime.datetime(2016, 2, 1, 0, 0),             datetime.datetime(2016, 3, 1, 0, 0)]      :param start_date: anchor date to start the series from     :type start_date: datetime.datetime     :param end_date: right boundary for the date range     :type end_date: datetime.datetime     :param num: alternatively to end_date, you can specify the number of         number of entries you want in the range. This number can be negative,         output will always be sorted regardless     :type num: int
Convert an array of time durations in seconds to the specified time unit.
Get a datetime object representing `n` days ago. By default the time is     set to midnight.
Initialize the role with the permissions and related view-menus.          :param role_name:         :param role_vms:         :param role_perms:         :return:
Delete the given Role          :param role_name: the name of a role in the ab_role table
Get all the roles associated with the user.          :param user: the ab_user in FAB model.         :return: a list of roles associated with the user.
Returns a set of tuples with the perm name and view menu name
Whether the user has this role name
Whether the user has this perm
FAB leaves faulty permissions that need to be cleaned up
Add the new permission , view_menu to ab_permission_view_role if not exists.         It will add the related entry to ab_permission         and ab_view_menu two meta tables as well.          :param permission_name: Name of the permission.         :type permission_name: str         :param view_menu_name: Name of the view-menu         :type view_menu_name: str         :return:
Admin should have all the permission-views.         Add the missing ones to the table for admin.          :return: None.
Set the access policy on the given DAG's ViewModel.          :param dag_id: the ID of the DAG whose permissions should be updated         :type dag_id: string         :param access_control: a dict where each key is a rolename and             each value is a set() of permission names (e.g.,             {'can_dag_read'}         :type access_control: dict
Create perm-vm if not exist and insert into FAB security model for all-dags.
Deferred load of Fernet key.      This function could fail either because Cryptography is not installed     or because the Fernet key is invalid.      :return: Fernet object     :raises: airflow.exceptions.AirflowException if there's a problem trying to load Fernet
Checks for existence of the partition in the AWS Glue Catalog table
Gets the AwsGlueCatalogHook
Check for message on subscribed queue and write to xcom the message with key ``messages``          :param context: the context object         :type context: dict         :return: ``True`` if message is available or ``False``
Returns a snakebite HDFSClient object.
Establishes a connection depending on the security mode set via config or environment variable.          :return: a hdfscli InsecureClient or KerberosClient object.         :rtype: hdfs.InsecureClient or hdfs.ext.kerberos.KerberosClient
Check for the existence of a path in HDFS by querying FileStatus.          :param hdfs_path: The path to check.         :type hdfs_path: str         :return: True if the path exists and False if not.         :rtype: bool
r"""         Uploads a file to HDFS.          :param source: Local path to file or folder.             If it's a folder, all the files inside of it will be uploaded.             .. note:: This implies that folders empty of files will not be created remotely.          :type source: str         :param destination: PTarget HDFS path.             If it already exists and is a directory, files will be uploaded inside.         :type destination: str         :param overwrite: Overwrite any existing file or directory.         :type overwrite: bool         :param parallelism: Number of threads to use for parallelization.             A value of `0` (or negative) uses as many threads as there are files.         :type parallelism: int         :param \**kwargs: Keyword arguments forwarded to :meth:`hdfs.client.Client.upload`.
Establish a connection to pinot broker through pinot dbqpi.
Get the connection uri for pinot broker.          e.g: http://localhost:9000/pql
Convert native python ``datetime.date`` object  to a format supported by the API
Convert native python ``datetime.time`` object  to a format supported by the API
Returns a Redis connection.
Executes the sql and returns a pandas dataframe          :param sql: the sql statement to be executed (str) or a list of             sql statements to execute         :type sql: str or list         :param parameters: The parameters to render the SQL query with.         :type parameters: mapping or iterable
Runs a command or a list of commands. Pass a list of sql         statements to the sql parameter to get them to execute         sequentially          :param sql: the sql statement to be executed (str) or a list of             sql statements to execute         :type sql: str or list         :param autocommit: What to set the connection's autocommit setting to             before executing the query.         :type autocommit: bool         :param parameters: The parameters to render the SQL query with.         :type parameters: mapping or iterable
Sets the autocommit flag on the connection
A generic way to insert a set of tuples into a table,         a new transaction is created every commit_every rows          :param table: Name of the target table         :type table: str         :param rows: The rows to insert into the table         :type rows: iterable of tuples         :param target_fields: The names of the columns to fill in the table         :type target_fields: iterable of strings         :param commit_every: The maximum number of rows to insert in one             transaction. Set to 0 to insert all rows in one transaction.         :type commit_every: int         :param replace: Whether to replace instead of insert         :type replace: bool
Returns the SQL literal of the cell as a string.          :param cell: The cell to insert into the table         :type cell: object         :param conn: The database connection         :type conn: connection object         :return: The serialized cell         :rtype: str
An endpoint helping check the health status of the Airflow instance,         including metadatabase and scheduler.
A restful endpoint that returns external links for a given Operator          It queries the operator that sent the request for the links it wishes         to provide for a given external link name.          API: GET         Args: dag_id: The id of the dag containing the task in question               task_id: The id of the task in question               execution_date: The date of execution of the task               link_name: The name of the link reference to find the actual URL for          Returns:             200: {url: <url of link>, error: None} - returned when there was no problem                 finding the URL             404: {url: None, error: <error message>} - returned when the operator does                 not return a URL
Opens a connection to the cloudant service and closes it automatically if used as context manager.          .. note::             In the connection form:             - 'host' equals the 'Account' (optional)             - 'login' equals the 'Username (or API Key)' (required)             - 'password' equals the 'Password' (required)          :return: an authorized cloudant session context manager object.         :rtype: cloudant
Call the SlackWebhookHook to post the provided Slack message
Returns the Credentials object for Google API
Returns an authorized HTTP object to be used to build a Google cloud         service hook connection.
Function decorator that intercepts HTTP Errors and raises AirflowException         with more informative message.
Decorator that provides fallback for Google Cloud Platform project id. If         the project is None it will be replaced with the project_id from the         service account the Hook is authenticated with. Project id can be specified         either via project_id kwarg or via first parameter in positional args.          :param func: function to wrap         :return: result of the function call
A list of states indicating that a task either has not completed         a run or has not even started.
Construct the spark-sql command to execute. Verbose output is enabled         as default.          :param cmd: command to append to the spark-sql command         :type cmd: str         :return: full command to be executed
Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.      See ``ToTensor`` for more details.      Args:         pic (PIL Image or numpy.ndarray): Image to be converted to tensor.      Returns:         Tensor: Converted image.
Normalize a tensor image with mean and standard deviation.      .. note::         This transform acts out of place by default, i.e., it does not mutates the input tensor.      See :class:`~torchvision.transforms.Normalize` for more details.      Args:         tensor (Tensor): Tensor image of size (C, H, W) to be normalized.         mean (sequence): Sequence of means for each channel.         std (sequence): Sequence of standard deviations for each channel.      Returns:         Tensor: Normalized Tensor image.
r"""Resize the input PIL Image to the given size.      Args:         img (PIL Image): Image to be resized.         size (sequence or int): Desired output size. If size is a sequence like             (h, w), the output size will be matched to this. If size is an int,             the smaller edge of the image will be matched to this number maintaing             the aspect ratio. i.e, if height > width, then image will be rescaled to             :math:`\left(\text{size} \times \frac{\text{height}}{\text{width}}, \text{size}\right)`         interpolation (int, optional): Desired interpolation. Default is             ``PIL.Image.BILINEAR``      Returns:         PIL Image: Resized image.
r"""Pad the given PIL Image on all sides with specified padding mode and fill value.      Args:         img (PIL Image): Image to be padded.         padding (int or tuple): Padding on each border. If a single int is provided this             is used to pad all borders. If tuple of length 2 is provided this is the padding             on left/right and top/bottom respectively. If a tuple of length 4 is provided             this is the padding for the left, top, right and bottom borders             respectively.         fill: Pixel fill value for constant fill. Default is 0. If a tuple of             length 3, it is used to fill R, G, B channels respectively.             This value is only used when the padding_mode is constant         padding_mode: Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.              - constant: pads with a constant value, this value is specified with fill              - edge: pads with the last value on the edge of the image              - reflect: pads with reflection of image (without repeating the last value on the edge)                         padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode                        will result in [3, 2, 1, 2, 3, 4, 3, 2]              - symmetric: pads with reflection of image (repeating the last value on the edge)                           padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode                          will result in [2, 1, 1, 2, 3, 4, 4, 3]      Returns:         PIL Image: Padded image.
Crop the given PIL Image.      Args:         img (PIL Image): Image to be cropped.         i (int): i in (i,j) i.e coordinates of the upper left corner.         j (int): j in (i,j) i.e coordinates of the upper left corner.         h (int): Height of the cropped image.         w (int): Width of the cropped image.      Returns:         PIL Image: Cropped image.
Crop the given PIL Image and resize it to desired size.      Notably used in :class:`~torchvision.transforms.RandomResizedCrop`.      Args:         img (PIL Image): Image to be cropped.         i (int): i in (i,j) i.e coordinates of the upper left corner         j (int): j in (i,j) i.e coordinates of the upper left corner         h (int): Height of the cropped image.         w (int): Width of the cropped image.         size (sequence or int): Desired output size. Same semantics as ``resize``.         interpolation (int, optional): Desired interpolation. Default is             ``PIL.Image.BILINEAR``.     Returns:         PIL Image: Cropped image.
Horizontally flip the given PIL Image.      Args:         img (PIL Image): Image to be flipped.      Returns:         PIL Image:  Horizontall flipped image.
Perform perspective transform of the given PIL Image.      Args:         img (PIL Image): Image to be transformed.         coeffs (tuple) : 8-tuple (a, b, c, d, e, f, g, h) which contains the coefficients.                             for a perspective transform.         interpolation: Default- Image.BICUBIC     Returns:         PIL Image:  Perspectively transformed Image.
Vertically flip the given PIL Image.      Args:         img (PIL Image): Image to be flipped.      Returns:         PIL Image:  Vertically flipped image.
Crop the given PIL Image into four corners and the central crop.      .. Note::         This transform returns a tuple of images and there may be a         mismatch in the number of inputs and targets your ``Dataset`` returns.      Args:        size (sequence or int): Desired output size of the crop. If size is an            int instead of sequence like (h, w), a square crop (size, size) is            made.      Returns:        tuple: tuple (tl, tr, bl, br, center)                 Corresponding top left, top right, bottom left, bottom right and center crop.
Adjust brightness of an Image.      Args:         img (PIL Image): PIL Image to be adjusted.         brightness_factor (float):  How much to adjust the brightness. Can be             any non negative number. 0 gives a black image, 1 gives the             original image while 2 increases the brightness by a factor of 2.      Returns:         PIL Image: Brightness adjusted image.
Adjust contrast of an Image.      Args:         img (PIL Image): PIL Image to be adjusted.         contrast_factor (float): How much to adjust the contrast. Can be any             non negative number. 0 gives a solid gray image, 1 gives the             original image while 2 increases the contrast by a factor of 2.      Returns:         PIL Image: Contrast adjusted image.
Adjust color saturation of an image.      Args:         img (PIL Image): PIL Image to be adjusted.         saturation_factor (float):  How much to adjust the saturation. 0 will             give a black and white image, 1 will give the original image while             2 will enhance the saturation by a factor of 2.      Returns:         PIL Image: Saturation adjusted image.
Adjust hue of an image.      The image hue is adjusted by converting the image to HSV and     cyclically shifting the intensities in the hue channel (H).     The image is then converted back to original image mode.      `hue_factor` is the amount of shift in H channel and must be in the     interval `[-0.5, 0.5]`.      See `Hue`_ for more details.      .. _Hue: https://en.wikipedia.org/wiki/Hue      Args:         img (PIL Image): PIL Image to be adjusted.         hue_factor (float):  How much to shift the hue channel. Should be in             [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in             HSV space in positive and negative direction respectively.             0 means no shift. Therefore, both -0.5 and 0.5 will give an image             with complementary colors while 0 gives the original image.      Returns:         PIL Image: Hue adjusted image.
r"""Perform gamma correction on an image.      Also known as Power Law Transform. Intensities in RGB mode are adjusted     based on the following equation:      .. math::         I_{\text{out}} = 255 \times \text{gain} \times \left(\frac{I_{\text{in}}}{255}\right)^{\gamma}      See `Gamma Correction`_ for more details.      .. _Gamma Correction: https://en.wikipedia.org/wiki/Gamma_correction      Args:         img (PIL Image): PIL Image to be adjusted.         gamma (float): Non negative real number, same as :math:`\gamma` in the equation.             gamma larger than 1 make the shadows darker,             while gamma smaller than 1 make dark regions lighter.         gain (float): The constant multiplier.
Rotate the image by angle.       Args:         img (PIL Image): PIL Image to be rotated.         angle (float or int): In degrees degrees counter clockwise order.         resample (``PIL.Image.NEAREST`` or ``PIL.Image.BILINEAR`` or ``PIL.Image.BICUBIC``, optional):             An optional resampling filter. See `filters`_ for more information.             If omitted, or if the image has mode "1" or "P", it is set to ``PIL.Image.NEAREST``.         expand (bool, optional): Optional expansion flag.             If true, expands the output image to make it large enough to hold the entire rotated image.             If false or omitted, make the output image the same size as the input image.             Note that the expand flag assumes rotation around the center and no translation.         center (2-tuple, optional): Optional center of rotation.             Origin is the upper left corner.             Default is the center of the image.      .. _filters: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters
Apply affine transformation on the image keeping image center invariant      Args:         img (PIL Image): PIL Image to be rotated.         angle (float or int): rotation angle in degrees between -180 and 180, clockwise direction.         translate (list or tuple of integers): horizontal and vertical translations (post-rotation translation)         scale (float): overall scale         shear (float): shear angle value in degrees between -180 to 180, clockwise direction.         resample (``PIL.Image.NEAREST`` or ``PIL.Image.BILINEAR`` or ``PIL.Image.BICUBIC``, optional):             An optional resampling filter.             See `filters`_ for more information.             If omitted, or if the image has mode "1" or "P", it is set to ``PIL.Image.NEAREST``.         fillcolor (int): Optional fill color for the area outside the transform in the output image. (Pillow>=5.0.0)
Convert image to grayscale version of image.      Args:         img (PIL Image): Image to be converted to grayscale.      Returns:         PIL Image: Grayscale version of the image.             if num_output_channels = 1 : returned image is single channel              if num_output_channels = 3 : returned image is 3 channel with r = g = b
Save a given Tensor into an image file.      Args:         tensor (Tensor or list): Image to be saved. If given a mini-batch tensor,             saves the tensor as a grid of images by calling ``make_grid``.         **kwargs: Other arguments are documented in ``make_grid``.
Finds the class folders in a dataset.          Args:             dir (string): Root directory path.          Returns:             tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.          Ensures:             No class is a subdirectory of another.
Return a Tensor containing the patches
Return a Tensor containing the list of labels        Read the file and keep only the ID of the 3D point.
Return a Tensor containing the ground truth matches        Read the file and keep only 3D point ID.        Matches are represented with a 1, non matches with a 0.
Computes the accuracy over the k top predictions for the specified values of k
This function disables printing when not in master process
Download a file from a url and place it in root.      Args:         url (str): URL to download file from         root (str): Directory to place downloaded file in         filename (str, optional): Name to save the file under. If None, use the basename of the URL         md5 (str, optional): MD5 checksum of the download. If None, do not check
List all directories at a given root      Args:         root (str): Path to directory whose folders need to be listed         prefix (bool, optional): If true, prepends the path to each result, otherwise             only returns the name of the directories found
List all files ending with a suffix at a given root      Args:         root (str): Path to directory whose folders need to be listed         suffix (str or tuple): Suffix of the files to match, e.g. '.png' or ('.jpg', '.png').             It uses the Python "str.endswith" method and is passed directly         prefix (bool, optional): If true, prepends the path to each result, otherwise             only returns the name of the files found
Download a Google Drive file from  and place it in root.      Args:         file_id (str): id of file to be downloaded         root (str): Directory to place downloaded file in         filename (str, optional): Name to save the file under. If None, use the id of the file.         md5 (str, optional): MD5 checksum of the download. If None, do not check
Get parameters for ``crop`` for a random crop.          Args:             img (PIL Image): Image to be cropped.             output_size (tuple): Expected output size of the crop.          Returns:             tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.
Get parameters for ``perspective`` for a random perspective transform.          Args:             width : width of the image.             height : height of the image.          Returns:             List containing [top-left, top-right, bottom-right, bottom-left] of the orignal image,             List containing [top-left, top-right, bottom-right, bottom-left] of the transformed image.
Get parameters for ``crop`` for a random sized crop.          Args:             img (PIL Image): Image to be cropped.             scale (tuple): range of size of the origin size cropped             ratio (tuple): range of aspect ratio of the origin aspect ratio cropped          Returns:             tuple: params (i, j, h, w) to be passed to ``crop`` for a random                 sized crop.
Get a randomized transform to be applied on image.          Arguments are same as that of __init__.          Returns:             Transform which randomly adjusts brightness, contrast and             saturation in a random order.
Get parameters for affine transformation          Returns:             sequence: params to be passed to the affine transformation
Download and extract the tarball, and download each individual photo.
Download the MNIST data if it doesn't exist in processed_folder already.
Download the EMNIST data if it doesn't exist in processed_folder already.
Returns theme name.      Checks in this order:     1. override     2. cookies     3. settings
Return autocompleter results
Render preferences page && save user preferences
Returns available themes list.
check if the searchQuery contain a bang, and create fitting autocompleter results
remove first and last lines to get only json
Embeds a custom gradient into a `Tensor`.    This function works by clever application of `stop_gradient`. I.e., observe   that:    ```none   h(x) = stop_gradient(f(x)) + stop_gradient(g(x)) * (x - stop_gradient(x))   ```    is such that `h(x) == stop_gradient(f(x))` and   `grad[h(x), x] == stop_gradient(g(x)).`    In addition to scalar-domain/scalar-range functions, this function also   supports tensor-domain/scalar-range functions.    Partial Custom Gradient:    Suppose `h(x) = htilde(x, y)`. Note that `dh/dx = stop(g(x))` but `dh/dy =   None`. This is because a `Tensor` cannot have only a portion of its gradient   stopped. To circumvent this issue, one must manually `stop_gradient` the   relevant portions of `f`, `g`. For example see the unit-test,   `test_works_correctly_fx_gx_manually_stopped`.    Args:     fx: `Tensor`. Output of function evaluated at `x`.     gx: `Tensor` or list of `Tensor`s. Gradient of function at (each) `x`.     x: `Tensor` or list of `Tensor`s. Args of evaluation for `f`.     fx_gx_manually_stopped: Python `bool` indicating that `fx`, `gx` manually       have `stop_gradient` applied.     name: Python `str` name prefixed to Ops created by this function.    Returns:     fx: Floating-type `Tensor` equal to `f(x)` but which has gradient       `stop_gradient(g(x))`.
Convenience function to efficiently construct a MultivariateNormalDiag.
Eight-schools joint log-prob.
Runs HMC on the eight-schools unnormalized posterior.
Decorator to programmatically expand the docstring.    Args:     **kwargs: Keyword arguments to set. For each key-value pair `k` and `v`,       the key is found as `${k}` in the docstring and replaced with `v`.    Returns:     Decorated function.
Infer the original name passed into a distribution constructor.    Distributions typically follow the pattern of   with.name_scope(name) as name:     super(name=name)   so we attempt to reverse the name-scope transformation to allow   addressing of RVs by the distribution's original, user-visible   name kwarg.    Args:     distribution: a tfd.Distribution instance.   Returns:     simple_name: the original name passed into the Distribution.    #### Example    ```   d1 = tfd.Normal(0., 1., name='x') # d1.name = 'x/'   d2 = tfd.Normal(0., 1., name='x') # d2.name = 'x_2/'   _simple_name(d2) # returns 'x'    ```
RandomVariable constructor with a dummy name argument.
Wrap an existing distribution as a traceable random variable.    This enables the use of custom or user-provided distributions in   Edward models. Unlike a bare `RandomVariable` object, this method   wraps the constructor so it is included in the Edward trace and its   values can be properly intercepted and overridden.    Where possible, you should prefer the built-in constructors   (`ed.Normal`, etc); these simultaneously construct a Distribution   and a RandomVariable object so that the distribution parameters   themselves may be intercepted and overridden. RVs constructed via   `as_random_variable()` have a fixed distribution and may not support   program transformations (e.g, conjugate marginalization) that rely   on overriding distribution parameters.    Args:     distribution: tfd.Distribution governing the distribution of the random       variable, such as sampling and log-probabilities.     sample_shape: tf.TensorShape of samples to draw from the random variable.       Default is `()` corresponding to a single sample.     value: Fixed tf.Tensor to associate with random variable. Must have shape       `sample_shape + distribution.batch_shape + distribution.event_shape`.       Default is to sample from random variable according to `sample_shape`.    Returns:     rv: a `RandomVariable` wrapping the provided distribution.    #### Example    ```python   from tensorflow_probability import distributions as tfd   from tensorflow_probability import edward2 as ed    def model():     # equivalent to ed.Normal(0., 1., name='x')     return ed.as_random_variable(tfd.Normal(0., 1., name='x'))    log_joint = ed.make_log_joint_fn(model)   output = log_joint(x=2.)   ```
Factory function to make random variable given distribution class.
Compute one-step-ahead predictive distributions for all timesteps.    Given samples from the posterior over parameters, return the predictive   distribution over observations at each time `T`, given observations up   through time `T-1`.    Args:     model: An instance of `StructuralTimeSeries` representing a       time-series model. This represents a joint distribution over       time-series and their parameters with batch shape `[b1, ..., bN]`.     observed_time_series: `float` `Tensor` of shape       `concat([sample_shape, model.batch_shape, [num_timesteps, 1]]) where       `sample_shape` corresponds to i.i.d. observations, and the trailing `[1]`       dimension may (optionally) be omitted if `num_timesteps > 1`. May       optionally be an instance of `tfp.sts.MaskedTimeSeries` including a       mask `Tensor` to encode the locations of missing observations.     parameter_samples: Python `list` of `Tensors` representing posterior samples       of model parameters, with shapes `[concat([[num_posterior_draws],       param.prior.batch_shape, param.prior.event_shape]) for param in       model.parameters]`. This may optionally also be a map (Python `dict`) of       parameter names to `Tensor` values.    Returns:     forecast_dist: a `tfd.MixtureSameFamily` instance with event shape       [num_timesteps] and       batch shape `concat([sample_shape, model.batch_shape])`, with       `num_posterior_draws` mixture components. The `t`th step represents the       forecast distribution `p(observed_time_series[t] |       observed_time_series[0:t-1], parameter_samples)`.    #### Examples    Suppose we've built a model and fit it to data using HMC:    ```python     day_of_week = tfp.sts.Seasonal(         num_seasons=7,         observed_time_series=observed_time_series,         name='day_of_week')     local_linear_trend = tfp.sts.LocalLinearTrend(         observed_time_series=observed_time_series,         name='local_linear_trend')     model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],                         observed_time_series=observed_time_series)      samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)   ```    Passing the posterior samples into `one_step_predictive`, we construct a   one-step-ahead predictive distribution:    ```python     one_step_predictive_dist = tfp.sts.one_step_predictive(       model, observed_time_series, parameter_samples=samples)      predictive_means = one_step_predictive_dist.mean()     predictive_scales = one_step_predictive_dist.stddev()   ```    If using variational inference instead of HMC, we'd construct a forecast using   samples from the variational posterior:    ```python     (variational_loss,      variational_distributions) = tfp.sts.build_factored_variational_loss(        model=model, observed_time_series=observed_time_series)      # OMITTED: take steps to optimize variational loss      samples = {k: q.sample(30) for (k, q) in variational_distributions.items()}     one_step_predictive_dist = tfp.sts.one_step_predictive(       model, observed_time_series, parameter_samples=samples)   ```    We can visualize the forecast by plotting:    ```python     from matplotlib import pylab as plt     def plot_one_step_predictive(observed_time_series,                                  forecast_mean,                                  forecast_scale):       plt.figure(figsize=(12, 6))       num_timesteps = forecast_mean.shape[-1]       c1, c2 = (0.12, 0.47, 0.71), (1.0, 0.5, 0.05)       plt.plot(observed_time_series, label="observed time series", color=c1)       plt.plot(forecast_mean, label="one-step prediction", color=c2)       plt.fill_between(np.arange(num_timesteps),                        forecast_mean - 2 * forecast_scale,                        forecast_mean + 2 * forecast_scale,                        alpha=0.1, color=c2)       plt.legend()      plot_one_step_predictive(observed_time_series,                              forecast_mean=predictive_means,                              forecast_scale=predictive_scales)   ```    To detect anomalous timesteps, we check whether the observed value at each   step is within a 95% predictive interval, i.e., two standard deviations from   the mean:    ```python     z_scores = ((observed_time_series[..., 1:] - predictive_means[..., :-1])                  / predictive_scales[..., :-1])     anomalous_timesteps = tf.boolean_mask(         tf.range(1, num_timesteps),         tf.abs(z_scores) > 2.0)   ```
Construct predictive distribution over future observations.    Given samples from the posterior over parameters, return the predictive   distribution over future observations for num_steps_forecast timesteps.    Args:     model: An instance of `StructuralTimeSeries` representing a       time-series model. This represents a joint distribution over       time-series and their parameters with batch shape `[b1, ..., bN]`.     observed_time_series: `float` `Tensor` of shape       `concat([sample_shape, model.batch_shape, [num_timesteps, 1]])` where       `sample_shape` corresponds to i.i.d. observations, and the trailing `[1]`       dimension may (optionally) be omitted if `num_timesteps > 1`. May       optionally be an instance of `tfp.sts.MaskedTimeSeries` including a       mask `Tensor` to encode the locations of missing observations.     parameter_samples: Python `list` of `Tensors` representing posterior samples       of model parameters, with shapes `[concat([[num_posterior_draws],       param.prior.batch_shape, param.prior.event_shape]) for param in       model.parameters]`. This may optionally also be a map (Python `dict`) of       parameter names to `Tensor` values.     num_steps_forecast: scalar `int` `Tensor` number of steps to forecast.    Returns:     forecast_dist: a `tfd.MixtureSameFamily` instance with event shape       [num_steps_forecast, 1] and batch shape       `concat([sample_shape, model.batch_shape])`, with `num_posterior_draws`       mixture components.    #### Examples    Suppose we've built a model and fit it to data using HMC:    ```python     day_of_week = tfp.sts.Seasonal(         num_seasons=7,         observed_time_series=observed_time_series,         name='day_of_week')     local_linear_trend = tfp.sts.LocalLinearTrend(         observed_time_series=observed_time_series,         name='local_linear_trend')     model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],                         observed_time_series=observed_time_series)      samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)   ```    Passing the posterior samples into `forecast`, we construct a forecast   distribution:    ```python     forecast_dist = tfp.sts.forecast(model, observed_time_series,                                      parameter_samples=samples,                                      num_steps_forecast=50)      forecast_mean = forecast_dist.mean()[..., 0]  # shape: [50]     forecast_scale = forecast_dist.stddev()[..., 0]  # shape: [50]     forecast_samples = forecast_dist.sample(10)[..., 0]  # shape: [10, 50]   ```    If using variational inference instead of HMC, we'd construct a forecast using   samples from the variational posterior:    ```python     (variational_loss,      variational_distributions) = tfp.sts.build_factored_variational_loss(        model=model, observed_time_series=observed_time_series)      # OMITTED: take steps to optimize variational loss      samples = {k: q.sample(30) for (k, q) in variational_distributions.items()}     forecast_dist = tfp.sts.forecast(model, observed_time_series,                                          parameter_samples=samples,                                          num_steps_forecast=50)   ```    We can visualize the forecast by plotting:    ```python     from matplotlib import pylab as plt     def plot_forecast(observed_time_series,                       forecast_mean,                       forecast_scale,                       forecast_samples):       plt.figure(figsize=(12, 6))        num_steps = observed_time_series.shape[-1]       num_steps_forecast = forecast_mean.shape[-1]       num_steps_train = num_steps - num_steps_forecast        c1, c2 = (0.12, 0.47, 0.71), (1.0, 0.5, 0.05)       plt.plot(np.arange(num_steps), observed_time_series,                lw=2, color=c1, label='ground truth')        forecast_steps = np.arange(num_steps_train,                        num_steps_train+num_steps_forecast)       plt.plot(forecast_steps, forecast_samples.T, lw=1, color=c2, alpha=0.1)       plt.plot(forecast_steps, forecast_mean, lw=2, ls='--', color=c2,                label='forecast')       plt.fill_between(forecast_steps,                        forecast_mean - 2 * forecast_scale,                        forecast_mean + 2 * forecast_scale, color=c2, alpha=0.2)        plt.xlim([0, num_steps])       plt.legend()      plot_forecast(observed_time_series,                   forecast_mean=forecast_mean,                   forecast_scale=forecast_scale,                   forecast_samples=forecast_samples)   ```
Returns `max` or `mask` if `max` is not finite.
Assert all elements of `x` are finite.    Args:     x:  Numeric `Tensor`.     data:  The tensors to print out if the condition is False.  Defaults to       error message and first few entries of `x`.     summarize: Print this many entries of each tensor.     message: A string to prefix to the default message.     name: A name for this operation (optional).       Defaults to "assert_finite".    Returns:     Op raising `InvalidArgumentError` unless `x` has specified rank or lower.     If static checks determine `x` has correct rank, a `no_op` is returned.    Raises:     ValueError:  If static checks determine `x` has wrong rank.
Assert `x` has rank equal to `rank` or smaller.    Example of adding a dependency to an operation:    ```python   with tf.control_dependencies([tf.assert_rank_at_most(x, 2)]):     output = tf.reduce_sum(x)   ```    Args:     x:  Numeric `Tensor`.     rank:  Scalar `Tensor`.     data:  The tensors to print out if the condition is False.  Defaults to       error message and first few entries of `x`.     summarize: Print this many entries of each tensor.     message: A string to prefix to the default message.     name: A name for this operation (optional).       Defaults to "assert_rank_at_most".    Returns:     Op raising `InvalidArgumentError` unless `x` has specified rank or lower.     If static checks determine `x` has correct rank, a `no_op` is returned.    Raises:     ValueError:  If static checks determine `x` has wrong rank.
Computes the number of elements in a tensor with shape `event_shape`.    Args:     event_shape: A tensor shape.     name: The name to use for the tensor op to compute the number of elements       (if such an op needs to be created).    Returns:     event_size: The number of elements in `tensor_shape`.  Returns a numpy int     when the number of elements can be computed immediately.  Otherwise, returns     a scalar tensor.
OneHotCategorical helper computing probs, cdf, etc over its support.
Return a convert-to-tensor func, given a name, config, callable, etc.
Number of `params` needed to create a `MixtureSameFamily` distribution.      Arguments:       num_components: Number of component distributions in the mixture         distribution.       component_params_size: Number of parameters needed to create a single         component distribution.       name: The name to use for the op to compute the number of parameters         (if such an op needs to be created).      Returns:      params_size: The number of parameters needed to create the mixture        distribution.
Yields the top-most interceptor on the thread-local interceptor stack.    Operations may be intercepted by multiple nested interceptors. Once reached,   an operation can be forwarded through nested interceptors until resolved.   To allow for nesting, implement interceptors by re-wrapping their first   argument (`f`) as an `interceptable`. To avoid nesting, manipulate the   computation without using `interceptable`.    This function allows for nesting by manipulating the thread-local interceptor   stack, so that operations are intercepted in the order of interceptor nesting.    #### Examples    ```python   from tensorflow_probability import edward2 as ed    def model():     x = ed.Normal(loc=0., scale=1., name="x")     y = ed.Normal(loc=x, scale=1., name="y")     return x + y    def double(f, *args, **kwargs):     return 2. * interceptable(f)(*args, **kwargs)    def set_y(f, *args, **kwargs):     if kwargs.get("name") == "y":       kwargs["value"] = 0.42     return interceptable(f)(*args, **kwargs)    with interception(double):     with interception(set_y):       z = model()   ```    This will firstly put `double` on the stack, and then `set_y`,   resulting in the stack:   (TOP) set_y -> double -> apply (BOTTOM)    The execution of `model` is then (top lines are current stack state):   1) (TOP) set_y -> double -> apply (BOTTOM);   `ed.Normal(0., 1., "x")` is intercepted by `set_y`, and as the name is not "y"   the operation is simply forwarded to the next interceptor on the stack.    2) (TOP) double -> apply (BOTTOM);   `ed.Normal(0., 1., "x")` is intercepted by `double`, to produce   `2*ed.Normal(0., 1., "x")`, with the operation being forwarded down the stack.    3) (TOP) apply (BOTTOM);   `ed.Normal(0., 1., "x")` is intercepted by `apply`, which simply calls the   constructor.    (At this point, the nested calls to `get_next_interceptor()`, produced by   forwarding operations, exit, and the current stack is again:   (TOP) set_y -> double -> apply (BOTTOM))    4) (TOP) set_y -> double -> apply (BOTTOM);   `ed.Normal(0., 1., "y")` is intercepted by `set_y`,   the value of `y` is set to 0.42 and the operation is forwarded down the stack.    5) (TOP) double -> apply (BOTTOM);   `ed.Normal(0., 1., "y")` is intercepted by `double`, to produce   `2*ed.Normal(0., 1., "y")`, with the operation being forwarded down the stack.    6) (TOP) apply (BOTTOM);   `ed.Normal(0., 1., "y")` is intercepted by `apply`, which simply calls the   constructor.    The final values for `x` and `y` inside of `model()` are tensors where `x` is   a random draw from Normal(0., 1.) doubled, and `y` is a constant 0.84, thus   z = 2 * Normal(0., 1.) + 0.84.
Decorator that wraps `func` so that its execution is intercepted.    The wrapper passes `func` to the interceptor for the current thread.    If there is no next interceptor, we perform an "immediate" call to `func`.   That is, `func` terminates without forwarding its execution to another   interceptor.    Args:     func: Function to wrap.    Returns:     The decorated function.
Context manager for recording interceptable executions onto a tape.    Similar to `tf.GradientTape`, operations are recorded if they are executed   within this context manager. In addition, the operation must be registered   (wrapped) as `ed.interceptable`.    Yields:     tape: OrderedDict where operations are recorded in sequence. Keys are       the `name` keyword argument to the operation (typically, a random       variable's `name`) and values are the corresponding output of the       operation. If the operation has no name, it is not recorded.    #### Examples    ```python   from tensorflow_probability import edward2 as ed    def probabilistic_matrix_factorization():     users = ed.Normal(0., 1., sample_shape=[5000, 128], name="users")     items = ed.Normal(0., 1., sample_shape=[7500, 128], name="items")     ratings = ed.Normal(loc=tf.matmul(users, items, transpose_b=True),                         scale=0.1,                         name="ratings")     return ratings    with ed.tape() as model_tape:     ratings = probabilistic_matrix_factorization()    assert model_tape["users"].shape == (5000, 128)   assert model_tape["items"].shape == (7500, 128)   assert model_tape["ratings"] == ratings   ```
Generates synthetic data for binary classification.    Args:     num_examples: The number of samples to generate (scalar Python `int`).     input_size: The input space dimension (scalar Python `int`).     weights_prior_stddev: The prior standard deviation of the weight       vector. (scalar Python `float`).    Returns:     random_weights: Sampled weights as a Numpy `array` of shape       `[input_size]`.     random_bias: Sampled bias as a scalar Python `float`.     design_matrix: Points sampled uniformly from the cube `[-1,        1]^{input_size}`, as a Numpy `array` of shape `(num_examples,        input_size)`.     labels: Labels sampled from the logistic model `p(label=1) =       logistic(dot(features, random_weights) + random_bias)`, as a Numpy       `int32` `array` of shape `(num_examples, 1)`.
Utility method to visualize decision boundaries in R^2.    Args:     features: Input points, as a Numpy `array` of shape `[num_examples, 2]`.     labels: Numpy `float`-like array of shape `[num_examples, 1]` giving a       label for each point.     true_w_b: A `tuple` `(w, b)` where `w` is a Numpy array of        shape `[2]` and `b` is a scalar `float`, interpreted as a        decision rule of the form `dot(features, w) + b > 0`.     candidate_w_bs: Python `iterable` containing tuples of the same form as        true_w_b.     fname: The filename to save the plot as a PNG image (Python `str`).
Build a Dataset iterator for supervised classification.    Args:     x: Numpy `array` of features, indexed by the first dimension.     y: Numpy `array` of labels, with the same first dimension as `x`.     batch_size: Number of elements in each training batch.    Returns:     batch_features: `Tensor` feed  features, of shape       `[batch_size] + x.shape[1:]`.     batch_labels: `Tensor` feed of labels, of shape       `[batch_size] + y.shape[1:]`.
Validate `map_values` if `validate_args`==True.
`TransitionOperator` that runs `fn` repeatedly and traces its outputs.    Args:     state: A nest of `Tensor`s or None.     fn: A `TransitionOperator`.     num_steps: Number of steps to run the function for. Must be greater than 1.     trace_fn: Callable that the unpacked outputs of `fn` and returns a nest of       `Tensor`s. These will be stacked and returned.    Returns:     state: The final state returned by `fn`.     traces: Stacked outputs of `trace_fn`.
Calls a transition operator with args, unpacking args if its a sequence.    Args:     fn: A `TransitionOperator`.     args: Arguments to `fn`    Returns:     ret: Return value of `fn`.
Calls `fn` and returns the gradients with respect to `fn`'s first output.    Args:     fn: A `TransitionOperator`.     args: Arguments to `fn`    Returns:     ret: First output of `fn`.     extra: Second output of `fn`.     grads: Gradients of `ret` with respect to `args`.
Maybe broadcasts `from_structure` to `to_structure`.    If `from_structure` is a singleton, it is tiled to match the structure of   `to_structure`. Note that the elements in `from_structure` are not copied if   this tiling occurs.    Args:     from_structure: A structure.     to_structure: A structure.    Returns:     new_from_structure: Same structure as `to_structure`.
Transforms a log-prob function using a bijector.    This takes a log-prob function and creates a new log-prob function that now   takes takes state in the domain of the bijector, forward transforms that state   and calls the original log-prob function. It then returns the log-probability   that correctly accounts for this transformation.    The forward-transformed state is pre-pended to the original log-prob   function's extra returns and returned as the new extra return.    For convenience you can also pass the initial state (in the original space),   and this function will return the inverse transformed as the 2nd return value.   You'd use this to initialize MCMC operators that operate in the transformed   space.    Args:     log_prob_fn: Log prob fn.     bijector: Bijector(s), must be of the same structure as the `log_prob_fn`       inputs.     init_state: Initial state, in the original space.    Returns:     transformed_log_prob_fn: Transformed log prob fn.     transformed_init_state: If `init_state` is provided. Initial state in the       transformed space.
Leapfrog `TransitionOperator`.    Args:     leapfrog_step_state: LeapFrogStepState.     step_size: Step size, structure broadcastable to the `target_log_prob_fn`       state.     target_log_prob_fn: Target log prob fn.     kinetic_energy_fn: Kinetic energy fn.    Returns:     leapfrog_step_state: LeapFrogStepState.     leapfrog_step_extras: LeapFrogStepExtras.
Metropolis-Hastings step.    This probabilistically chooses between `current_state` and `proposed_state`   based on the `energy_change` so as to preserve detailed balance.    Energy change is the negative of `log_accept_ratio`.    Args:     current_state: Current state.     proposed_state: Proposed state.     energy_change: E(proposed_state) - E(previous_state).     seed: For reproducibility.    Returns:     new_state: The chosen state.     is_accepted: Whether the proposed state was accepted.     log_uniform: The random number that was used to select between the two       states.
Hamiltonian Monte Carlo `TransitionOperator`.    #### Example    ```python   step_size = 0.2   num_steps = 2000   num_leapfrog_steps = 10   state = tf.ones([16, 2])    base_mean = [1., 0]   base_cov = [[1, 0.5], [0.5, 1]]    bijector = tfb.Softplus()   base_dist = tfd.MultivariateNormalFullCovariance(       loc=base_mean, covariance_matrix=base_cov)   target_dist = bijector(base_dist)    def orig_target_log_prob_fn(x):     return target_dist.log_prob(x), ()    target_log_prob_fn, state = fun_mcmc.transform_log_prob_fn(       orig_target_log_prob_fn, bijector, state)    kernel = tf.function(lambda state: fun_mcmc.hamiltonian_monte_carlo(       state,       step_size=step_size,       num_leapfrog_steps=num_leapfrog_steps,       target_log_prob_fn=target_log_prob_fn,       seed=tfp_test_util.test_seed()))    _, chain = fun_mcmc.trace(       state=fun_mcmc.HamiltonianMonteCarloState(           state=state,           state_grads=None,           target_log_prob=None,           state_extra=None),       fn=kernel,       num_steps=num_steps,       trace_fn=lambda state, extra: state.state_extra[0])   ```    Args:     hmc_state: HamiltonianMonteCarloState.     target_log_prob_fn: Target log prob fn.     step_size: Step size, structure broadcastable to the `target_log_prob_fn`       state.     num_leapfrog_steps: Number of leapfrog steps to take.     momentum: Initial momentum, passed to `momentum_sample_fn`. Default: zeroes.     kinetic_energy_fn: Kinetic energy function.     momentum_sample_fn: Sampler for the momentum.     leapfrog_trace_fn: Trace function for the leapfrog integrator.     seed: For reproducibility.    Returns:     hmc_state: HamiltonianMonteCarloState     hmc_extra: HamiltonianMonteCarloExtra
A function to do simple sign-based control of a variable.    ```   control = control * (1. + adaptation_rate) ** sign(output - set_point)   ```    Args:     control: The control variable.     output: The output variable.     set_point: The set point for `output`. This function will adjust `control`       so that `output` matches `set_point`.     adaptation_rate: Adaptation rate.    Returns:     control: New control.
Creates a layer from its config.      This method is the reverse of `get_config`, capable of instantiating the     same layer from the config dictionary.      Args:       config: A Python dictionary, typically the output of `get_config`.      Returns:       layer: A layer instance.
Convenience to convert to `Tensor` or leave as `None`.
Construct `scale` from various components.      Args:       identity_multiplier: floating point rank 0 `Tensor` representing a scaling         done to the identity matrix.       diag: Floating-point `Tensor` representing the diagonal matrix.`diag` has         shape `[N1, N2, ...  k]`, which represents a k x k diagonal matrix.       tril: Floating-point `Tensor` representing the lower triangular matrix.        `tril` has shape `[N1, N2, ...  k, k]`, which represents a k x k lower        triangular matrix.       perturb_diag: Floating-point `Tensor` representing the diagonal matrix of         the low rank update.       perturb_factor: Floating-point `Tensor` representing factor matrix.       shift: Floating-point `Tensor` representing `shift in `scale @ X + shift`.       validate_args: Python `bool` indicating whether arguments should be         checked for correctness.       dtype: `DType` for arg `Tensor` conversions.      Returns:       scale. In the case of scaling by a constant, scale is a       floating point `Tensor`. Otherwise, scale is a `LinearOperator`.      Raises:       ValueError: if all of `tril`, `diag` and `identity_multiplier` are `None`.
Returns a callable that adds a random normal perturbation to the input.    This function returns a callable that accepts a Python `list` of `Tensor`s of   any shapes and `dtypes`  representing the state parts of the `current_state`   and a random seed. The supplied argument `scale` must be a `Tensor` or Python   `list` of `Tensor`s representing the scale of the generated   proposal. `scale` must broadcast with the state parts of `current_state`.   The callable adds a sample from a zero-mean normal distribution with the   supplied scales to each state part and returns a same-type `list` of `Tensor`s   as the state parts of `current_state`.    Args:     scale: a `Tensor` or Python `list` of `Tensor`s of any shapes and `dtypes`       controlling the scale of the normal proposal distribution.     name: Python `str` name prefixed to Ops created by this function.         Default value: 'random_walk_normal_fn'.    Returns:     random_walk_normal_fn: A callable accepting a Python `list` of `Tensor`s       representing the state parts of the `current_state` and an `int`       representing the random seed to be used to generate the proposal. The       callable returns the same-type `list` of `Tensor`s as the input and       represents the proposal for the RWM algorithm.
Returns a callable that adds a random uniform perturbation to the input.    For more details on `random_walk_uniform_fn`, see   `random_walk_normal_fn`. `scale` might   be a `Tensor` or a list of `Tensor`s that should broadcast with state parts   of the `current_state`. The generated uniform perturbation is sampled as a   uniform point on the rectangle `[-scale, scale]`.    Args:     scale: a `Tensor` or Python `list` of `Tensor`s of any shapes and `dtypes`       controlling the upper and lower bound of the uniform proposal       distribution.     name: Python `str` name prefixed to Ops created by this function.         Default value: 'random_walk_uniform_fn'.    Returns:     random_walk_uniform_fn: A callable accepting a Python `list` of `Tensor`s       representing the state parts of the `current_state` and an `int`       representing the random seed used to generate the proposal. The callable       returns the same-type `list` of `Tensor`s as the input and represents the       proposal for the RWM algorithm.
Expand the rank of x up to static_event_rank times for broadcasting.      The static event rank was checked to not be None at construction time.      Args:       x: A tensor to expand.     Returns:       The expanded tensor.
r"""A lower bound on the entropy of this mixture model.      The bound below is not always very tight, and its usefulness depends     on the mixture probabilities and the components in use.      A lower bound is useful for ELBO when the `Mixture` is the variational     distribution:      \\(     \log p(x) >= ELBO = \int q(z) \log p(x, z) dz + H[q]     \\)      where \\( p \\) is the prior distribution, \\( q \\) is the variational,     and \\( H[q] \\) is the entropy of \\( q \\). If there is a lower bound     \\( G[q] \\) such that \\( H[q] \geq G[q] \\) then it can be used in     place of \\( H[q] \\).      For a mixture of distributions \\( q(Z) = \sum_i c_i q_i(Z) \\) with     \\( \sum_i c_i = 1 \\), by the concavity of \\( f(x) = -x \log x \\), a     simple lower bound is:      \\(     \begin{align}     H[q] & = - \int q(z) \log q(z) dz \\\        & = - \int (\sum_i c_i q_i(z)) \log(\sum_i c_i q_i(z)) dz \\\        & \geq - \sum_i c_i \int q_i(z) \log q_i(z) dz \\\        & = \sum_i c_i H[q_i]     \end{align}     \\)      This is the term we calculate below for \\( G[q] \\).      Args:       name: A name for this operation (optional).      Returns:       A lower bound on the Mixture's entropy.
Get a list of num_components batchwise probabilities.
Validate `outcomes`, `logits` and `probs`'s shapes.
Attempt to import tensorflow, and ensure its version is sufficient.    Raises:     ImportError: if either tensorflow is not importable or its version is     inadequate.
Bayesian logistic regression, which returns labels given features.
Builds the Covertype data set.
Cholesky factor of the covariance matrix of vector-variate random samples.    This function can be use to fit a multivariate normal to data.    ```python   tf.enable_eager_execution()   import tensorflow_probability as tfp   tfd = tfp.distributions    # Assume data.shape = (1000, 2).  1000 samples of a random variable in R^2.   observed_data = read_data_samples(...)    # The mean is easy   mu = tf.reduce_mean(observed_data, axis=0)    # Get the scale matrix   L = tfp.stats.cholesky_covariance(observed_data)    # Make the best fit multivariate normal (under maximum likelihood condition).   mvn = tfd.MultivariateNormalTriL(loc=mu, scale_tril=L)    # Plot contours of the pdf.   xs, ys = tf.meshgrid(       tf.linspace(-5., 5., 50), tf.linspace(-5., 5., 50), indexing='ij')   xy = tf.stack((tf.reshape(xs, [-1]), tf.reshape(ys, [-1])), axis=-1)   pdf = tf.reshape(mvn.prob(xy), (50, 50))   CS = plt.contour(xs, ys, pdf, 10)   plt.clabel(CS, inline=1, fontsize=10)   ```    Why does this work?   Given vector-variate random variables `X = (X1, ..., Xd)`, one may obtain the   sample covariance matrix in `R^{d x d}` (see `tfp.stats.covariance`).    The [Cholesky factor](https://en.wikipedia.org/wiki/Cholesky_decomposition)   of this matrix is analogous to standard deviation for scalar random variables:   Suppose `X` has covariance matrix `C`, with Cholesky factorization `C = L L^T`   Then multiplying a vector of iid random variables which have unit variance by   `L` produces a vector with covariance `L L^T`, which is the same as `X`.    ```python   observed_data = read_data_samples(...)   L = tfp.stats.cholesky_covariance(observed_data, sample_axis=0)    # Make fake_data with the same covariance as observed_data.   uncorrelated_normal = tf.random_normal(shape=(500, 10))   fake_data = tf.linalg.matvec(L, uncorrelated_normal)   ```    Args:     x:  Numeric `Tensor`.  The rightmost dimension of `x` indexes events. E.g.       dimensions of a random vector.     sample_axis: Scalar or vector `Tensor` designating axis holding samples.       Default value: `0` (leftmost dimension). Cannot be the rightmost dimension         (since this indexes events).     keepdims:  Boolean.  Whether to keep the sample axis as singletons.     name: Python `str` name prefixed to Ops created by this function.           Default value: `None` (i.e., `'covariance'`).    Returns:     chol:  `Tensor` of same `dtype` as `x`.  The last two dimensions hold       lower triangular matrices (the Cholesky factors).
Estimate standard deviation using samples.    Given `N` samples of scalar valued random variable `X`, standard deviation may   be estimated as    ```none   Stddev[X] := Sqrt[Var[X]],   Var[X] := N^{-1} sum_{n=1}^N (X_n - Xbar) Conj{(X_n - Xbar)},   Xbar := N^{-1} sum_{n=1}^N X_n   ```    ```python   x = tf.random_normal(shape=(100, 2, 3))    # stddev[i, j] is the sample standard deviation of the (i, j) batch member.   stddev = tfp.stats.stddev(x, sample_axis=0)   ```    Scaling a unit normal by a standard deviation produces normal samples   with that standard deviation.    ```python   observed_data = read_data_samples(...)   stddev = tfp.stats.stddev(observed_data)    # Make fake_data with the same standard deviation as observed_data.   fake_data = stddev * tf.random_normal(shape=(100,))   ```    Notice we divide by `N` (the numpy default), which does not create `NaN`   when `N = 1`, but is slightly biased.    Args:     x:  A numeric `Tensor` holding samples.     sample_axis: Scalar or vector `Tensor` designating axis holding samples, or       `None` (meaning all axis hold samples).       Default value: `0` (leftmost dimension).     keepdims:  Boolean.  Whether to keep the sample axis as singletons.     name: Python `str` name prefixed to Ops created by this function.           Default value: `None` (i.e., `'stddev'`).    Returns:     stddev: A `Tensor` of same `dtype` as the `x`, and rank equal to       `rank(x) - len(sample_axis)`
Estimate variance using samples.    Given `N` samples of scalar valued random variable `X`, variance may   be estimated as    ```none   Var[X] := N^{-1} sum_{n=1}^N (X_n - Xbar) Conj{(X_n - Xbar)}   Xbar := N^{-1} sum_{n=1}^N X_n   ```    ```python   x = tf.random_normal(shape=(100, 2, 3))    # var[i, j] is the sample variance of the (i, j) batch member of x.   var = tfp.stats.variance(x, sample_axis=0)   ```    Notice we divide by `N` (the numpy default), which does not create `NaN`   when `N = 1`, but is slightly biased.    Args:     x:  A numeric `Tensor` holding samples.     sample_axis: Scalar or vector `Tensor` designating axis holding samples, or       `None` (meaning all axis hold samples).       Default value: `0` (leftmost dimension).     keepdims:  Boolean.  Whether to keep the sample axis as singletons.     name: Python `str` name prefixed to Ops created by this function.           Default value: `None` (i.e., `'variance'`).    Returns:     var: A `Tensor` of same `dtype` as the `x`, and rank equal to       `rank(x) - len(sample_axis)`
Rectify possibly negatively axis. Prefer return Python list.
A version of squeeze that works with dynamic axis.
Standardize input `x` to a unit normal.
Reconstruct input `x` from a its normalized version.
Build the transition matrix for a semi-local linear trend model.
Build the transition noise model for a semi-local linear trend model.
r"""Returns a sample from the `dim` dimensional Halton sequence.    Warning: The sequence elements take values only between 0 and 1. Care must be   taken to appropriately transform the domain of a function if it differs from   the unit cube before evaluating integrals using Halton samples. It is also   important to remember that quasi-random numbers without randomization are not   a replacement for pseudo-random numbers in every context. Quasi random numbers   are completely deterministic and typically have significant negative   autocorrelation unless randomization is used.    Computes the members of the low discrepancy Halton sequence in dimension   `dim`. The `dim`-dimensional sequence takes values in the unit hypercube in   `dim` dimensions. Currently, only dimensions up to 1000 are supported. The   prime base for the k-th axes is the k-th prime starting from 2. For example,   if `dim` = 3, then the bases will be [2, 3, 5] respectively and the first   element of the non-randomized sequence will be: [0.5, 0.333, 0.2]. For a more   complete description of the Halton sequences see   [here](https://en.wikipedia.org/wiki/Halton_sequence). For low discrepancy   sequences and their applications see   [here](https://en.wikipedia.org/wiki/Low-discrepancy_sequence).    If `randomized` is true, this function produces a scrambled version of the   Halton sequence introduced by [Owen (2017)][1]. For the advantages of   randomization of low discrepancy sequences see [here](   https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method#Randomization_of_quasi-Monte_Carlo).    The number of samples produced is controlled by the `num_results` and   `sequence_indices` parameters. The user must supply either `num_results` or   `sequence_indices` but not both.   The former is the number of samples to produce starting from the first   element. If `sequence_indices` is given instead, the specified elements of   the sequence are generated. For example, sequence_indices=tf.range(10) is   equivalent to specifying n=10.    #### Examples    ```python   import tensorflow as tf   import tensorflow_probability as tfp    # Produce the first 1000 members of the Halton sequence in 3 dimensions.   num_results = 1000   dim = 3   sample = tfp.mcmc.sample_halton_sequence(     dim,     num_results=num_results,     seed=127)    # Evaluate the integral of x_1 * x_2^2 * x_3^3  over the three dimensional   # hypercube.   powers = tf.range(1.0, limit=dim + 1)   integral = tf.reduce_mean(tf.reduce_prod(sample ** powers, axis=-1))   true_value = 1.0 / tf.reduce_prod(powers + 1.0)   with tf.Session() as session:     values = session.run((integral, true_value))    # Produces a relative absolute error of 1.7%.   print ("Estimated: %f, True Value: %f" % values)    # Now skip the first 1000 samples and recompute the integral with the next   # thousand samples. The sequence_indices argument can be used to do this.     sequence_indices = tf.range(start=1000, limit=1000 + num_results,                               dtype=tf.int32)   sample_leaped = tfp.mcmc.sample_halton_sequence(       dim,       sequence_indices=sequence_indices,       seed=111217)    integral_leaped = tf.reduce_mean(tf.reduce_prod(sample_leaped ** powers,                                                   axis=-1))   with tf.Session() as session:     values = session.run((integral_leaped, true_value))   # Now produces a relative absolute error of 0.05%.   print ("Leaped Estimated: %f, True Value: %f" % values)   ```    Args:     dim: Positive Python `int` representing each sample's `event_size.` Must       not be greater than 1000.     num_results: (Optional) Positive scalar `Tensor` of dtype int32. The number       of samples to generate. Either this parameter or sequence_indices must       be specified but not both. If this parameter is None, then the behaviour       is determined by the `sequence_indices`.       Default value: `None`.     sequence_indices: (Optional) `Tensor` of dtype int32 and rank 1. The       elements of the sequence to compute specified by their position in the       sequence. The entries index into the Halton sequence starting with 0 and       hence, must be whole numbers. For example, sequence_indices=[0, 5, 6] will       produce the first, sixth and seventh elements of the sequence. If this       parameter is None, then the `num_results` parameter must be specified       which gives the number of desired samples starting from the first sample.       Default value: `None`.     dtype: (Optional) The dtype of the sample. One of: `float16`, `float32` or       `float64`.       Default value: `tf.float32`.     randomized: (Optional) bool indicating whether to produce a randomized       Halton sequence. If True, applies the randomization described in       [Owen (2017)][1].       Default value: `True`.     seed: (Optional) Python integer to seed the random number generator. Only       used if `randomized` is True. If not supplied and `randomized` is True,       no seed is set.       Default value: `None`.     name:  (Optional) Python `str` describing ops managed by this function. If       not supplied the name of this function is used.       Default value: "sample_halton_sequence".    Returns:     halton_elements: Elements of the Halton sequence. `Tensor` of supplied dtype       and `shape` `[num_results, dim]` if `num_results` was specified or shape       `[s, dim]` where s is the size of `sequence_indices` if `sequence_indices`       were specified.    Raises:     ValueError: if both `sequence_indices` and `num_results` were specified or       if dimension `dim` is less than 1 or greater than 1000.    #### References    [1]: Art B. Owen. A randomized Halton algorithm in R. _arXiv preprint        arXiv:1706.02808_, 2017. https://arxiv.org/abs/1706.02808
Uniform iid sample from the space of permutations.    Draws a sample of size `num_results` from the group of permutations of degrees   specified by the `dims` tensor. These are packed together into one tensor   such that each row is one sample from each of the dimensions in `dims`. For   example, if dims = [2,3] and num_results = 2, the result is a tensor of shape   [2, 2 + 3] and the first row of the result might look like:   [1, 0, 2, 0, 1]. The first two elements are a permutation over 2 elements   while the next three are a permutation over 3 elements.    Args:     num_results: A positive scalar `Tensor` of integral type. The number of       draws from the discrete uniform distribution over the permutation groups.     dims: A 1D `Tensor` of the same dtype as `num_results`. The degree of the       permutation groups from which to sample.     seed: (Optional) Python integer to seed the random number generator.    Returns:     permutations: A `Tensor` of shape `[num_results, sum(dims)]` and the same     dtype as `dims`.
Generates starting points for the Halton sequence procedure.    The k'th element of the sequence is generated starting from a positive integer   which must be distinct for each `k`. It is conventional to choose the starting   point as `k` itself (or `k+1` if k is zero based). This function generates   the starting integers for the required elements and reshapes the result for   later use.    Args:     num_results: Positive scalar `Tensor` of dtype int32. The number of samples       to generate. If this parameter is supplied, then `sequence_indices`       should be None.     sequence_indices: `Tensor` of dtype int32 and rank 1. The entries       index into the Halton sequence starting with 0 and hence, must be whole       numbers. For example, sequence_indices=[0, 5, 6] will produce the first,       sixth and seventh elements of the sequence. If this parameter is not None       then `n` must be None.     dtype: The dtype of the sample. One of `float32` or `float64`.       Default is `float32`.     name: Python `str` name which describes ops created by this function.    Returns:     indices: `Tensor` of dtype `dtype` and shape = `[n, 1, 1]`.
Computes the number of terms in the place value expansion.    Let num = a0 + a1 b + a2 b^2 + ... ak b^k be the place value expansion of   `num` in base b (ak <> 0). This function computes and returns `k+1` for each   base `b` specified in `bases`.    This can be inferred from the base `b` logarithm of `num` as follows:     $$k = Floor(log_b (num)) + 1  = Floor( log(num) / log(b)) + 1$$    Args:     num: Scalar `Tensor` of dtype either `float32` or `float64`. The number to       compute the base expansion size of.     bases: `Tensor` of the same dtype as num. The bases to compute the size       against.    Returns:     Tensor of same dtype and shape as `bases` containing the size of num when     written in that base.
Returns sorted array of primes such that `2 <= prime < n`.
Returns the machine epsilon for the supplied dtype.
The Hager Zhang line search algorithm.    Performs an inexact line search based on the algorithm of   [Hager and Zhang (2006)][2].   The univariate objective function `value_and_gradients_function` is typically   generated by projecting a multivariate objective function along a search   direction. Suppose the multivariate function to be minimized is   `g(x1,x2, .. xn)`. Let (d1, d2, ..., dn) be the direction along which we wish   to perform a line search. Then the projected univariate function to be used   for line search is    ```None     f(a) = g(x1 + d1 * a, x2 + d2 * a, ..., xn + dn * a)   ```    The directional derivative along (d1, d2, ..., dn) is needed for this   procedure. This also corresponds to the derivative of the projected function   `f(a)` with respect to `a`. Note that this derivative must be negative for   `a = 0` if the direction is a descent direction.    The usual stopping criteria for the line search is the satisfaction of the   (weak) Wolfe conditions. For details of the Wolfe conditions, see   ref. [3]. On a finite precision machine, the exact Wolfe conditions can   be difficult to satisfy when one is very close to the minimum and as argued   by [Hager and Zhang (2005)][1], one can only expect the minimum to be   determined within square root of machine precision. To improve the situation,   they propose to replace the Wolfe conditions with an approximate version   depending on the derivative of the function which is applied only when one   is very close to the minimum. The following algorithm implements this   enhanced scheme.    ### Usage:    Primary use of line search methods is as an internal component of a class of   optimization algorithms (called line search based methods as opposed to   trust region methods). Hence, the end user will typically not want to access   line search directly. In particular, inexact line search should not be   confused with a univariate minimization method. The stopping criteria of line   search is the satisfaction of Wolfe conditions and not the discovery of the   minimum of the function.    With this caveat in mind, the following example illustrates the standalone   usage of the line search.    ```python     # Define value and gradient namedtuple     ValueAndGradient = namedtuple('ValueAndGradient', ['x', 'f', 'df'])     # Define a quadratic target with minimum at 1.3.     def value_and_gradients_function(x):       return ValueAndGradient(x=x, f=(x - 1.3) ** 2, df=2 * (x-1.3))     # Set initial step size.     step_size = tf.constant(0.1)     ls_result = tfp.optimizer.linesearch.hager_zhang(         value_and_gradients_function, initial_step_size=step_size)     # Evaluate the results.     with tf.Session() as session:       results = session.run(ls_result)       # Ensure convergence.       assert results.converged       # If the line search converged, the left and the right ends of the       # bracketing interval are identical.       assert results.left.x == result.right.x       # Print the number of evaluations and the final step size.       print ("Final Step Size: %f, Evaluations: %d" % (results.left.x,                                                        results.func_evals))   ```    ### References:   [1]: William Hager, Hongchao Zhang. A new conjugate gradient method with     guaranteed descent and an efficient line search. SIAM J. Optim., Vol 16. 1,     pp. 170-172. 2005.     https://www.math.lsu.edu/~hozhang/papers/cg_descent.pdf    [2]: William Hager, Hongchao Zhang. Algorithm 851: CG_DESCENT, a conjugate     gradient method with guaranteed descent. ACM Transactions on Mathematical     Software, Vol 32., 1, pp. 113-137. 2006.     http://users.clas.ufl.edu/hager/papers/CG/cg_compare.pdf    [3]: Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series in     Operations Research. pp 33-36. 2006    Args:     value_and_gradients_function: A Python callable that accepts a real scalar       tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that       correspond to scalar tensors of real dtype containing the point at which       the function was evaluated, the value of the function, and its       derivative at that point. The other namedtuple fields, if present,       should be tensors or sequences (possibly nested) of tensors.       In usual optimization application, this function would be generated by       projecting the multivariate objective function along some specific       direction. The direction is determined by some other procedure but should       be a descent direction (i.e. the derivative of the projected univariate       function must be negative at 0.).       Alternatively, the function may represent the batching of `n` such line       functions (e.g. projecting a single multivariate objective function along       `n` distinct directions at once) accepting n points as input, i.e. a       tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned       namedtuple should each be a tensor of shape [n], with the corresponding       input points, function values, and derivatives at those input points.     initial_step_size: (Optional) Scalar positive `Tensor` of real dtype, or       a tensor of shape [n] in batching mode. The initial value (or values) to       try to bracket the minimum. Default is `1.` as a float32.       Note that this point need not necessarily bracket the minimum for the line       search to work correctly but the supplied value must be greater than 0.       A good initial value will make the search converge faster.     value_at_initial_step: (Optional) The full return value of evaluating       value_and_gradients_function at initial_step_size, i.e. a namedtuple with       'x', 'f', 'df', if already known by the caller. If supplied the value of       `initial_step_size` will be ignored, otherwise the tuple will be computed       by evaluating value_and_gradients_function.     value_at_zero: (Optional) The full return value of       value_and_gradients_function at `0.`, i.e. a namedtuple with       'x', 'f', 'df', if already known by the caller. If not supplied the tuple       will be computed by evaluating value_and_gradients_function.     converged: (Optional) In batching mode a tensor of shape [n], indicating       batch members which have already converged and no further search should       be performed. These batch members are also reported as converged in the       output, and both their `left` and `right` are set to the       `value_at_initial_step`.     threshold_use_approximate_wolfe_condition: Scalar positive `Tensor`       of real dtype. Corresponds to the parameter 'epsilon' in       [Hager and Zhang (2006)][2]. Used to estimate the       threshold at which the line search switches to approximate Wolfe       conditions.     shrinkage_param: Scalar positive Tensor of real dtype. Must be less than       `1.`. Corresponds to the parameter `gamma` in       [Hager and Zhang (2006)][2].       If the secant**2 step does not shrink the bracketing interval by this       proportion, a bisection step is performed to reduce the interval width.     expansion_param: Scalar positive `Tensor` of real dtype. Must be greater       than `1.`. Used to expand the initial interval in case it does not bracket       a minimum. Corresponds to `rho` in [Hager and Zhang (2006)][2].     sufficient_decrease_param: Positive scalar `Tensor` of real dtype.       Bounded above by the curvature param. Corresponds to `delta` in the       terminology of [Hager and Zhang (2006)][2].     curvature_param: Positive scalar `Tensor` of real dtype. Bounded above       by `1.`. Corresponds to 'sigma' in the terminology of       [Hager and Zhang (2006)][2].     step_size_shrink_param: Positive scalar `Tensor` of real dtype. Bounded       above by `1`. If the supplied step size is too big (i.e. either the       objective value or the gradient at that point is infinite), this factor       is used to shrink the step size until it is finite.     max_iterations: Positive scalar `Tensor` of integral dtype or None. The       maximum number of iterations to perform in the line search. The number of       iterations used to bracket the minimum are also counted against this       parameter.     name: (Optional) Python str. The name prefixed to the ops created by this       function. If not supplied, the default name 'hager_zhang' is used.    Returns:     results: A namedtuple containing the following attributes.       converged: Boolean `Tensor` of shape [n]. Whether a point satisfying         Wolfe/Approx wolfe was found.       failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.         if either the objective function or the gradient are not finite at         an evaluation point.       iterations: Scalar int32 `Tensor`. Number of line search iterations made.       func_evals: Scalar int32 `Tensor`. Number of function evaluations made.       left: A namedtuple, as returned by value_and_gradients_function,         of the left end point of the final bracketing interval. Values are         equal to those of `right` on batch members where converged is True.         Otherwise, it corresponds to the last interval computed.       right: A namedtuple, as returned by value_and_gradients_function,         of the right end point of the final bracketing interval. Values are         equal to those of `left` on batch members where converged is True.         Otherwise, it corresponds to the last interval computed.
Shrinks the input step size until the value and grad become finite.
Brackets the minimum and performs a line search.    Args:     value_and_gradients_function: A Python callable that accepts a real scalar       tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that       correspond to scalar tensors of real dtype containing the point at which       the function was evaluated, the value of the function, and its       derivative at that point. The other namedtuple fields, if present,       should be tensors or sequences (possibly nested) of tensors.       In usual optimization application, this function would be generated by       projecting the multivariate objective function along some specific       direction. The direction is determined by some other procedure but should       be a descent direction (i.e. the derivative of the projected univariate       function must be negative at 0.).       Alternatively, the function may represent the batching of `n` such line       functions (e.g. projecting a single multivariate objective function along       `n` distinct directions at once) accepting n points as input, i.e. a       tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned       namedtuple should each be a tensor of shape [n], with the corresponding       input points, function values, and derivatives at those input points.     init_interval: Instance of `HagerZhangLineSearchResults` containing       the initial line search interval. The gradient of init_interval.left must       be negative (i.e. must be a descent direction), while init_interval.right       must be positive and finite.     f_lim: Scalar `Tensor` of float dtype.     max_iterations: Positive scalar `Tensor` of integral dtype. The maximum       number of iterations to perform in the line search. The number of       iterations used to bracket the minimum are also counted against this       parameter.     shrinkage_param: Scalar positive Tensor of real dtype. Must be less than       `1.`. Corresponds to the parameter `gamma` in [Hager and Zhang (2006)][2].     expansion_param: Scalar positive `Tensor` of real dtype. Must be greater       than `1.`. Used to expand the initial interval in case it does not bracket       a minimum. Corresponds to `rho` in [Hager and Zhang (2006)][2].     sufficient_decrease_param: Positive scalar `Tensor` of real dtype.       Bounded above by the curvature param. Corresponds to `delta` in the       terminology of [Hager and Zhang (2006)][2].     curvature_param: Positive scalar `Tensor` of real dtype. Bounded above       by `1.`. Corresponds to 'sigma' in the terminology of       [Hager and Zhang (2006)][2].    Returns:     A namedtuple containing the following fields.       converged: Boolean `Tensor` of shape [n]. Whether a point satisfying         Wolfe/Approx wolfe was found.       failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.         if either the objective function or the gradient are not finite at         an evaluation point.       iterations: Scalar int32 `Tensor`. Number of line search iterations made.       func_evals: Scalar int32 `Tensor`. Number of function evaluations made.       left: A namedtuple, as returned by value_and_gradients_function,         of the left end point of the updated bracketing interval.       right: A namedtuple, as returned by value_and_gradients_function,         of the right end point of the updated bracketing interval.
The main loop of line search after the minimum has been bracketed.    Args:     value_and_gradients_function: A Python callable that accepts a real scalar       tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that       correspond to scalar tensors of real dtype containing the point at which       the function was evaluated, the value of the function, and its       derivative at that point. The other namedtuple fields, if present,       should be tensors or sequences (possibly nested) of tensors.       In usual optimization application, this function would be generated by       projecting the multivariate objective function along some specific       direction. The direction is determined by some other procedure but should       be a descent direction (i.e. the derivative of the projected univariate       function must be negative at 0.).       Alternatively, the function may represent the batching of `n` such line       functions (e.g. projecting a single multivariate objective function along       `n` distinct directions at once) accepting n points as input, i.e. a       tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned       namedtuple should each be a tensor of shape [n], with the corresponding       input points, function values, and derivatives at those input points.     search_interval: Instance of `HagerZhangLineSearchResults` containing       the current line search interval.     val_0: A namedtuple as returned by value_and_gradients_function evaluated       at `0.`. The gradient must be negative (i.e. must be a descent direction).     f_lim: Scalar `Tensor` of float dtype.     max_iterations: Positive scalar `Tensor` of integral dtype. The maximum       number of iterations to perform in the line search. The number of       iterations used to bracket the minimum are also counted against this       parameter.     sufficient_decrease_param: Positive scalar `Tensor` of real dtype.       Bounded above by the curvature param. Corresponds to `delta` in the       terminology of [Hager and Zhang (2006)][2].     curvature_param: Positive scalar `Tensor` of real dtype. Bounded above       by `1.`. Corresponds to 'sigma' in the terminology of       [Hager and Zhang (2006)][2].     shrinkage_param: Scalar positive Tensor of real dtype. Must be less than       `1.`. Corresponds to the parameter `gamma` in [Hager and Zhang (2006)][2].    Returns:     A namedtuple containing the following fields.       converged: Boolean `Tensor` of shape [n]. Whether a point satisfying         Wolfe/Approx wolfe was found.       failed: Boolean `Tensor` of shape [n]. Whether line search failed e.g.         if either the objective function or the gradient are not finite at         an evaluation point.       iterations: Scalar int32 `Tensor`. Number of line search iterations made.       func_evals: Scalar int32 `Tensor`. Number of function evaluations made.       left: A namedtuple, as returned by value_and_gradients_function,         of the left end point of the updated bracketing interval.       right: A namedtuple, as returned by value_and_gradients_function,         of the right end point of the updated bracketing interval.
Performs bisection and updates the interval.
Prepares the arguments for the line search initialization.    Args:     value_and_gradients_function: A Python callable that accepts a real scalar       tensor and returns a namedtuple with the fields 'x', 'f', and 'df' that       correspond to scalar tensors of real dtype containing the point at which       the function was evaluated, the value of the function, and its       derivative at that point. The other namedtuple fields, if present,       should be tensors or sequences (possibly nested) of tensors.       In usual optimization application, this function would be generated by       projecting the multivariate objective function along some specific       direction. The direction is determined by some other procedure but should       be a descent direction (i.e. the derivative of the projected univariate       function must be negative at 0.).       Alternatively, the function may represent the batching of `n` such line       functions (e.g. projecting a single multivariate objective function along       `n` distinct directions at once) accepting n points as input, i.e. a       tensor of shape [n], and the fields 'x', 'f' and 'df' in the returned       namedtuple should each be a tensor of shape [n], with the corresponding       input points, function values, and derivatives at those input points.     initial_step_size: Scalar positive `Tensor` of real dtype, or a tensor of       shape [n] in batching mode. The initial value (or values) to try to       bracket the minimum. Default is `1.` as a float32.       Note that this point need not necessarily bracket the minimum for the line       search to work correctly but the supplied value must be greater than 0.       A good initial value will make the search converge faster.     val_initial: The full return value of evaluating       value_and_gradients_function at initial_step_size, i.e. a namedtuple with       'x', 'f', 'df', if already known by the caller. If not None the value of       `initial_step_size` will be ignored, otherwise the tuple will be computed       by evaluating value_and_gradients_function.     val_0: The full return value of value_and_gradients_function at `0.`, i.e.       a namedtuple with 'x', 'f', 'df', if already known by the caller. If None       the tuple will be computed by evaluating value_and_gradients_function.     approximate_wolfe_threshold: Scalar positive `Tensor` of       real dtype. Corresponds to the parameter 'epsilon' in       [Hager and Zhang (2006)][2]. Used to estimate the       threshold at which the line search switches to approximate Wolfe       conditions.    Returns:     left: A namedtuple, as returned by value_and_gradients_function,       containing the value and derivative of the function at `0.`.     val_initial: A namedtuple, as returned by value_and_gradients_function,       containing the value and derivative of the function at       `initial_step_size`.     f_lim: Real `Tensor` of shape [n]. The function value threshold for       the approximate Wolfe conditions to be checked.     eval_count: Scalar int32 `Tensor`. The number of target function       evaluations made by this function.
Wrapper for tf.Print which supports lists and namedtuples for printing.
Use Gauss-Hermite quadrature to form quadrature on `K - 1` simplex.    A `SoftmaxNormal` random variable `Y` may be generated via    ```   Y = SoftmaxCentered(X),   X = Normal(normal_loc, normal_scale)   ```    Note: for a given `quadrature_size`, this method is generally less accurate   than `quadrature_scheme_softmaxnormal_quantiles`.    Args:     normal_loc: `float`-like `Tensor` with shape `[b1, ..., bB, K-1]`, B>=0.       The location parameter of the Normal used to construct the SoftmaxNormal.     normal_scale: `float`-like `Tensor`. Broadcastable with `normal_loc`.       The scale parameter of the Normal used to construct the SoftmaxNormal.     quadrature_size: Python `int` scalar representing the number of quadrature       points.     validate_args: Python `bool`, default `False`. When `True` distribution       parameters are checked for validity despite possibly degrading runtime       performance. When `False` invalid inputs may silently render incorrect       outputs.     name: Python `str` name prefixed to Ops created by this class.    Returns:     grid: Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the       convex combination of affine parameters for `K` components.       `grid[..., :, n]` is the `n`-th grid point, living in the `K - 1` simplex.     probs:  Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the       associated with each grid point.
Use SoftmaxNormal quantiles to form quadrature on `K - 1` simplex.    A `SoftmaxNormal` random variable `Y` may be generated via    ```   Y = SoftmaxCentered(X),   X = Normal(normal_loc, normal_scale)   ```    Args:     normal_loc: `float`-like `Tensor` with shape `[b1, ..., bB, K-1]`, B>=0.       The location parameter of the Normal used to construct the SoftmaxNormal.     normal_scale: `float`-like `Tensor`. Broadcastable with `normal_loc`.       The scale parameter of the Normal used to construct the SoftmaxNormal.     quadrature_size: Python `int` scalar representing the number of quadrature       points.     validate_args: Python `bool`, default `False`. When `True` distribution       parameters are checked for validity despite possibly degrading runtime       performance. When `False` invalid inputs may silently render incorrect       outputs.     name: Python `str` name prefixed to Ops created by this class.    Returns:     grid: Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the       convex combination of affine parameters for `K` components.       `grid[..., :, n]` is the `n`-th grid point, living in the `K - 1` simplex.     probs:  Shape `[b1, ..., bB, K, quadrature_size]` `Tensor` representing the       associated with each grid point.
Helper which checks validity of `loc` and `scale` init args.
Helper to infer batch_shape and event_shape.
Helper which interpolates between two locs.
Helper which interpolates between two scales.
Creates weighted `LinOp` from existing `LinOp`.
Concatenates input vectors, statically if possible.
Multiply tensor of vectors by matrices assuming values stored are logs.
Multiply tensor of matrices by vectors assuming values stored are logs.
Multiply tensor of vectors by matrices.
Tabulate log probabilities from a batch of distributions.
Compute marginal pdf for each individual observable.
Compute marginal posterior distribution for each state.      This function computes, for each time step, the marginal     conditional probability that the hidden Markov model was in     each possible state given the observations that were made     at each time step.     So if the hidden states are `z[0],...,z[num_steps - 1]` and     the observations are `x[0], ..., x[num_steps - 1]`, then     this function computes `P(z[i] | x[0], ..., x[num_steps - 1])`     for all `i` from `0` to `num_steps - 1`.      This operation is sometimes called smoothing. It uses a form     of the forward-backward algorithm.      Note: the behavior of this function is undefined if the     `observations` argument represents impossible observations     from the model.      Args:       observations: A tensor representing a batch of observations         made on the hidden Markov model.  The rightmost dimension of this tensor         gives the steps in a sequence of observations from a single sample from         the hidden Markov model. The size of this dimension should match the         `num_steps` parameter of the hidden Markov model object. The other         dimensions are the dimensions of the batch and these are broadcast with         the hidden Markov model's parameters.       name: Python `str` name prefixed to Ops created by this class.         Default value: "HiddenMarkovModel".      Returns:       posterior_marginal: A `Categorical` distribution object representing the         marginal probability of the hidden Markov model being in each state at         each step. The rightmost dimension of the `Categorical` distributions         batch will equal the `num_steps` parameter providing one marginal         distribution for each step. The other dimensions are the dimensions         corresponding to the batch of observations.      Raises:       ValueError: if rightmost dimension of `observations` does not       have size `num_steps`.
Compute maximum likelihood sequence of hidden states.      When this function is provided with a sequence of observations     `x[0], ..., x[num_steps - 1]`, it returns the sequence of hidden     states `z[0], ..., z[num_steps - 1]`, drawn from the underlying     Markov chain, that is most likely to yield those observations.      It uses the [Viterbi algorithm](     https://en.wikipedia.org/wiki/Viterbi_algorithm).      Note: the behavior of this function is undefined if the     `observations` argument represents impossible observations     from the model.      Note: if there isn't a unique most likely sequence then one     of the equally most likely sequences is chosen.      Args:       observations: A tensor representing a batch of observations made on the         hidden Markov model.  The rightmost dimensions of this tensor correspond         to the dimensions of the observation distributions of the underlying         Markov chain.  The next dimension from the right indexes the steps in a         sequence of observations from a single sample from the hidden Markov         model.  The size of this dimension should match the `num_steps`         parameter of the hidden Markov model object.  The other dimensions are         the dimensions of the batch and these are broadcast with the hidden         Markov model's parameters.       name: Python `str` name prefixed to Ops created by this class.         Default value: "HiddenMarkovModel".      Returns:       posterior_mode: A `Tensor` representing the most likely sequence of hidden         states. The rightmost dimension of this tensor will equal the         `num_steps` parameter providing one hidden state for each step. The         other dimensions are those of the batch.      Raises:       ValueError: if the `observations` tensor does not consist of       sequences of `num_steps` observations.      #### Examples      ```python     tfd = tfp.distributions      # A simple weather model.      # Represent a cold day with 0 and a hot day with 1.     # Suppose the first day of a sequence has a 0.8 chance of being cold.      initial_distribution = tfd.Categorical(probs=[0.8, 0.2])      # Suppose a cold day has a 30% chance of being followed by a hot day     # and a hot day has a 20% chance of being followed by a cold day.      transition_distribution = tfd.Categorical(probs=[[0.7, 0.3],                                                      [0.2, 0.8]])      # Suppose additionally that on each day the temperature is     # normally distributed with mean and standard deviation 0 and 5 on     # a cold day and mean and standard deviation 15 and 10 on a hot day.      observation_distribution = tfd.Normal(loc=[0., 15.], scale=[5., 10.])      # This gives the hidden Markov model:      model = tfd.HiddenMarkovModel(         initial_distribution=initial_distribution,         transition_distribution=transition_distribution,         observation_distribution=observation_distribution,         num_steps=7)      # Suppose we observe gradually rising temperatures over a week:     temps = [-2., 0., 2., 4., 6., 8., 10.]      # We can now compute the most probable sequence of hidden states:      model.posterior_mode(temps)      # The result is [0 0 0 0 0 1 1] telling us that the transition     # from "cold" to "hot" most likely happened between the     # 5th and 6th days.     ```
Chooses a random direction in the event space.
Applies a single iteration of slice sampling update.    Applies hit and run style slice sampling. Chooses a uniform random direction   on the unit sphere in the event space. Applies the one dimensional slice   sampling update along that direction.    Args:     target_log_prob_fn: Python callable which takes an argument like       `*current_state_parts` and returns its (possibly unnormalized) log-density       under the target distribution.     current_state_parts: Python `list` of `Tensor`s representing the current       state(s) of the Markov chain(s). The first `independent_chain_ndims` of       the `Tensor`(s) index different chains.     step_sizes: Python `list` of `Tensor`s. Provides a measure of the width       of the density. Used to find the slice bounds. Must broadcast with the       shape of `current_state_parts`.     max_doublings: Integer number of doublings to allow while locating the slice       boundaries.     current_target_log_prob: `Tensor` representing the value of       `target_log_prob_fn(*current_state_parts)`. The only reason to specify       this argument is to reduce TF graph size.     batch_rank: Integer. The number of axes in the state that correspond to       independent batches.     seed: Python integer to seed random number generators.     name: Python `str` name prefixed to Ops created by this function.       Default value: `None` (i.e., 'find_slice_bounds').    Returns:     proposed_state_parts: Tensor or Python list of `Tensor`s representing the       state(s) of the Markov chain(s) at each result step. Has same shape as       input `current_state_parts`.     proposed_target_log_prob: `Tensor` representing the value of       `target_log_prob_fn` at `next_state`.     bounds_satisfied: Boolean `Tensor` of the same shape as the log density.       True indicates whether the an interval containing the slice for that       batch was found successfully.     direction: `Tensor` or Python list of `Tensors`s representing the direction       along which the slice was sampled. Has the same shape and dtype(s) as       `current_state_parts`.     upper_bounds: `Tensor` of batch shape and the dtype of the input state. The       upper bounds of the slices along the sampling direction.     lower_bounds: `Tensor` of batch shape and the dtype of the input state. The       lower bounds of the slices along the sampling direction.
Helper which computes `fn_result` if needed.
Pads the shape of x to the right to be of rank final_rank.    Expands the dims of `x` to the right such that its rank is equal to   final_rank. For example, if `x` is of shape [1, 5, 7, 2] and `final_rank` is   7, we return padded_x, which is of shape [1, 5, 7, 2, 1, 1, 1].    Args:     x: The tensor whose shape is to be padded.     final_rank: Scalar int32 `Tensor` or Python `int`. The desired rank of x.    Returns:     padded_x: A tensor of rank final_rank.
Runs one iteration of Slice Sampler.      Args:       current_state: `Tensor` or Python `list` of `Tensor`s representing the         current state(s) of the Markov chain(s). The first `r` dimensions         index independent chains,         `r = tf.rank(target_log_prob_fn(*current_state))`.       previous_kernel_results: `collections.namedtuple` containing `Tensor`s         representing values from previous calls to this function (or from the         `bootstrap_results` function.)      Returns:       next_state: Tensor or Python list of `Tensor`s representing the state(s)         of the Markov chain(s) after taking exactly one step. Has same type and         shape as `current_state`.       kernel_results: `collections.namedtuple` of internal calculations used to         advance the chain.      Raises:       ValueError: if there isn't one `step_size` or a list with same length as         `current_state`.       TypeError: if `not target_log_prob.dtype.is_floating`.
Built a transformed-normal variational dist over a parameter's support.
Build a loss function for variational inference in STS models.    Variational inference searches for the distribution within some family of   approximate posteriors that minimizes a divergence between the approximate   posterior `q(z)` and true posterior `p(z|observed_time_series)`. By converting   inference to optimization, it's generally much faster than sampling-based   inference algorithms such as HMC. The tradeoff is that the approximating   family rarely contains the true posterior, so it may miss important aspects of   posterior structure (in particular, dependence between variables) and should   not be blindly trusted. Results may vary; it's generally wise to compare to   HMC to evaluate whether inference quality is sufficient for your task at hand.    This method constructs a loss function for variational inference using the   Kullback-Liebler divergence `KL[q(z) || p(z|observed_time_series)]`, with an   approximating family given by independent Normal distributions transformed to   the appropriate parameter space for each parameter. Minimizing this loss (the   negative ELBO) maximizes a lower bound on the log model evidence `-log   p(observed_time_series)`. This is equivalent to the 'mean-field' method   implemented in [1]. and is a standard approach. The resulting posterior   approximations are unimodal; they will tend to underestimate posterior   uncertainty when the true posterior contains multiple modes (the `KL[q||p]`   divergence encourages choosing a single mode) or dependence between variables.    Args:     model: An instance of `StructuralTimeSeries` representing a       time-series model. This represents a joint distribution over       time-series and their parameters with batch shape `[b1, ..., bN]`.     observed_time_series: `float` `Tensor` of shape       `concat([sample_shape, model.batch_shape, [num_timesteps, 1]]) where       `sample_shape` corresponds to i.i.d. observations, and the trailing `[1]`       dimension may (optionally) be omitted if `num_timesteps > 1`. May       optionally be an instance of `tfp.sts.MaskedTimeSeries`, which includes       a mask `Tensor` to specify timesteps with missing observations.     init_batch_shape: Batch shape (Python `tuple`, `list`, or `int`) of initial       states to optimize in parallel.       Default value: `()`. (i.e., just run a single optimization).     seed: Python integer to seed the random number generator.     name: Python `str` name prefixed to ops created by this function.       Default value: `None` (i.e., 'build_factored_variational_loss').    Returns:     variational_loss: `float` `Tensor` of shape       `concat([init_batch_shape, model.batch_shape])`, encoding a stochastic       estimate of an upper bound on the negative model evidence `-log p(y)`.       Minimizing this loss performs variational inference; the gap between the       variational bound and the true (generally unknown) model evidence       corresponds to the divergence `KL[q||p]` between the approximate and true       posterior.     variational_distributions: `collections.OrderedDict` giving       the approximate posterior for each model parameter. The keys are       Python `str` parameter names in order, corresponding to       `[param.name for param in model.parameters]`. The values are       `tfd.Distribution` instances with batch shape       `concat([init_batch_shape, model.batch_shape])`; these will typically be       of the form `tfd.TransformedDistribution(tfd.Normal(...),       bijector=param.bijector)`.    #### Examples    Assume we've built a structural time-series model:    ```python     day_of_week = tfp.sts.Seasonal(         num_seasons=7,         observed_time_series=observed_time_series,         name='day_of_week')     local_linear_trend = tfp.sts.LocalLinearTrend(         observed_time_series=observed_time_series,         name='local_linear_trend')     model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],                         observed_time_series=observed_time_series)   ```    To run variational inference, we simply construct the loss and optimize   it:    ```python     (variational_loss,      variational_distributions) = tfp.sts.build_factored_variational_loss(        model=model, observed_time_series=observed_time_series)      train_op = tf.train.AdamOptimizer(0.1).minimize(variational_loss)     with tf.Session() as sess:       sess.run(tf.global_variables_initializer())        for step in range(200):         _, loss_ = sess.run((train_op, variational_loss))          if step % 20 == 0:           print("step {} loss {}".format(step, loss_))        posterior_samples_ = sess.run({         param_name: q.sample(50)         for param_name, q in variational_distributions.items()})   ```    As a more complex example, we might try to avoid local optima by optimizing   from multiple initializations in parallel, and selecting the result with the   lowest loss:    ```python     (variational_loss,      variational_distributions) = tfp.sts.build_factored_variational_loss(        model=model, observed_time_series=observed_time_series,        init_batch_shape=[10])      train_op = tf.train.AdamOptimizer(0.1).minimize(variational_loss)     with tf.Session() as sess:       sess.run(tf.global_variables_initializer())        for step in range(200):         _, loss_ = sess.run((train_op, variational_loss))          if step % 20 == 0:           print("step {} losses {}".format(step, loss_))        # Draw multiple samples to reduce Monte Carlo error in the optimized       # variational bounds.       avg_loss = np.mean(         [sess.run(variational_loss) for _ in range(25)], axis=0)       best_posterior_idx = np.argmin(avg_loss, axis=0).astype(np.int32)   ```    #### References    [1]: Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and        David M. Blei. Automatic Differentiation Variational Inference. In        _Journal of Machine Learning Research_, 2017.        https://arxiv.org/abs/1603.00788
Run an optimizer within the graph to minimize a loss function.
Compute mean and variance, accounting for a mask.    Args:     time_series_tensor: float `Tensor` time series of shape       `concat([batch_shape, [num_timesteps]])`.     broadcast_mask: bool `Tensor` of the same shape as `time_series`.   Returns:     mean: float `Tensor` of shape `batch_shape`.     variance: float `Tensor` of shape `batch_shape`.
Get the first unmasked entry of each time series in the batch.    Args:     time_series_tensor: float `Tensor` of shape [..., num_timesteps].     broadcast_mask: bool `Tensor` of same shape as `time_series`.
Get broadcast batch shape from distributions, statically if possible.
Combine MultivariateNormals into a factored joint distribution.     Given a list of multivariate normal distributions    `dist[i] = Normal(loc[i], scale[i])`, construct the joint    distribution given by concatenating independent samples from these    distributions. This is multivariate normal with mean vector given by the    concatenation of the component mean vectors, and block-diagonal covariance    matrix in which the blocks are the component covariances.     Note that for computational efficiency, multivariate normals are represented    by a 'scale' (factored covariance) linear operator rather than the full    covariance matrix.    Args:     distributions: Python `iterable` of MultivariateNormal distribution       instances (e.g., `tfd.MultivariateNormalDiag`,       `tfd.MultivariateNormalTriL`, etc.). These must be broadcastable to a       consistent batch shape, but may have different event shapes       (i.e., defined over spaces of different dimension).    Returns:     joint_distribution: An instance of `tfd.MultivariateNormalLinearOperator`       representing the joint distribution constructed by concatenating       an independent sample from each input distributions.
Attempt to sum MultivariateNormal distributions.    The sum of (multivariate) normal random variables is itself (multivariate)   normal, with mean given by the sum of means and (co)variance given by the   sum of (co)variances. This method exploits this fact to compute the   sum of a list of `tfd.MultivariateNormalDiag` objects.    It may in the future be extended to support summation of other forms of   (Multivariate)Normal distributions.    Args:     distributions: Python `iterable` of `tfd.MultivariateNormalDiag`       distribution instances. These must all have the same event       shape, and broadcast to a consistent batch shape.    Returns:     sum_distribution: A `tfd.MultivariateNormalDiag` instance with mean       equal to the sum of input means and covariance equal to the sum of       input covariances.
Compute statistics of a provided time series, as heuristic initialization.    Args:     observed_time_series: `Tensor` representing a time series, or batch of time        series, of shape either `batch_shape + [num_timesteps, 1]` or        `batch_shape + [num_timesteps]` (allowed if `num_timesteps > 1`).    Returns:     observed_mean: `Tensor` of shape `batch_shape`, giving the empirical       mean of each time series in the batch.     observed_stddev: `Tensor` of shape `batch_shape`, giving the empirical       standard deviation of each time series in the batch.     observed_initial_centered: `Tensor of shape `batch_shape`, giving the       initial value of each time series in the batch after centering       (subtracting the mean).
Ensures `observed_time_series_tensor` has a trailing dimension of size 1.    The `tfd.LinearGaussianStateSpaceModel` Distribution has event shape of   `[num_timesteps, observation_size]`, but canonical BSTS models   are univariate, so their observation_size is always `1`. The extra trailing   dimension gets annoying, so this method allows arguments with or without the   extra dimension. There is no ambiguity except in the trivial special case   where  `num_timesteps = 1`; this can be avoided by specifying any unit-length   series in the explicit `[num_timesteps, 1]` style.    Most users should not call this method directly, and instead call   `canonicalize_observed_time_series_with_mask`, which handles converting   to `Tensor` and specifying an optional missingness mask.    Args:     observed_time_series_tensor: `Tensor` of shape       `batch_shape + [num_timesteps, 1]` or `batch_shape + [num_timesteps]`,       where `num_timesteps > 1`.    Returns:     expanded_time_series: `Tensor` of shape `batch_shape + [num_timesteps, 1]`.
Extract a Tensor with canonical shape and optional mask.    Args:     maybe_masked_observed_time_series: a `Tensor`-like object with shape       `[..., num_timesteps]` or `[..., num_timesteps, 1]`, or a       `tfp.sts.MaskedTimeSeries` containing such an object.   Returns:     masked_time_series: a `tfp.sts.MaskedTimeSeries` namedtuple, in which       the `observed_time_series` is converted to `Tensor` with canonical shape       `[..., num_timesteps, 1]`, and `is_missing` is either `None` or a boolean       `Tensor`.
Construct a predictive normal distribution that mixes over posterior draws.    Args:     means: float `Tensor` of shape       `[num_posterior_draws, ..., num_timesteps]`.     variances: float `Tensor` of shape       `[num_posterior_draws, ..., num_timesteps]`.    Returns:     mixture_dist: `tfd.MixtureSameFamily(tfd.Independent(tfd.Normal))` instance       representing a uniform mixture over the posterior samples, with       `batch_shape = ...` and `event_shape = [num_timesteps]`.
`high - low`.
Factory for making summary statistics, eg, mean, mode, stddev.
Creates `dist_fn_wrapped` which calls `dist_fn` with all prev nodes.    Args:     i: Python `int` corresponding to position in topologically sorted DAG.     dist_fn: Python `callable` which takes a subset of previously constructed       distributions (in reverse order) and produces a new distribution instance.    Returns:     dist_fn_wrapped: Python `callable` which takes all previous distributions       (in non reverse order) and produces a  new distribution instance.     args: `tuple` of `str` representing the arg names of `dist_fn` (and in non       wrapped, "natural" order). `None` is returned only if the input is not a       `callable`.
Uses arg names to resolve distribution names.
Returns the distribution's required args.
Calculate the KL divergence between two `JointDistributionSequential`s.    Args:     d0: instance of a `JointDistributionSequential` object.     d1: instance of a `JointDistributionSequential` object.     name: (optional) Name to use for created operations.       Default value: `"kl_joint_joint"`.    Returns:     kl_joint_joint: `Tensor` The sum of KL divergences between elemental       distributions of two joint distributions.    Raises:     ValueError: when joint distributions have a different number of elemental       distributions.     ValueError: when either joint distribution has a distribution with dynamic       dependency, i.e., when either joint distribution is not a collection of       independent distributions.
Creates `dist_fn`, `dist_fn_wrapped`, `dist_fn_args`.
Creates a `tuple` of `tuple`s of dependencies.      This function is **experimental**. That said, we encourage its use     and ask that you report problems to `tfprobability@tensorflow.org`.      Args:       distribution_names: `list` of `str` or `None` names corresponding to each         of `model` elements. (`None`s are expanding into the         appropriate `str`.)       leaf_name: `str` used when no maker depends on a particular         `model` element.      Returns:       graph: `tuple` of `(str tuple)` pairs representing the name of each         distribution (maker) and the names of its dependencies.      #### Example      ```python     d = tfd.JointDistributionSequential([                      tfd.Independent(tfd.Exponential(rate=[100, 120]), 1),         lambda    e: tfd.Gamma(concentration=e[..., 0], rate=e[..., 1]),                      tfd.Normal(loc=0, scale=2.),         lambda n, g: tfd.Normal(loc=n, scale=g),     ])     d._resolve_graph()     # ==> (     #       ('e', ()),     #       ('g', ('e',)),     #       ('n', ()),     #       ('x', ('n', 'g')),     #     )     ```
Shannon entropy in nats.
Decorator function for argument bounds checking.    This decorator is meant to be used with methods that require the first   argument to be in the support of the distribution. If `validate_args` is   `True`, the method is wrapped with an assertion that the first argument is   greater than or equal to `loc`, since the support of the half-Cauchy   distribution is given by `[loc, infinity)`.     Args:     f: method to be decorated.    Returns:     Returns a decorated method that, when `validate_args` attribute of the class     is `True`, will assert that all elements in the first argument are within     the support of the distribution before executing the original method.
Visualizes sequences as TensorBoard summaries.    Args:     seqs: A tensor of shape [n, t, h, w, c].     name: String name of this summary.     num: Integer for the number of examples to visualize. Defaults to       all examples.
Visualizes the reconstruction of inputs in TensorBoard.    Args:     inputs: A tensor of the original inputs, of shape [batch, timesteps,       h, w, c].     reconstruct: A tensor of a reconstruction of inputs, of shape       [batch, timesteps, h, w, c].     num: Integer for the number of examples to visualize.     name: String name of this summary.
Visualizes a qualitative analysis of a given model.    Args:     inputs: A tensor of the original inputs, of shape [batch, timesteps,       h, w, c].     model: A DisentangledSequentialVAE model.     samples: Number of samples to draw from the latent distributions.     batch_size: Number of sequences to generate.     length: Number of timesteps to generate for each sequence.
Summarize the parameters of a distribution.    Args:     dist: A Distribution object with mean and standard deviation       parameters.     name: The name of the distribution.     name_scope: The name scope of this summary.
Summarize the mean of a tensor in nats and bits per unit.    Args:     inputs: A tensor of values measured in nats.     units: The units of the tensor with which to compute the mean bits       per unit.     name: The name of the tensor.     nats_name_scope: The name scope of the nats summary.     bits_name_scope: The name scope of the bits summary.
Runs the model to generate multivariate normal distribution.      Args:       inputs: Unused.      Returns:       A MultivariateNormalDiag distribution with event shape       [dimensions], batch shape [], and sample shape [sample_shape,       dimensions].
Returns an initial state for the LSTM cell.      Args:       sample_batch_shape: A 0D or 1D tensor of the combined sample and         batch shape.      Returns:       A tuple of the initial previous output at timestep 0 of shape       [sample_batch_shape, dimensions], and the cell state.
Runs the model to generate a distribution for a single timestep.      This generates a batched MultivariateNormalDiag distribution using     the output of the recurrent model at the current timestep to     parameterize the distribution.      Args:       inputs: The sampled value of `z` at the previous timestep, i.e.,         `z_{t-1}`, of shape [..., dimensions].         `z_0` should be set to the empty matrix.       state: A tuple containing the (hidden, cell) state.      Returns:       A tuple of a MultivariateNormalDiag distribution, and the state of       the recurrent function at the end of the current timestep. The       distribution will have event shape [dimensions], batch shape       [...], and sample shape [sample_shape, ..., dimensions].
Runs the model to generate an intermediate representation of x_t.      Args:       inputs: A batch of image sequences `x_{1:T}` of shape         `[sample_shape, batch_size, timesteps, height, width,         channels]`.      Returns:       A batch of intermediate representations of shape [sample_shape,       batch_size, timesteps, hidden_size].
Generate new sequences.      Args:       batch_size: Number of sequences to generate.       length: Number of timesteps to generate for each sequence.       samples: Number of samples to draw from the latent distributions.       fix_static: Boolean for whether or not to share the same random         sample of the static latent variable `f` from its prior across         all examples.       fix_dynamic: Boolean for whether or not to share the same random         sample of the dynamic latent variable `z_{1:T}` from its prior         across all examples.      Returns:       A batched Independent distribution wrapping a set of Normal       distributions over the pixels of the generated sequences, where       the Independent distribution has event shape [height, width,       channels], batch shape [samples, batch_size, timesteps], and       sample shape [sample_shape, samples, batch_size, timesteps,       height, width, channels].
Reconstruct the given input sequences.      Args:       inputs: A batch of image sequences `x_{1:T}` of shape         `[batch_size, timesteps, height, width, channels]`.       samples: Number of samples to draw from the latent distributions.       sample_static: Boolean for whether or not to randomly sample the         static latent variable `f` from its prior distribution.       sample_dynamic: Boolean for whether or not to randomly sample the         dynamic latent variable `z_{1:T}` from its prior distribution.       swap_static: Boolean for whether or not to swap the encodings for         the static latent variable `f` between the examples.       swap_dynamic: Boolean for whether or not to swap the encodings for         the dynamic latent variable `z_{1:T}` between the examples.       fix_static: Boolean for whether or not to share the same random         sample of the static latent variable `f` from its prior across         all examples.       fix_dynamic: Boolean for whether or not to share the same random         sample of the dynamic latent variable `z_{1:T}` from its prior         across all examples.      Returns:       A batched Independent distribution wrapping a set of Normal       distributions over the pixels of the reconstruction of the input,       where the Independent distribution has event shape [height, width,       channels], batch shape [samples, batch_size, timesteps], and       sample shape [sample_shape, samples, batch_size, timesteps,       height, width, channels].
Sample the static latent prior.      Args:       samples: Number of samples to draw from the latent distribution.       batch_size: Number of sequences to sample.       fixed: Boolean for whether or not to share the same random         sample across all sequences.      Returns:       A tuple of a sample tensor of shape [samples, batch_size,       latent_size], and a MultivariateNormalDiag distribution from which       the tensor was sampled, with event shape [latent_size], and batch       shape [].
Sample the dynamic latent prior.      Args:       samples: Number of samples to draw from the latent distribution.       batch_size: Number of sequences to sample.       length: Number of timesteps to sample for each sequence.       fixed: Boolean for whether or not to share the same random         sample across all sequences.      Returns:       A tuple of a sample tensor of shape [samples, batch_size, length       latent_size], and a MultivariateNormalDiag distribution from which       the tensor was sampled, with event shape [latent_size], and batch       shape [samples, 1, length] if fixed or [samples, batch_size,       length] otherwise.
Static batch shape of models represented by this component.      Returns:       batch_shape: A `tf.TensorShape` giving the broadcast batch shape of         all model parameters. This should match the batch shape of         derived state space models, i.e.,         `self.make_state_space_model(...).batch_shape`. It may be partially         defined or unknown.
Runtime batch shape of models represented by this component.      Returns:       batch_shape: `int` `Tensor` giving the broadcast batch shape of         all model parameters. This should match the batch shape of         derived state space models, i.e.,         `self.make_state_space_model(...).batch_shape_tensor()`.
Instantiate this model as a Distribution over specified `num_timesteps`.      Args:       num_timesteps: Python `int` number of timesteps to model.       param_vals: a list of `Tensor` parameter values in order corresponding to         `self.parameters`, or a dict mapping from parameter names to values.       initial_state_prior: an optional `Distribution` instance overriding the         default prior on the model's initial state. This is used in forecasting         ("today's prior is yesterday's posterior").       initial_step: optional `int` specifying the initial timestep to model.         This is relevant when the model contains time-varying components,         e.g., holidays or seasonality.      Returns:       dist: a `LinearGaussianStateSpaceModel` Distribution object.
Sample from the joint prior over model parameters and trajectories.      Args:       num_timesteps: Scalar `int` `Tensor` number of timesteps to model.       initial_step: Optional scalar `int` `Tensor` specifying the starting         timestep.           Default value: 0.       params_sample_shape: Number of possible worlds to sample iid from the         parameter prior, or more generally, `Tensor` `int` shape to fill with         iid samples.           Default value: [] (i.e., draw a single sample and don't expand the           shape).       trajectories_sample_shape: For each sampled set of parameters, number         of trajectories to sample, or more generally, `Tensor` `int` shape to         fill with iid samples.         Default value: [] (i.e., draw a single sample and don't expand the           shape).       seed: Python `int` random seed.      Returns:       trajectories: `float` `Tensor` of shape         `trajectories_sample_shape + params_sample_shape + [num_timesteps, 1]`         containing all sampled trajectories.       param_samples: list of sampled parameter value `Tensor`s, in order         corresponding to `self.parameters`, each of shape         `params_sample_shape + prior.batch_shape + prior.event_shape`.
Computes the min_event_ndims associated with the give list of bijectors.    Given a list `bijector_list` of bijectors, compute the min_event_ndims that is   associated with the composition of bijectors in that list.    min_event_ndims is the # of right most dimensions for which the bijector has   done necessary computation on (i.e. the non-broadcastable part of the   computation).    We can derive the min_event_ndims for a chain of bijectors as follows:    In the case where there are no rank changing bijectors, this will simply be   `max(b.forward_min_event_ndims for b in bijector_list)`. This is because the   bijector with the most forward_min_event_ndims requires the most dimensions,   and hence the chain also requires operating on those dimensions.    However in the case of rank changing, more care is needed in determining the   exact amount of dimensions. Padding dimensions causes subsequent bijectors to   operate on the padded dimensions, and Removing dimensions causes bijectors to   operate more left.    Args:     bijector_list: List of bijectors to be composed by chain.     compute_forward: Boolean. If True, computes the min_event_ndims associated       with a forward call to Chain, and otherwise computes the min_event_ndims       associated with an inverse call to Chain. The latter is the same as the       min_event_ndims associated with a forward call to Invert(Chain(....)).    Returns:     min_event_ndims
Convert a vector size to a matrix size.
Numpy implementation of `tf.argsort`.
Numpy implementation of `tf.sort`.
Normal distribution function.    Returns the area under the Gaussian probability density function, integrated   from minus infinity to x:    ```                     1       / x      ndtr(x)  = ----------  |    exp(-0.5 t**2) dt                 sqrt(2 pi)  /-inf                = 0.5 (1 + erf(x / sqrt(2)))               = 0.5 erfc(x / sqrt(2))   ```    Args:     x: `Tensor` of type `float32`, `float64`.     name: Python string. A name for the operation (default="ndtr").    Returns:     ndtr: `Tensor` with `dtype=x.dtype`.    Raises:     TypeError: if `x` is not floating-type.
Implements ndtr core logic.
The inverse of the CDF of the Normal distribution function.    Returns x such that the area under the pdf from minus infinity to x is equal   to p.    A piece-wise rational approximation is done for the function.   This is a port of the implementation in netlib.    Args:     p: `Tensor` of type `float32`, `float64`.     name: Python string. A name for the operation (default="ndtri").    Returns:     x: `Tensor` with `dtype=p.dtype`.    Raises:     TypeError: if `p` is not floating-type.
Log Normal distribution function.    For details of the Normal distribution function see `ndtr`.    This function calculates `(log o ndtr)(x)` by either calling `log(ndtr(x))` or   using an asymptotic series. Specifically:   - For `x > upper_segment`, use the approximation `-ndtr(-x)` based on     `log(1-x) ~= -x, x << 1`.   - For `lower_segment < x <= upper_segment`, use the existing `ndtr` technique     and take a log.   - For `x <= lower_segment`, we use the series approximation of erf to compute     the log CDF directly.    The `lower_segment` is set based on the precision of the input:    ```   lower_segment = { -20,  x.dtype=float64                   { -10,  x.dtype=float32   upper_segment = {   8,  x.dtype=float64                   {   5,  x.dtype=float32   ```    When `x < lower_segment`, the `ndtr` asymptotic series approximation is:    ```      ndtr(x) = scale * (1 + sum) + R_N      scale   = exp(-0.5 x**2) / (-x sqrt(2 pi))      sum     = Sum{(-1)^n (2n-1)!! / (x**2)^n, n=1:N}      R_N     = O(exp(-0.5 x**2) (2N+1)!! / |x|^{2N+3})   ```    where `(2n-1)!! = (2n-1) (2n-3) (2n-5) ...  (3) (1)` is a   [double-factorial](https://en.wikipedia.org/wiki/Double_factorial).     Args:     x: `Tensor` of type `float32`, `float64`.     series_order: Positive Python `integer`. Maximum depth to       evaluate the asymptotic expansion. This is the `N` above.     name: Python string. A name for the operation (default="log_ndtr").    Returns:     log_ndtr: `Tensor` with `dtype=x.dtype`.    Raises:     TypeError: if `x.dtype` is not handled.     TypeError: if `series_order` is a not Python `integer.`     ValueError:  if `series_order` is not in `[0, 30]`.
Calculates the asymptotic series used in log_ndtr.
The inverse function for erf, the error function.    Args:     x: `Tensor` of type `float32`, `float64`.     name: Python string. A name for the operation (default="erfinv").    Returns:     x: `Tensor` with `dtype=x.dtype`.    Raises:     TypeError: if `x` is not floating-type.
Log Laplace distribution function.    This function calculates `Log[L(x)]`, where `L(x)` is the cumulative   distribution function of the Laplace distribution, i.e.    ```L(x) := 0.5 * int_{-infty}^x e^{-|t|} dt```    For numerical accuracy, `L(x)` is computed in different ways depending on `x`,    ```   x <= 0:     Log[L(x)] = Log[0.5] + x, which is exact    0 < x:     Log[L(x)] = Log[1 - 0.5 * e^{-x}], which is exact   ```    Args:     x: `Tensor` of type `float32`, `float64`.     name: Python string. A name for the operation (default="log_ndtr").    Returns:     `Tensor` with `dtype=x.dtype`.    Raises:     TypeError: if `x.dtype` is not handled.
Joint log probability function.
Runs HMC on the text-messages unnormalized posterior.
True if the given index_points would yield a univariate marginal.      Args:       index_points: the set of index set locations at which to compute the       marginal Gaussian distribution. If this set is of size 1, the marginal is       univariate.      Returns:       is_univariate: Boolean indicating whether the marginal is univariate or       multivariate. In the case of dynamic shape in the number of index points,       defaults to "multivariate" since that's the best we can do.
Compute the marginal of this GP over function values at `index_points`.      Args:       index_points: `float` `Tensor` representing finite (batch of) vector(s) of         points in the index set over which the GP is defined. Shape has the form         `[b1, ..., bB, e, f1, ..., fF]` where `F` is the number of feature         dimensions and must equal `kernel.feature_ndims` and `e` is the number         (size) of index points in each batch. Ultimately this distribution         corresponds to a `e`-dimensional multivariate normal. The batch shape         must be broadcastable with `kernel.batch_shape` and any batch dims         yielded by `mean_fn`.      Returns:       marginal: a `Normal` or `MultivariateNormalLinearOperator` distribution,         according to whether `index_points` consists of one or many index         points, respectively.
Return `index_points` if not None, else `self._index_points`.      Args:       index_points: if given, this is what is returned; else,       `self._index_points`      Returns:       index_points: the given arg, if not None, else the class member       `self._index_points`.      Rases:       ValueError: if `index_points` and `self._index_points` are both `None`.
Creates an stacked IAF bijector.    This bijector operates on vector-valued events.    Args:     total_event_size: Number of dimensions to operate over.     num_hidden_layers: How many hidden layers to use in each IAF.     seed: Random seed for the initializers.     dtype: DType for the variables.    Returns:     bijector: The created bijector.
Runs one iteration of NeuTra.      Args:       current_state: `Tensor` or Python `list` of `Tensor`s representing the         current state(s) of the Markov chain(s). The first `r` dimensions index         independent chains, `r = tf.rank(target_log_prob_fn(*current_state))`.       previous_kernel_results: `collections.namedtuple` containing `Tensor`s         representing values from previous calls to this function (or from the         `bootstrap_results` function.)      Returns:       next_state: Tensor or Python list of `Tensor`s representing the state(s)         of the Markov chain(s) after taking exactly one step. Has same type and         shape as `current_state`.       kernel_results: `collections.namedtuple` of internal calculations used to         advance the chain.
Trains the bijector and creates initial `previous_kernel_results`.      The supplied `state` is only used to determine the number of chains to run     in parallel_iterations      Args:       state: `Tensor` or Python `list` of `Tensor`s representing the initial         state(s) of the Markov chain(s). The first `r` dimensions index         independent chains, `r = tf.rank(target_log_prob_fn(*state))`.      Returns:       kernel_results: Instance of         `UncalibratedHamiltonianMonteCarloKernelResults` inside         `MetropolisHastingsResults` inside `TransformedTransitionKernelResults`         inside `SimpleStepSizeAdaptationResults`.
Convenience function analogous to tf.squared_difference.
Enables uniform interface to value and batch jacobian calculation.    Works in both eager and graph modes.    Arguments:     f: The scalar function to evaluate.     x: The value at which to compute the value and the batch jacobian.    Returns:     A tuple (f(x), J(x)), where J(x) is the batch jacobian.
Disables computation of the second derivatives for a tensor.    NB: you need to apply a non-identity function to the output tensor for the   exception to be raised.    Arguments:     x: A tensor.    Returns:     A tensor with the same value and the same derivative as x, but that raises     LookupError when trying to compute the second derivatives.
Performs distributional transform of the mixture samples.      Distributional transform removes the parameters from samples of a     multivariate distribution by applying conditional CDFs:       (F(x_1), F(x_2 | x1_), ..., F(x_d | x_1, ..., x_d-1))     (the indexing is over the "flattened" event dimensions).     The result is a sample of product of Uniform[0, 1] distributions.      We assume that the components are factorized, so the conditional CDFs become       F(x_i | x_1, ..., x_i-1) = sum_k w_i^k F_k (x_i),     where w_i^k is the posterior mixture weight: for i > 0       w_i^k = w_k prob_k(x_1, ..., x_i-1) / sum_k' w_k' prob_k'(x_1, ..., x_i-1)     and w_0^k = w_k is the mixture probability of the k-th component.      Arguments:       x: Sample of mixture distribution      Returns:       Result of the distributional transform
Split a covariance matrix into block-diagonal marginals of given sizes.
Utility method to decompose a joint posterior into components.    Args:     model: `tfp.sts.Sum` instance defining an additive STS model.     posterior_means: float `Tensor` of shape `concat(       [[num_posterior_draws], batch_shape, num_timesteps, latent_size])`       representing the posterior mean over latents in an       `AdditiveStateSpaceModel`.     posterior_covs: float `Tensor` of shape `concat(       [[num_posterior_draws], batch_shape, num_timesteps,       latent_size, latent_size])`       representing the posterior marginal covariances over latents in an       `AdditiveStateSpaceModel`.     parameter_samples: Python `list` of `Tensors` representing posterior       samples of model parameters, with shapes `[concat([       [num_posterior_draws], param.prior.batch_shape,       param.prior.event_shape]) for param in model.parameters]`. This may       optionally also be a map (Python `dict`) of parameter names to       `Tensor` values.    Returns:     component_dists: A `collections.OrderedDict` instance mapping       component StructuralTimeSeries instances (elements of `model.components`)       to `tfd.Distribution` instances representing the posterior marginal       distributions on the process modeled by each component. Each distribution       has batch shape matching that of `posterior_means`/`posterior_covs`, and       event shape of `[num_timesteps]`.
Decompose an observed time series into contributions from each component.    This method decomposes a time series according to the posterior represention   of a structural time series model. In particular, it:     - Computes the posterior marginal mean and covariances over the additive       model's latent space.     - Decomposes the latent posterior into the marginal blocks for each       model component.     - Maps the per-component latent posteriors back through each component's       observation model, to generate the time series modeled by that component.    Args:     model: An instance of `tfp.sts.Sum` representing a structural time series       model.     observed_time_series: `float` `Tensor` of shape       `batch_shape + [num_timesteps, 1]` (omitting the trailing unit dimension       is also supported when `num_timesteps > 1`), specifying an observed time       series. May optionally be an instance of `tfp.sts.MaskedTimeSeries`, which       includes a mask `Tensor` to specify timesteps with missing observations.     parameter_samples: Python `list` of `Tensors` representing posterior       samples of model parameters, with shapes `[concat([       [num_posterior_draws], param.prior.batch_shape,       param.prior.event_shape]) for param in model.parameters]`. This may       optionally also be a map (Python `dict`) of parameter names to       `Tensor` values.   Returns:     component_dists: A `collections.OrderedDict` instance mapping       component StructuralTimeSeries instances (elements of `model.components`)       to `tfd.Distribution` instances representing the posterior marginal       distributions on the process modeled by each component. Each distribution       has batch shape matching that of `posterior_means`/`posterior_covs`, and       event shape of `[num_timesteps]`.    #### Examples    Suppose we've built a model and fit it to data:    ```python     day_of_week = tfp.sts.Seasonal(         num_seasons=7,         observed_time_series=observed_time_series,         name='day_of_week')     local_linear_trend = tfp.sts.LocalLinearTrend(         observed_time_series=observed_time_series,         name='local_linear_trend')     model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],                         observed_time_series=observed_time_series)      num_steps_forecast = 50     samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)   ```    To extract the contributions of individual components, pass the time series   and sampled parameters into `decompose_by_component`:    ```python     component_dists = decompose_by_component(       model,       observed_time_series=observed_time_series,       parameter_samples=samples)      # Component mean and stddev have shape `[len(observed_time_series)]`.     day_of_week_effect_mean = component_dists[day_of_week].mean()     day_of_week_effect_stddev = component_dists[day_of_week].stddev()   ```    Using the component distributions, we can visualize the uncertainty for   each component:    ```   from matplotlib import pylab as plt   num_components = len(component_dists)   xs = np.arange(len(observed_time_series))   fig = plt.figure(figsize=(12, 3 * num_components))   for i, (component, component_dist) in enumerate(component_dists.items()):      # If in graph mode, replace `.numpy()` with `.eval()` or `sess.run()`.     component_mean = component_dist.mean().numpy()     component_stddev = component_dist.stddev().numpy()      ax = fig.add_subplot(num_components, 1, 1 + i)     ax.plot(xs, component_mean, lw=2)     ax.fill_between(xs,                     component_mean - 2 * component_stddev,                     component_mean + 2 * component_stddev,                     alpha=0.5)     ax.set_title(component.name)   ```
Decompose a forecast distribution into contributions from each component.    Args:     model: An instance of `tfp.sts.Sum` representing a structural time series       model.     forecast_dist: A `Distribution` instance returned by `tfp.sts.forecast()`.       (specifically, must be a `tfd.MixtureSameFamily` over a       `tfd.LinearGaussianStateSpaceModel` parameterized by posterior samples).     parameter_samples: Python `list` of `Tensors` representing posterior samples       of model parameters, with shapes `[concat([[num_posterior_draws],       param.prior.batch_shape, param.prior.event_shape]) for param in       model.parameters]`. This may optionally also be a map (Python `dict`) of       parameter names to `Tensor` values.   Returns:     component_forecasts: A `collections.OrderedDict` instance mapping       component StructuralTimeSeries instances (elements of `model.components`)       to `tfd.Distribution` instances representing the marginal forecast for       each component. Each distribution has batch and event shape matching       `forecast_dist` (specifically, the event shape is       `[num_steps_forecast]`).    #### Examples    Suppose we've built a model, fit it to data, and constructed a forecast   distribution:    ```python     day_of_week = tfp.sts.Seasonal(         num_seasons=7,         observed_time_series=observed_time_series,         name='day_of_week')     local_linear_trend = tfp.sts.LocalLinearTrend(         observed_time_series=observed_time_series,         name='local_linear_trend')     model = tfp.sts.Sum(components=[day_of_week, local_linear_trend],                         observed_time_series=observed_time_series)      num_steps_forecast = 50     samples, kernel_results = tfp.sts.fit_with_hmc(model, observed_time_series)     forecast_dist = tfp.sts.forecast(model, observed_time_series,                                  parameter_samples=samples,                                  num_steps_forecast=num_steps_forecast)   ```    To extract the forecast for individual components, pass the forecast   distribution into `decompose_forecast_by_components`:    ```python     component_forecasts = decompose_forecast_by_component(       model, forecast_dist, samples)      # Component mean and stddev have shape `[num_steps_forecast]`.     day_of_week_effect_mean = forecast_components[day_of_week].mean()     day_of_week_effect_stddev = forecast_components[day_of_week].stddev()   ```    Using the component forecasts, we can visualize the uncertainty for each   component:    ```   from matplotlib import pylab as plt   num_components = len(component_forecasts)   xs = np.arange(num_steps_forecast)   fig = plt.figure(figsize=(12, 3 * num_components))   for i, (component, component_dist) in enumerate(component_forecasts.items()):      # If in graph mode, replace `.numpy()` with `.eval()` or `sess.run()`.     component_mean = component_dist.mean().numpy()     component_stddev = component_dist.stddev().numpy()      ax = fig.add_subplot(num_components, 1, 1 + i)     ax.plot(xs, component_mean, lw=2)     ax.fill_between(xs,                     component_mean - 2 * component_stddev,                     component_mean + 2 * component_stddev,                     alpha=0.5)     ax.set_title(component.name)   ```
Converts dense `Tensor` to `SparseTensor`, dropping `ignore_value` cells.    Args:     x: A `Tensor`.     ignore_value: Entries in `x` equal to this value will be       absent from the return `SparseTensor`. If `None`, default value of       `x` dtype will be used (e.g. '' for `str`, 0 for `int`).     name: Python `str` prefix for ops created by this function.    Returns:     sparse_x: A `tf.SparseTensor` with the same shape as `x`.    Raises:     ValueError: when `x`'s rank is `None`.
Defers an operator overload to `attr`.    Args:     attr: Operator attribute to use.    Returns:     Function calling operator attribute.
Human-readable representation of a tensor's numpy value.
Sample shape of random variable as a `TensorShape`.
Sample shape of random variable as a 1-D `Tensor`.      Args:       name: name to give to the op      Returns:       sample_shape: `Tensor`.
Get tensor that the random variable corresponds to.
In a session, computes and returns the value of this random variable.      This is not a graph construction method, it does not add ops to the graph.      This convenience method requires a session where the graph     containing this variable has been launched. If no session is     passed, the default session is used.      Args:       session: tf.BaseSession.         The `tf.Session` to use to evaluate this random variable. If         none, the default session is used.       feed_dict: dict.         A dictionary that maps `tf.Tensor` objects to feed values. See         `tf.Session.run()` for a description of the valid feed values.      Returns:       Value of the random variable.      #### Examples      ```python     x = Normal(0.0, 1.0)     with tf.Session() as sess:       # Usage passing the session explicitly.       print(x.eval(sess))       # Usage with the default session.  The 'with' block       # above makes 'sess' the default session.       print(x.eval())     ```
Value as NumPy array, only available for TF Eager.
Posterior Normal distribution with conjugate prior on the mean.    This model assumes that `n` observations (with sum `s`) come from a   Normal with unknown mean `loc` (described by the Normal `prior`)   and known variance `scale**2`. The "known scale posterior" is   the distribution of the unknown `loc`.    Accepts a prior Normal distribution object, having parameters   `loc0` and `scale0`, as well as known `scale` values of the predictive   distribution(s) (also assumed Normal),   and statistical estimates `s` (the sum(s) of the observations) and   `n` (the number(s) of observations).    Returns a posterior (also Normal) distribution object, with parameters   `(loc', scale'**2)`, where:    ```   mu ~ N(mu', sigma'**2)   sigma'**2 = 1/(1/sigma0**2 + n/sigma**2),   mu' = (mu0/sigma0**2 + s/sigma**2) * sigma'**2.   ```    Distribution parameters from `prior`, as well as `scale`, `s`, and `n`.   will broadcast in the case of multidimensional sets of parameters.    Args:     prior: `Normal` object of type `dtype`:       the prior distribution having parameters `(loc0, scale0)`.     scale: tensor of type `dtype`, taking values `scale > 0`.       The known stddev parameter(s).     s: Tensor of type `dtype`. The sum(s) of observations.     n: Tensor of type `int`. The number(s) of observations.    Returns:     A new Normal posterior distribution object for the unknown observation     mean `loc`.    Raises:     TypeError: if dtype of `s` does not match `dtype`, or `prior` is not a       Normal object.
Build a scale-and-shift function using a multi-layer neural network.    This will be wrapped in a make_template to ensure the variables are only   created once. It takes the `d`-dimensional input x[0:d] and returns the `D-d`   dimensional outputs `loc` ("mu") and `log_scale` ("alpha").    The default template does not support conditioning and will raise an   exception if `condition_kwargs` are passed to it. To use conditioning in   real nvp bijector, implement a conditioned shift/scale template that   handles the `condition_kwargs`.    Arguments:     hidden_layers: Python `list`-like of non-negative integer, scalars       indicating the number of units in each hidden layer. Default: `[512, 512].     shift_only: Python `bool` indicating if only the `shift` term shall be       computed (i.e. NICE bijector). Default: `False`.     activation: Activation function (callable). Explicitly setting to `None`       implies a linear activation.     name: A name for ops managed by this function. Default:       "real_nvp_default_template".     *args: `tf.layers.dense` arguments.     **kwargs: `tf.layers.dense` keyword arguments.    Returns:     shift: `Float`-like `Tensor` of shift terms ("mu" in       [Papamakarios et al.  (2016)][1]).     log_scale: `Float`-like `Tensor` of log(scale) terms ("alpha" in       [Papamakarios et al. (2016)][1]).    Raises:     NotImplementedError: if rightmost dimension of `inputs` is unknown prior to       graph execution, or if `condition_kwargs` is not empty.    #### References    [1]: George Papamakarios, Theo Pavlakou, and Iain Murray. Masked        Autoregressive Flow for Density Estimation. In _Neural Information        Processing Systems_, 2017. https://arxiv.org/abs/1705.07057
Returns a batch of points chosen uniformly from the unit hypersphere.
Returns the unnormalized log density of an LKJ distribution.      Args:       x: `float` or `double` `Tensor` of correlation matrices.  The shape of `x`         must be `B + [D, D]`, where `B` broadcasts with the shape of         `concentration`.       name: Python `str` name prefixed to Ops created by this function.      Returns:       log_p: A Tensor of the unnormalized log density of each matrix element of         `x`, with respect to an LKJ distribution with parameter the         corresponding element of `concentration`.
Returns the log normalization of an LKJ distribution.      Args:       name: Python `str` name prefixed to Ops created by this function.      Returns:       log_z: A Tensor of the same shape and dtype as `concentration`, containing         the corresponding log normalizers.
Returns explict dtype from `args_list` if exists, else preferred_dtype.
Factory for implementing summary statistics, eg, mean, stddev, mode.
Helper to broadcast a tensor using a list of target tensors.
Pdf evaluated at the peak.
Estimate a lower bound on effective sample size for each independent chain.    Roughly speaking, "effective sample size" (ESS) is the size of an iid sample   with the same variance as `state`.    More precisely, given a stationary sequence of possibly correlated random   variables `X_1, X_2,...,X_N`, each identically distributed ESS is the number   such that    ```Variance{ N**-1 * Sum{X_i} } = ESS**-1 * Variance{ X_1 }.```    If the sequence is uncorrelated, `ESS = N`.  In general, one should expect   `ESS <= N`, with more highly correlated sequences having smaller `ESS`.    Args:     states:  `Tensor` or list of `Tensor` objects.  Dimension zero should index       identically distributed states.     filter_threshold:  `Tensor` or list of `Tensor` objects.       Must broadcast with `state`.  The auto-correlation sequence is truncated       after the first appearance of a term less than `filter_threshold`.       Setting to `None` means we use no threshold filter.  Since `|R_k| <= 1`,       setting to any number less than `-1` has the same effect.     filter_beyond_lag:  `Tensor` or list of `Tensor` objects.  Must be       `int`-like and scalar valued.  The auto-correlation sequence is truncated       to this length.  Setting to `None` means we do not filter based on number       of lags.     name:  `String` name to prepend to created ops.    Returns:     ess:  `Tensor` or list of `Tensor` objects.  The effective sample size of       each component of `states`.  Shape will be `states.shape[1:]`.    Raises:     ValueError:  If `states` and `filter_threshold` or `states` and       `filter_beyond_lag` are both lists with different lengths.    #### Examples    We use ESS to estimate standard error.    ```   import tensorflow as tf   import tensorflow_probability as tfp   tfd = tfp.distributions    target = tfd.MultivariateNormalDiag(scale_diag=[1., 2.])    # Get 1000 states from one chain.   states = tfp.mcmc.sample_chain(       num_burnin_steps=200,       num_results=1000,       current_state=tf.constant([0., 0.]),       kernel=tfp.mcmc.HamiltonianMonteCarlo(         target_log_prob_fn=target.log_prob,         step_size=0.05,         num_leapfrog_steps=20))   states.shape   ==> (1000, 2)    ess = effective_sample_size(states)   ==> Shape (2,) Tensor    mean, variance = tf.nn.moments(states, axis=0)   standard_error = tf.sqrt(variance / ess)   ```    Some math shows that, with `R_k` the auto-correlation sequence,   `R_k := Covariance{X_1, X_{1+k}} / Variance{X_1}`, we have    ```ESS(N) =  N / [ 1 + 2 * ( (N - 1) / N * R_1 + ... + 1 / N * R_{N-1}  ) ]```    This function estimates the above by first estimating the auto-correlation.   Since `R_k` must be estimated using only `N - k` samples, it becomes   progressively noisier for larger `k`.  For this reason, the summation over   `R_k` should be truncated at some number `filter_beyond_lag < N`.  Since many   MCMC methods generate chains where `R_k > 0`, a reasonable criteria is to   truncate at the first index where the estimated auto-correlation becomes   negative.    The arguments `filter_beyond_lag`, `filter_threshold` are filters intended to   remove noisy tail terms from `R_k`.  They combine in an "OR" manner meaning   terms are removed if they were to be filtered under the `filter_beyond_lag` OR   `filter_threshold` criteria.
ESS computation for one single Tensor argument.
potential_scale_reduction for one single state `Tensor`.
Get number of elements of `x` in `axis`, as type `x.dtype`.
Broadcast a listable secondary_arg to that of states.
Use Gauss-Hermite quadrature to form quadrature on positive-reals.    Note: for a given `quadrature_size`, this method is generally less accurate   than `quadrature_scheme_lognormal_quantiles`.    Args:     loc: `float`-like (batch of) scalar `Tensor`; the location parameter of       the LogNormal prior.     scale: `float`-like (batch of) scalar `Tensor`; the scale parameter of       the LogNormal prior.     quadrature_size: Python `int` scalar representing the number of quadrature       points.     validate_args: Python `bool`, default `False`. When `True` distribution       parameters are checked for validity despite possibly degrading runtime       performance. When `False` invalid inputs may silently render incorrect       outputs.     name: Python `str` name prefixed to Ops created by this class.    Returns:     grid: (Batch of) length-`quadrature_size` vectors representing the       `log_rate` parameters of a `Poisson`.     probs: (Batch of) length-`quadrature_size` vectors representing the       weight associate with each `grid` value.
Use LogNormal quantiles to form quadrature on positive-reals.    Args:     loc: `float`-like (batch of) scalar `Tensor`; the location parameter of       the LogNormal prior.     scale: `float`-like (batch of) scalar `Tensor`; the scale parameter of       the LogNormal prior.     quadrature_size: Python `int` scalar representing the number of quadrature       points.     validate_args: Python `bool`, default `False`. When `True` distribution       parameters are checked for validity despite possibly degrading runtime       performance. When `False` invalid inputs may silently render incorrect       outputs.     name: Python `str` name prefixed to Ops created by this class.    Returns:     grid: (Batch of) length-`quadrature_size` vectors representing the       `log_rate` parameters of a `Poisson`.     probs: (Batch of) length-`quadrature_size` vectors representing the       weight associate with each `grid` value.
Returns new _Mapping with args merged with self.      Args:       x: `Tensor` or None. Input to forward; output of inverse.       y: `Tensor` or None. Input to inverse; output of forward.       ildj: `Tensor`. This is the (un-reduce_sum'ed) inverse log det jacobian.       kwargs: Python dictionary. Extra args supplied to forward/inverse/etc         functions.       mapping: Instance of _Mapping to merge. Can only be specified if no other         arg is specified.      Returns:       mapping: New instance of `_Mapping` which has inputs merged with self.      Raises:       ValueError: if mapping and any other arg is not `None`.
To support weak referencing, removes cache key from the cache value.
Helper to merge which handles merging one value.
Converts nested `tuple`, `list`, or `dict` to nested `tuple`.
Computes the doubling increments for the left end point.    The doubling procedure expands an initial interval to find a superset of the   true slice. At each doubling iteration, the interval width is doubled to   either the left or the right hand side with equal probability.   If, initially, the left end point is at `L(0)` and the width of the   interval is `w(0)`, then the left end point and the width at the   k-th iteration (denoted L(k) and w(k) respectively) are given by the following   recursions:    ```none   w(k) = 2 * w(k-1)   L(k) = L(k-1) - w(k-1) * X_k, X_k ~ Bernoulli(0.5)   or, L(0) - L(k) = w(0) Sum(2^i * X(i+1), 0 <= i < k)   ```    This function computes the sequence of `L(0)-L(k)` and `w(k)` for k between 0   and `max_doublings` independently for each chain.    Args:     batch_shape: Positive int32 `tf.Tensor`. The batch shape.     max_doublings: Scalar positive int32 `tf.Tensor`. The maximum number of       doublings to consider.     step_size: A real `tf.Tensor` with shape compatible with [num_chains].       The size of the initial interval.     seed: (Optional) positive int. The random seed. If None, no seed is set.     name: Python `str` name prefixed to Ops created by this function.       Default value: `None` (i.e., 'find_slice_bounds').    Returns:     left_increments: A tensor of shape (max_doublings+1, batch_shape). The       relative position of the left end point after the doublings.     widths: A tensor of shape (max_doublings+1, ones_like(batch_shape)). The       widths of the intervals at each stage of the doubling.
Finds the index of the optimal set of bounds for each chain.    For each chain, finds the smallest set of bounds for which both edges lie   outside the slice. This is equivalent to the point at which a for loop   implementation (P715 of Neal (2003)) of the algorithm would terminate.    Performs the following calculation, where i is the number of doublings that   have been performed and k is the max number of doublings:    (2 * k - i) * flag + i    The argmax of the above returns the earliest index where the bounds were   outside the slice and if there is no such point, the widest bounds.    Args:     x: A tensor of shape (max_doublings+1, batch_shape). Type int32, with value       0 or 1. Indicates if this set of bounds is outside the slice.     name: Python `str` name prefixed to Ops created by this function.       Default value: `None` (i.e., 'find_slice_bounds').    Returns:     indices: A tensor of shape batch_shape. Type int32, with the index of the       first set of bounds outside the slice and if there are none, the index of       the widest set.
Returns the bounds of the slice at each stage of doubling procedure.    Precomputes the x coordinates of the left (L) and right (R) endpoints of the   interval `I` produced in the "doubling" algorithm [Neal 2003][1] P713. Note   that we simultaneously compute all possible doubling values for each chain,   for the reason that at small-medium densities, the gains from parallel   evaluation might cause a speed-up, but this will be benchmarked against the   while loop implementation.    Args:     x_initial: `tf.Tensor` of any shape and any real dtype consumable by       `target_log_prob`. The initial points.     target_log_prob: A callable taking a `tf.Tensor` of shape and dtype as       `x_initial` and returning a tensor of the same shape. The log density of       the target distribution.     log_slice_heights: `tf.Tensor` with the same shape as `x_initial` and the       same dtype as returned by `target_log_prob`. The log of the height of the       slice for each chain. The values must be bounded above by       `target_log_prob(x_initial)`.     max_doublings: Scalar positive int32 `tf.Tensor`. The maximum number of       doublings to consider.     step_size: `tf.Tensor` with same dtype as and shape compatible with       `x_initial`. The size of the initial interval.     seed: (Optional) positive int. The random seed. If None, no seed is set.     name: Python `str` name prefixed to Ops created by this function.       Default value: `None` (i.e., 'find_slice_bounds').    Returns:     upper_bounds: A tensor of same shape and dtype as `x_initial`. Slice upper       bounds for each chain.     lower_bounds: A tensor of same shape and dtype as `x_initial`. Slice lower       bounds for each chain.     both_ok: A tensor of shape `x_initial` and boolean dtype. Indicates if both       the chosen upper and lower bound lie outside of the slice.    #### References    [1]: Radford M. Neal. Slice Sampling. The Annals of Statistics. 2003, Vol 31,        No. 3 , 705-767.        https://projecteuclid.org/download/pdf_1/euclid.aos/1056562461
Samples from the slice by applying shrinkage for rejected points.    Implements the one dimensional slice sampling algorithm of Neal (2003), with a   doubling algorithm (Neal 2003 P715 Fig. 4), which doubles the size of the   interval at each iteration and shrinkage (Neal 2003 P716 Fig. 5), which   reduces the width of the slice when a selected point is rejected, by setting   the relevant bound that that value. Randomly sampled points are checked for   two criteria: that they lie within the slice and that they pass the   acceptability check (Neal 2003 P717 Fig. 6), which tests that the new state   could have generated the previous one.    Args:     x_initial: A tensor of any shape. The initial positions of the chains. This       function assumes that all the dimensions of `x_initial` are batch       dimensions (i.e. the event shape is `[]`).     target_log_prob: Callable accepting a tensor like `x_initial` and returning       a tensor containing the log density at that point of the same shape.     log_slice_heights: Tensor of the same shape and dtype as the return value       of `target_log_prob` when applied to `x_initial`. The log of the height of       the chosen slice.     step_size: A tensor of shape and dtype compatible with `x_initial`. The min       interval size in the doubling algorithm.     lower_bounds: Tensor of same shape and dtype as `x_initial`. Slice lower       bounds for each chain.     upper_bounds: Tensor of same shape and dtype as `x_initial`. Slice upper       bounds for each chain.     seed: (Optional) positive int. The random seed. If None, no seed is set.     name: Python `str` name prefixed to Ops created by this function.       Default value: `None` (i.e., 'find_slice_bounds').    Returns:     x_proposed: A tensor of the same shape and dtype as `x_initial`. The next       proposed state of the chain.
For a given x position in each Markov chain, returns the next x.    Applies the one dimensional slice sampling algorithm as defined in Neal (2003)   to an input tensor x of shape (num_chains,) where num_chains is the number of   simulataneous Markov chains, and returns the next tensor x of shape   (num_chains,) when these chains are evolved by the slice sampling algorithm.    Args:     target_log_prob: Callable accepting a tensor like `x_initial` and returning       a tensor containing the log density at that point of the same shape.     x_initial: A tensor of any shape. The initial positions of the chains. This       function assumes that all the dimensions of `x_initial` are batch       dimensions (i.e. the event shape is `[]`).     step_size: A tensor of shape and dtype compatible with `x_initial`. The min       interval size in the doubling algorithm.     max_doublings: Scalar tensor of dtype `tf.int32`. The maximum number of       doublings to try to find the slice bounds.     seed: (Optional) positive int. The random seed. If None, no seed is set.     name: Python `str` name prefixed to Ops created by this function.       Default value: `None` (i.e., 'find_slice_bounds').    Returns:     retval: A tensor of the same shape and dtype as `x_initial`. The next state       of the Markov chain.     next_target_log_prob: The target log density evaluated at `retval`.     bounds_satisfied: A tensor of bool dtype and shape batch dimensions.     upper_bounds: Tensor of the same shape and dtype as `x_initial`. The upper       bounds for the slice found.     lower_bounds: Tensor of the same shape and dtype as `x_initial`. The lower       bounds for the slice found.
Creates a value-setting interceptor.    This function creates an interceptor that sets values of Edward2 random   variable objects. This is useful for a range of tasks, including conditioning   on observed data, sampling from posterior predictive distributions, and as a   building block of inference primitives such as computing log joint   probabilities (see examples below).    Args:     **model_kwargs: dict of str to Tensor. Keys are the names of random       variables in the model to which this interceptor is being applied. Values       are Tensors to set their value to. Variables not included in this dict       will not be set and will maintain their existing value semantics (by       default, a sample from the parent-conditional distribution).    Returns:     set_values: function that sets the value of intercepted ops.    #### Examples    Consider for illustration a model with latent `z` and   observed `x`, and a corresponding trainable posterior model:    ```python   num_observations = 10   def model():     z = ed.Normal(loc=0, scale=1., name='z')  # log rate     x = ed.Poisson(rate=tf.exp(z) * tf.ones(num_observations), name='x')     return x    def variational_model():     return ed.Normal(loc=tf.Variable(0.),                      scale=tf.nn.softplus(tf.Variable(-4.)),                      name='z')  # for simplicity, match name of the model RV.   ```    We can use a value-setting interceptor to condition the model on observed   data. This approach is slightly more cumbersome than that of partially   evaluating the complete log-joint function, but has the potential advantage   that it returns a new model callable, which may be used to sample downstream   variables, passed into additional transformations, etc.    ```python   x_observed = np.array([6, 3, 1, 8, 7, 0, 6, 4, 7, 5])   def observed_model():     with ed.interception(make_value_setter(x=x_observed)):       model()   observed_log_joint_fn = ed.make_log_joint_fn(observed_model)    # After fixing 'x', the observed log joint is now only a function of 'z'.   # This enables us to define a variational lower bound,   # `E_q[ log p(x, z) - log q(z)]`, simply by evaluating the observed and   # variational log joints at variational samples.   variational_log_joint_fn = ed.make_log_joint_fn(variational_model)   with ed.tape() as variational_sample:  # Sample trace from variational model.     variational_model()   elbo_loss = -(observed_log_joint_fn(**variational_sample) -                 variational_log_joint_fn(**variational_sample))   ```    After performing inference by minimizing the variational loss, a value-setting   interceptor enables simulation from the posterior predictive distribution:    ```python   with ed.tape() as posterior_samples:  # tape is a map {rv.name : rv}     variational_model()   with ed.interception(ed.make_value_setter(**posterior_samples)):     x = model()   # x is a sample from p(X | Z = z') where z' ~ q(z) (the variational model)   ```    As another example, using a value setter inside of `ed.tape` enables   computing the log joint probability, by setting all variables to   posterior values and then accumulating the log probs of those values under   the induced parent-conditional distributions. This is one way that we could   have implemented `ed.make_log_joint_fn`:    ```python   def make_log_joint_fn_demo(model):     def log_joint_fn(**model_kwargs):       with ed.tape() as model_tape:         with ed.make_value_setter(**model_kwargs):           model()        # accumulate sum_i log p(X_i = x_i | X_{:i-1} = x_{:i-1})       log_prob = 0.       for rv in model_tape.values():         log_prob += tf.reduce_sum(rv.log_prob(rv.value))        return log_prob     return log_joint_fn   ```
Takes Edward probabilistic program and returns its log joint function.    Args:     model: Python callable which executes the generative process of a       computable probability distribution using `ed.RandomVariable`s.    Returns:     A log-joint probability function. Its inputs are `model`'s original inputs     and random variables which appear during the program execution. Its output     is a scalar tf.Tensor.    #### Examples    Below we define Bayesian logistic regression as an Edward program,   representing the model's generative process. We apply `make_log_joint_fn` in   order to represent the model in terms of its joint probability function.    ```python   from tensorflow_probability import edward2 as ed    def logistic_regression(features):     coeffs = ed.Normal(loc=0., scale=1.,                        sample_shape=features.shape[1], name="coeffs")     outcomes = ed.Bernoulli(logits=tf.tensordot(features, coeffs, [[1], [0]]),                             name="outcomes")     return outcomes    log_joint = ed.make_log_joint_fn(logistic_regression)    features = tf.random_normal([3, 2])   coeffs_value = tf.random_normal([2])   outcomes_value = tf.round(tf.random_uniform([3]))   output = log_joint(features, coeffs=coeffs_value, outcomes=outcomes_value)   ```
Filters inputs to be compatible with function `f`'s signature.    Args:     f: Function according to whose input signature we filter arguments.     src_kwargs: Keyword arguments to filter according to `f`.    Returns:     kwargs: Dict of key-value pairs in `src_kwargs` which exist in `f`'s       signature.
Network block for VGG.
Builds a tree at a given tree depth and at a given state.    The `current` state is immediately adjacent to, but outside of,   the subtrajectory spanned by the returned `forward` and `reverse` states.    Args:     value_and_gradients_fn: Python callable which takes an argument like       `*current_state` and returns a tuple of its (possibly unnormalized)       log-density under the target distribution and its gradient with respect to       each state.     current_state: List of `Tensor`s representing the current states of the       NUTS trajectory.     current_target_log_prob: Scalar `Tensor` representing the value of       `target_log_prob_fn` at the `current_state`.     current_grads_target_log_prob: List of `Tensor`s representing gradient of       `current_target_log_prob` with respect to `current_state`. Must have same       shape as `current_state`.     current_momentum: List of `Tensor`s representing the momentums of       `current_state`. Must have same shape as `current_state`.     direction: int that is either -1 or 1. It determines whether to perform       leapfrog integration backwards (reverse) or forward in time respectively.     depth: non-negative int that indicates how deep of a tree to build.       Each call to `_build_tree` takes `2**depth` leapfrog steps.     step_size: List of `Tensor`s representing the step sizes for the leapfrog       integrator. Must have same shape as `current_state`.     log_slice_sample: The log of an auxiliary slice variable. It is used       together with `max_simulation_error` to avoid simulating trajectories with       too much numerical error.     max_simulation_error: Maximum simulation error to tolerate before       terminating the trajectory. Simulation error is the       `log_slice_sample` minus the log-joint probability at the simulated state.     seed: Integer to seed the random number generator.    Returns:     reverse_state: List of `Tensor`s representing the "reverse" states of the       NUTS trajectory. Has same shape as `current_state`.     reverse_target_log_prob: Scalar `Tensor` representing the value of       `target_log_prob_fn` at the `reverse_state`.     reverse_grads_target_log_prob: List of `Tensor`s representing gradient of       `reverse_target_log_prob` with respect to `reverse_state`. Has same shape       as `reverse_state`.     reverse_momentum: List of `Tensor`s representing the momentums of       `reverse_state`. Has same shape as `reverse_state`.     forward_state: List of `Tensor`s representing the "forward" states of the       NUTS trajectory. Has same shape as `current_state`.     forward_target_log_prob: Scalar `Tensor` representing the value of       `target_log_prob_fn` at the `forward_state`.     forward_grads_target_log_prob: List of `Tensor`s representing gradient of       `forward_target_log_prob` with respect to `forward_state`. Has same shape       as `forward_state`.     forward_momentum: List of `Tensor`s representing the momentums of       `forward_state`. Has same shape as `forward_state`.     next_state: List of `Tensor`s representing the next states of the NUTS       trajectory. Has same shape as `current_state`.     next_target_log_prob: Scalar `Tensor` representing the value of       `target_log_prob_fn` at `next_state`.     next_grads_target_log_prob: List of `Tensor`s representing the gradient of       `next_target_log_prob` with respect to `next_state`.     num_states: Number of acceptable candidate states in the subtree. A state is       acceptable if it is "in the slice", that is, if its log-joint probability       with its momentum is greater than `log_slice_sample`.     continue_trajectory: bool determining whether to continue the simulation       trajectory. The trajectory is continued if no U-turns are encountered       within the built subtree, and if the log-probability accumulation due to       integration error does not exceed `max_simulation_error`.
Wraps value and gradients function to assist with None gradients.
If two given states and momentum do not exhibit a U-turn pattern.
Runs one step of leapfrog integration.
Log-joint probability given a state's log-probability and momentum.
Returns samples from a Bernoulli distribution.
Makes closure which creates `loc`, `scale` params from `tf.get_variable`.    This function produces a closure which produces `loc`, `scale` using   `tf.get_variable`. The closure accepts the following arguments:      dtype: Type of parameter's event.     shape: Python `list`-like representing the parameter's event shape.     name: Python `str` name prepended to any created (or existing)       `tf.Variable`s.     trainable: Python `bool` indicating all created `tf.Variable`s should be       added to the graph collection `GraphKeys.TRAINABLE_VARIABLES`.     add_variable_fn: `tf.get_variable`-like `callable` used to create (or       access existing) `tf.Variable`s.    Args:     is_singular: Python `bool` indicating if `scale is None`. Default: `False`.     loc_initializer: Initializer function for the `loc` parameters.       The default is `tf.random_normal_initializer(mean=0., stddev=0.1)`.     untransformed_scale_initializer: Initializer function for the `scale`       parameters. Default value: `tf.random_normal_initializer(mean=-3.,       stddev=0.1)`. This implies the softplus transformed result is initialized       near `0`. It allows a `Normal` distribution with `scale` parameter set to       this value to approximately act like a point mass.     loc_regularizer: Regularizer function for the `loc` parameters.       The default (`None`) is to use the `tf.get_variable` default.     untransformed_scale_regularizer: Regularizer function for the `scale`       parameters. The default (`None`) is to use the `tf.get_variable` default.     loc_constraint: An optional projection function to be applied to the       loc after being updated by an `Optimizer`. The function must take as input       the unprojected variable and must return the projected variable (which       must have the same shape). Constraints are not safe to use when doing       asynchronous distributed training.       The default (`None`) is to use the `tf.get_variable` default.     untransformed_scale_constraint: An optional projection function to be       applied to the `scale` parameters after being updated by an `Optimizer`       (e.g. used to implement norm constraints or value constraints). The       function must take as input the unprojected variable and must return the       projected variable (which must have the same shape). Constraints are not       safe to use when doing asynchronous distributed training. The default       (`None`) is to use the `tf.get_variable` default.    Returns:     default_loc_scale_fn: Python `callable` which instantiates `loc`, `scale`     parameters from args: `dtype, shape, name, trainable, add_variable_fn`.
Creates a function to build Normal distributions with trainable params.    This function produces a closure which produces `tfd.Normal`   parameterized by a loc` and `scale` each created using `tf.get_variable`.    Args:     is_singular: Python `bool` if `True`, forces the special case limit of       `scale->0`, i.e., a `Deterministic` distribution.     loc_initializer: Initializer function for the `loc` parameters.       The default is `tf.random_normal_initializer(mean=0., stddev=0.1)`.     untransformed_scale_initializer: Initializer function for the `scale`       parameters. Default value: `tf.random_normal_initializer(mean=-3.,       stddev=0.1)`. This implies the softplus transformed result is initialized       near `0`. It allows a `Normal` distribution with `scale` parameter set to       this value to approximately act like a point mass.     loc_regularizer: Regularizer function for the `loc` parameters.     untransformed_scale_regularizer: Regularizer function for the `scale`       parameters.     loc_constraint: An optional projection function to be applied to the       loc after being updated by an `Optimizer`. The function must take as input       the unprojected variable and must return the projected variable (which       must have the same shape). Constraints are not safe to use when doing       asynchronous distributed training.     untransformed_scale_constraint: An optional projection function to be       applied to the `scale` parameters after being updated by an `Optimizer`       (e.g. used to implement norm constraints or value constraints). The       function must take as input the unprojected variable and must return the       projected variable (which must have the same shape). Constraints are not       safe to use when doing asynchronous distributed training.    Returns:     make_normal_fn: Python `callable` which creates a `tfd.Normal`       using from args: `dtype, shape, name, trainable, add_variable_fn`.
Creates multivariate standard `Normal` distribution.    Args:     dtype: Type of parameter's event.     shape: Python `list`-like representing the parameter's event shape.     name: Python `str` name prepended to any created (or existing)       `tf.Variable`s.     trainable: Python `bool` indicating all created `tf.Variable`s should be       added to the graph collection `GraphKeys.TRAINABLE_VARIABLES`.     add_variable_fn: `tf.get_variable`-like `callable` used to create (or       access existing) `tf.Variable`s.    Returns:     Multivariate standard `Normal` distribution.
Deserializes the Keras-serialized function.    (De)serializing Python functions from/to bytecode is unsafe. Therefore we   also use the function's type as an anonymous function ('lambda') or named   function in the Python environment ('function'). In the latter case, this lets   us use the Python scope to obtain the function rather than reload it from   bytecode. (Note that both cases are brittle!)    Keras-deserialized functions do not perform lexical scoping. Any modules that   the function requires must be imported within the function itself.    This serialization mimicks the implementation in `tf.keras.layers.Lambda`.    Args:     serial: Serialized Keras object: typically a dict, string, or bytecode.     function_type: Python string denoting 'function' or 'lambda'.    Returns:     function: Function the serialized Keras object represents.    #### Examples    ```python   serial, function_type = serialize_function(lambda x: x)   function = deserialize_function(serial, function_type)   assert function(2.3) == 2.3  # function is identity   ```
Serializes function for Keras.    (De)serializing Python functions from/to bytecode is unsafe. Therefore we   return the function's type as an anonymous function ('lambda') or named   function in the Python environment ('function'). In the latter case, this lets   us use the Python scope to obtain the function rather than reload it from   bytecode. (Note that both cases are brittle!)    This serialization mimicks the implementation in `tf.keras.layers.Lambda`.    Args:     func: Python function to serialize.    Returns:     (serial, function_type): Serialized object, which is a tuple of its     bytecode (if function is anonymous) or name (if function is named), and its     function type.
Broadcasts `from_structure` to `to_structure`.    This is useful for downstream usage of `zip` or `tf.nest.map_structure`.    If `from_structure` is a singleton, it is tiled to match the structure of   `to_structure`. Note that the elements in `from_structure` are not copied if   this tiling occurs.    Args:     to_structure: A structure.     from_structure: A structure.    Returns:     new_from_structure: Same structure as `to_structure`.    #### Example:    ```python   a_structure = ['a', 'b', 'c']   b_structure = broadcast_structure(a_structure, 'd')   # -> ['d', 'd', 'd']   c_structure = tf.nest.map_structure(       lambda a, b: a + b, a_structure, b_structure)   # -> ['ad', 'bd', 'cd']   ```
Eagerly converts struct to Tensor, recursing upon failure.
Converts `args` to `Tensor`s.    Use this when it is necessary to convert user-provided arguments that will   then be passed to user-provided callables.    When `dtype` is `None` this function behaves as follows:    1A. If the top-level structure is a `list`/`tuple` but not a `namedtuple`,       then it is left as is and only its elements are converted to `Tensor`s.    2A. The sub-structures are converted to `Tensor`s eagerly. E.g. if `args` is       `{'arg': [[1], [2]]}` it is converted to       `{'arg': tf.constant([[1], [2]])}`. If the conversion fails, it will       attempt to recurse into its children.    When `dtype` is specified, it acts as both a structural and numeric type   constraint. `dtype` can be a single `DType`, `None` or a nested collection   thereof. The conversion rule becomes as follows:    1B. The return value of this function will have the same structure as `dtype`.    2B. If the leaf of `dtype` is a concrete `DType`, then the corresponding       sub-structure in `args` is converted to a `Tensor`.    3B. If the leaf of `dtype` is `None`, then the corresponding sub-structure is       converted eagerly as described in the rule 2A above.    Args:     args: Arguments to convert to `Tensor`s.     dtype: Optional structure/numeric type constraint.     name: Optional name-scope to use.    Returns:     args: Converted `args`.    #### Examples.    This table shows some useful conversion cases. `T` means `Tensor`, `NT` means   `namedtuple` and `CNT` means a `namedtuple` with a `Tensor`-conversion   function registered.    |     args     |    dtype   |       output       |   |:------------:|:----------:|:------------------:|   | `{"a": 1}`   | `None`     | `{"a": T(1)}`      |   | `T(1)`       | `None`     | `T(1)`             |   | `[1]`        | `None`     | `[T(1)]`           |   | `[1]`        | `tf.int32` | `T([1])`           |   | `[[T(1)]]`   | `None`     | `[T([1])]`         |   | `[[T(1)]]`   | `[[None]]` | `[[T(1)]]`         |   | `NT(1, 2)`   | `None`     | `NT(T(1), T(2))`   |   | `NT(1, 2)`   | `tf.int32` | `T([1, 2])`        |   | `CNT(1, 2)`  | `None`     | `T(...)`           |   | `[[1, [2]]]` | `None`     | `[[T(1), T([2])]]` |
Calls `fn` with `args`, possibly expanding `args`.    Use this function when calling a user-provided callable using user-provided   arguments.    The expansion rules are as follows:    `fn(*args)` if `args` is a `list` or a `tuple`, but not a `namedtuple`.   `fn(**args)` if `args` is a `dict`.   `fn(args)` otherwise.    Args:     fn: A callable that takes either `args` as an argument(s).     args: Arguments to `fn`.    Returns:     result: Return value of `fn`.
Returns `Tensor` attributes related to shape and Python builtins.
Creates the mixture of Gaussians prior distribution.    Args:     latent_size: The dimensionality of the latent representation.     mixture_components: Number of elements of the mixture.    Returns:     random_prior: A `tfd.Distribution` instance representing the distribution       over encodings in the absence of any evidence.
Helper utility to make a field of images.
Downloads a file.
Builds fake MNIST-style data for unit testing.
Helper to validate block sizes.
Verifies that `parts` don't broadcast.
Constructs a trainable `tfd.MultivariateNormalTriL` distribution.    This function creates a MultivariateNormal (MVN) with lower-triangular scale   matrix. By default the MVN is parameterized via affine transformation of input   tensor `x`. Using default args, this function is mathematically equivalent to:    ```none   Y = MVN(loc=matmul(W, x) + b,           scale_tril=f(reshape_tril(matmul(M, x) + c)))    where,     W in R^[d, n]     M in R^[d*(d+1)/2, n]     b in R^d     c in R^d     f(S) = set_diag(S, softplus(matrix_diag_part(S)) + 1e-5)   ```    Observe that `f` makes the diagonal of the triangular-lower scale matrix be   positive and no smaller than `1e-5`.    #### Examples    ```python   # This example fits a multilinear regression loss.   import tensorflow as tf   import tensorflow_probability as tfp    # Create fictitious training data.   dtype = np.float32   n = 3000    # number of samples   x_size = 4  # size of single x   y_size = 2  # size of single y   def make_training_data():     np.random.seed(142)     x = np.random.randn(n, x_size).astype(dtype)     w = np.random.randn(x_size, y_size).astype(dtype)     b = np.random.randn(1, y_size).astype(dtype)     true_mean = np.tensordot(x, w, axes=[[-1], [0]]) + b     noise = np.random.randn(n, y_size).astype(dtype)     y = true_mean + noise     return y, x   y, x = make_training_data()    # Build TF graph for fitting MVNTriL maximum likelihood estimator.   mvn = tfp.trainable_distributions.multivariate_normal_tril(x, dims=y_size)   loss = -tf.reduce_mean(mvn.log_prob(y))   train_op = tf.train.AdamOptimizer(learning_rate=2.**-3).minimize(loss)   mse = tf.reduce_mean(tf.squared_difference(y, mvn.mean()))   init_op = tf.global_variables_initializer()    # Run graph 1000 times.   num_steps = 1000   loss_ = np.zeros(num_steps)   # Style: `_` to indicate sess.run result.   mse_ = np.zeros(num_steps)   with tf.Session() as sess:     sess.run(init_op)     for it in xrange(loss_.size):       _, loss_[it], mse_[it] = sess.run([train_op, loss, mse])       if it % 200 == 0 or it == loss_.size - 1:         print("iteration:{}  loss:{}  mse:{}".format(it, loss_[it], mse_[it]))    # ==> iteration:0    loss:38.2020797729  mse:4.17175960541   #     iteration:200  loss:2.90179634094  mse:0.990987896919   #     iteration:400  loss:2.82727336884  mse:0.990926623344   #     iteration:600  loss:2.82726788521  mse:0.990926682949   #     iteration:800  loss:2.82726788521  mse:0.990926682949   #     iteration:999  loss:2.82726788521  mse:0.990926682949   ```    Args:     x: `Tensor` with floating type. Must have statically defined rank and       statically known right-most dimension.     dims: Scalar, `int`, `Tensor` indicated the MVN event size, i.e., the       created MVN will be distribution over length-`dims` vectors.     layer_fn: Python `callable` which takes input `x` and `int` scalar `d` and       returns a transformation of `x` with shape       `tf.concat([tf.shape(x)[:-1], [d]], axis=0)`.       Default value: `tf.layers.dense`.     loc_fn: Python `callable` which transforms the `loc` parameter. Takes a       (batch of) length-`dims` vectors and returns a `Tensor` of same shape and       `dtype`.       Default value: `lambda x: x`.     scale_fn: Python `callable` which transforms the `scale` parameters. Takes a       (batch of) length-`dims * (dims + 1) / 2` vectors and returns a       lower-triangular `Tensor` of same batch shape with rightmost dimensions       having shape `[dims, dims]`.       Default value: `tril_with_diag_softplus_and_shift`.     name: A `name_scope` name for operations created by this function.       Default value: `None` (i.e., "multivariate_normal_tril").    Returns:     mvntril: An instance of `tfd.MultivariateNormalTriL`.
Constructs a trainable `tfd.Bernoulli` distribution.    This function creates a Bernoulli distribution parameterized by logits.   Using default args, this function is mathematically equivalent to:    ```none   Y = Bernoulli(logits=matmul(W, x) + b)    where,     W in R^[d, n]     b in R^d   ```    #### Examples    This function can be used as a [logistic regression](   https://en.wikipedia.org/wiki/Logistic_regression) loss.    ```python   # This example fits a logistic regression loss.   import tensorflow as tf   import tensorflow_probability as tfp    # Create fictitious training data.   dtype = np.float32   n = 3000    # number of samples   x_size = 4  # size of single x   def make_training_data():     np.random.seed(142)     x = np.random.randn(n, x_size).astype(dtype)     w = np.random.randn(x_size).astype(dtype)     b = np.random.randn(1).astype(dtype)     true_logits = np.tensordot(x, w, axes=[[-1], [-1]]) + b     noise = np.random.logistic(size=n).astype(dtype)     y = dtype(true_logits + noise > 0.)     return y, x   y, x = make_training_data()    # Build TF graph for fitting Bernoulli maximum likelihood estimator.   bernoulli = tfp.trainable_distributions.bernoulli(x)   loss = -tf.reduce_mean(bernoulli.log_prob(y))   train_op = tf.train.AdamOptimizer(learning_rate=2.**-5).minimize(loss)   mse = tf.reduce_mean(tf.squared_difference(y, bernoulli.mean()))   init_op = tf.global_variables_initializer()    # Run graph 1000 times.   num_steps = 1000   loss_ = np.zeros(num_steps)   # Style: `_` to indicate sess.run result.   mse_ = np.zeros(num_steps)   with tf.Session() as sess:     sess.run(init_op)     for it in xrange(loss_.size):       _, loss_[it], mse_[it] = sess.run([train_op, loss, mse])       if it % 200 == 0 or it == loss_.size - 1:         print("iteration:{}  loss:{}  mse:{}".format(it, loss_[it], mse_[it]))    # ==> iteration:0    loss:0.635675370693  mse:0.222526371479   #     iteration:200  loss:0.440077394247  mse:0.143687799573   #     iteration:400  loss:0.440077394247  mse:0.143687844276   #     iteration:600  loss:0.440077394247  mse:0.143687844276   #     iteration:800  loss:0.440077424049  mse:0.143687844276   #     iteration:999  loss:0.440077424049  mse:0.143687844276   ```    Args:     x: `Tensor` with floating type. Must have statically defined rank and       statically known right-most dimension.     layer_fn: Python `callable` which takes input `x` and `int` scalar `d` and       returns a transformation of `x` with shape       `tf.concat([tf.shape(x)[:-1], [1]], axis=0)`.       Default value: `tf.layers.dense`.     name: A `name_scope` name for operations created by this function.       Default value: `None` (i.e., "bernoulli").    Returns:     bernoulli: An instance of `tfd.Bernoulli`.
Constructs a trainable `tfd.Normal` distribution.     This function creates a Normal distribution parameterized by loc and scale.   Using default args, this function is mathematically equivalent to:    ```none   Y = Normal(loc=matmul(W, x) + b, scale=1)    where,     W in R^[d, n]     b in R^d   ```    #### Examples    This function can be used as a [linear regression](   https://en.wikipedia.org/wiki/Linear_regression) loss.    ```python   # This example fits a linear regression loss.   import tensorflow as tf   import tensorflow_probability as tfp    # Create fictitious training data.   dtype = np.float32   n = 3000    # number of samples   x_size = 4  # size of single x   def make_training_data():     np.random.seed(142)     x = np.random.randn(n, x_size).astype(dtype)     w = np.random.randn(x_size).astype(dtype)     b = np.random.randn(1).astype(dtype)     true_mean = np.tensordot(x, w, axes=[[-1], [-1]]) + b     noise = np.random.randn(n).astype(dtype)     y = true_mean + noise     return y, x   y, x = make_training_data()    # Build TF graph for fitting Normal maximum likelihood estimator.   normal = tfp.trainable_distributions.normal(x)   loss = -tf.reduce_mean(normal.log_prob(y))   train_op = tf.train.AdamOptimizer(learning_rate=2.**-5).minimize(loss)   mse = tf.reduce_mean(tf.squared_difference(y, normal.mean()))   init_op = tf.global_variables_initializer()    # Run graph 1000 times.   num_steps = 1000   loss_ = np.zeros(num_steps)   # Style: `_` to indicate sess.run result.   mse_ = np.zeros(num_steps)   with tf.Session() as sess:     sess.run(init_op)     for it in xrange(loss_.size):       _, loss_[it], mse_[it] = sess.run([train_op, loss, mse])       if it % 200 == 0 or it == loss_.size - 1:         print("iteration:{}  loss:{}  mse:{}".format(it, loss_[it], mse_[it]))    # ==> iteration:0    loss:6.34114170074  mse:10.8444051743   #     iteration:200  loss:1.40146839619  mse:0.965059816837   #     iteration:400  loss:1.40052902699  mse:0.963181257248   #     iteration:600  loss:1.40052902699  mse:0.963181257248   #     iteration:800  loss:1.40052902699  mse:0.963181257248   #     iteration:999  loss:1.40052902699  mse:0.963181257248   ```    Args:     x: `Tensor` with floating type. Must have statically defined rank and       statically known right-most dimension.     layer_fn: Python `callable` which takes input `x` and `int` scalar `d` and       returns a transformation of `x` with shape       `tf.concat([tf.shape(x)[:-1], [1]], axis=0)`.       Default value: `tf.layers.dense`.     loc_fn: Python `callable` which transforms the `loc` parameter. Takes a       (batch of) length-`dims` vectors and returns a `Tensor` of same shape and       `dtype`.       Default value: `lambda x: x`.     scale_fn: Python `callable` or `Tensor`. If a `callable` transforms the       `scale` parameters; if `Tensor` is the `tfd.Normal` `scale` argument.       Takes a (batch of) length-`dims` vectors and returns a `Tensor` of same       size. (Taking a `callable` or `Tensor` is how `tf.Variable` intializers       behave.)       Default value: `1`.     name: A `name_scope` name for operations created by this function.       Default value: `None` (i.e., "normal").    Returns:     normal: An instance of `tfd.Normal`.
Constructs a trainable `tfd.Poisson` distribution.    This function creates a Poisson distribution parameterized by log rate.   Using default args, this function is mathematically equivalent to:    ```none   Y = Poisson(log_rate=matmul(W, x) + b)    where,     W in R^[d, n]     b in R^d   ```    #### Examples    This can be used as a [Poisson regression](   https://en.wikipedia.org/wiki/Poisson_regression) loss.    ```python   # This example fits a poisson regression loss.   import numpy as np   import tensorflow as tf   import tensorflow_probability as tfp    # Create fictitious training data.   dtype = np.float32   n = 3000    # number of samples   x_size = 4  # size of single x   def make_training_data():     np.random.seed(142)     x = np.random.randn(n, x_size).astype(dtype)     w = np.random.randn(x_size).astype(dtype)     b = np.random.randn(1).astype(dtype)     true_log_rate = np.tensordot(x, w, axes=[[-1], [-1]]) + b     y = np.random.poisson(lam=np.exp(true_log_rate)).astype(dtype)     return y, x   y, x = make_training_data()    # Build TF graph for fitting Poisson maximum likelihood estimator.   poisson = tfp.trainable_distributions.poisson(x)   loss = -tf.reduce_mean(poisson.log_prob(y))   train_op = tf.train.AdamOptimizer(learning_rate=2.**-5).minimize(loss)   mse = tf.reduce_mean(tf.squared_difference(y, poisson.mean()))   init_op = tf.global_variables_initializer()    # Run graph 1000 times.   num_steps = 1000   loss_ = np.zeros(num_steps)   # Style: `_` to indicate sess.run result.   mse_ = np.zeros(num_steps)   with tf.Session() as sess:     sess.run(init_op)     for it in xrange(loss_.size):       _, loss_[it], mse_[it] = sess.run([train_op, loss, mse])       if it % 200 == 0 or it == loss_.size - 1:         print("iteration:{}  loss:{}  mse:{}".format(it, loss_[it], mse_[it]))    # ==> iteration:0    loss:37.0814208984  mse:6359.41259766   #     iteration:200  loss:1.42010736465  mse:40.7654914856   #     iteration:400  loss:1.39027583599  mse:8.77660560608   #     iteration:600  loss:1.3902695179   mse:8.78443241119   #     iteration:800  loss:1.39026939869  mse:8.78443622589   #     iteration:999  loss:1.39026939869  mse:8.78444766998   ```    Args:     x: `Tensor` with floating type. Must have statically defined rank and       statically known right-most dimension.     layer_fn: Python `callable` which takes input `x` and `int` scalar `d` and       returns a transformation of `x` with shape       `tf.concat([tf.shape(x)[:-1], [1]], axis=0)`.       Default value: `tf.layers.dense`.     log_rate_fn: Python `callable` which transforms the `log_rate` parameter.       Takes a (batch of) length-`dims` vectors and returns a `Tensor` of same       shape and `dtype`.       Default value: `lambda x: x`.     name: A `name_scope` name for operations created by this function.       Default value: `None` (i.e., "poisson").    Returns:     poisson: An instance of `tfd.Poisson`.
Applies one step of Euler-Maruyama method.    Generates proposal of the form:    ```python   tfd.Normal(loc=state_parts + _get_drift(state_parts, ...),              scale=tf.sqrt(step_size * volatility_fn(current_state)))   ```    `_get_drift(state_parts, ..)` is a diffusion drift value at `state_parts`.     Args:     random_draw_parts: Python `list` of `Tensor`s containing the value(s) of the       random perturbation variable(s). Must broadcast with the shape of       `state_parts`.     state_parts: Python `list` of `Tensor`s representing the current       state(s) of the Markov chain(s).     drift_parts: Python `list` of `Tensor`s representing value of the drift       `_get_drift(*state_parts, ..)`. Must broadcast with the shape of       `state_parts`.     step_size_parts: Python `list` of `Tensor`s representing the step size for       the Euler-Maruyama method. Must broadcast with the shape of       `state_parts`.  Larger step sizes lead to faster progress, but       too-large step sizes make rejection exponentially more likely. When       possible, it's often helpful to match per-variable step sizes to the       standard deviations of the target distribution in each variable.     volatility_parts: Python `list` of `Tensor`s representing the value of       `volatility_fn(*state_parts)`. Must broadcast with the shape of       `state_parts`.     name: Python `str` name prefixed to Ops created by this function.       Default value: `None` (i.e., 'mala_euler_method').    Returns:     proposed_state_parts: Tensor or Python list of `Tensor`s representing the       state(s) of the Markov chain(s) at each result step. Has same shape as       input `current_state_parts`.
Compute diffusion drift at the current location `current_state`.    The drift of the diffusion at is computed as    ```none   0.5 * `step_size` * volatility_parts * `target_log_prob_fn(current_state)`   + `step_size` * `grads_volatility`   ```    where `volatility_parts` = `volatility_fn(current_state)**2` and   `grads_volatility` is a gradient of `volatility_parts` at the `current_state`.    Args:     step_size_parts: Python `list` of `Tensor`s representing the step size for       Euler-Maruyama method. Must broadcast with the shape of       `volatility_parts`.  Larger step sizes lead to faster progress, but       too-large step sizes make rejection exponentially more likely. When       possible, it's often helpful to match per-variable step sizes to the       standard deviations of the target distribution in each variable.     volatility_parts: Python `list` of `Tensor`s representing the value of       `volatility_fn(*state_parts)`.     grads_volatility: Python list of `Tensor`s representing the value of the       gradient of `volatility_parts**2` wrt the state of the chain.     grads_target_log_prob: Python list of `Tensor`s representing       gradient of `target_log_prob_fn(*state_parts`) wrt `state_parts`. Must       have same shape as `volatility_parts`.     name: Python `str` name prefixed to Ops created by this function.       Default value: `None` (i.e., 'mala_get_drift').    Returns:     drift_parts: Tensor or Python list of `Tensor`s representing the       state(s) of the Markov chain(s) at each result step. Has same shape as       input `current_state_parts`.
r"""Helper to `kernel` which computes the log acceptance-correction.    Computes `log_acceptance_correction` as described in `MetropolisHastings`   class. The proposal density is normal. More specifically,     ```none   q(proposed_state | current_state) \sim N(current_state + current_drift,   step_size * current_volatility**2)    q(current_state | proposed_state) \sim N(proposed_state + proposed_drift,   step_size * proposed_volatility**2)   ```    The `log_acceptance_correction` is then    ```none   log_acceptance_correctio = q(current_state | proposed_state)   - q(proposed_state | current_state)   ```    Args:     current_state_parts: Python `list` of `Tensor`s representing the value(s) of       the current state of the chain.     proposed_state_parts:  Python `list` of `Tensor`s representing the value(s)       of the proposed state of the chain. Must broadcast with the shape of       `current_state_parts`.     current_volatility_parts: Python `list` of `Tensor`s representing the value       of `volatility_fn(*current_volatility_parts)`. Must broadcast with the       shape of `current_state_parts`.     proposed_volatility_parts: Python `list` of `Tensor`s representing the value       of `volatility_fn(*proposed_volatility_parts)`. Must broadcast with the       shape of `current_state_parts`     current_drift_parts: Python `list` of `Tensor`s representing value of the       drift `_get_drift(*current_state_parts, ..)`. Must broadcast with the       shape of `current_state_parts`.     proposed_drift_parts: Python `list` of `Tensor`s representing value of the       drift `_get_drift(*proposed_drift_parts, ..)`. Must broadcast with the       shape of `current_state_parts`.     step_size_parts: Python `list` of `Tensor`s representing the step size for       Euler-Maruyama method. Must broadcast with the shape of       `current_state_parts`.     independent_chain_ndims: Scalar `int` `Tensor` representing the number of       leftmost `Tensor` dimensions which index independent chains.     name: Python `str` name prefixed to Ops created by this function.       Default value: `None` (i.e., 'compute_log_acceptance_correction').    Returns:     log_acceptance_correction: `Tensor` representing the `log`       acceptance-correction.  (See docstring for mathematical definition.)
Helper which computes `volatility_fn` results and grads, if needed.
Helper to broadcast `volatility_parts` to the shape of `state_parts`.
Build transition matrix for an autoregressive StateSpaceModel.    When applied to a vector of previous values, this matrix computes   the expected new value (summing the previous states according to the   autoregressive coefficients) in the top dimension of the state space,   and moves all previous values down by one dimension, 'forgetting' the   final (least recent) value. That is, it looks like this:    ```   ar_matrix = [ coefs[0], coefs[1], ..., coefs[order]                 1.,       0 ,       ..., 0.                 0.,       1.,       ..., 0.                 ...                 0.,       0.,  ..., 1.,  0.            ]   ```    Args:     coefficients: float `Tensor` of shape `concat([batch_shape, [order]])`.    Returns:     ar_matrix: float `Tensor` with shape `concat([batch_shape,     [order, order]])`.
Computes graph and static `sample_shape`.
Calls `fn`, appropriately reshaping its input `x` and output.
Calls `fn` and appropriately reshapes its output.
The binomial cumulative distribution function.    Args:     k: floating point `Tensor`.     n: floating point `Tensor`.     p: floating point `Tensor`.    Returns:     `sum_{j=0}^k p^j (1 - p)^(n - j)`.
Executes `model`, creating both samples and distributions.
Latent Dirichlet Allocation in terms of its generative process.    The model posits a distribution over bags of words and is parameterized by   a concentration and the topic-word probabilities. It collapses per-word   topic assignments.    Args:     concentration: A Tensor of shape [1, num_topics], which parameterizes the       Dirichlet prior over topics.     topics_words: A Tensor of shape [num_topics, num_words], where each row       (topic) denotes the probability of each word being in that topic.    Returns:     bag_of_words: A random variable capturing a sample from the model, of shape       [1, num_words]. It represents one generated document as a bag of words.
Creates the variational distribution for LDA.    Args:     activation: Activation function to use.     num_topics: The number of topics.     layer_sizes: The number of hidden units per layer in the encoder.    Returns:     lda_variational: A function that takes a bag-of-words Tensor as       input and returns a distribution over topics.
Returns the summary of the learned topics.    Arguments:     topics_words: KxV tensor with topics as rows and words as columns.     alpha: 1xK tensor of prior Dirichlet concentrations for the         topics.     vocabulary: A mapping of word's integer index to the corresponding string.     topics_to_print: The number of topics with highest prior weight to         summarize.     words_per_topic: Number of wodrs per topic to return.    Returns:     summary: A np.array with strings.
20 newsgroups as a tf.data.Dataset.
Builds fake data for unit testing.
Builds iterators for train and evaluation data.    Each object is represented as a bag-of-words vector.    Arguments:     data_dir: Folder in which to store the data.     batch_size: Batch size for both train and evaluation.    Returns:     train_input_fn: A function that returns an iterator over the training data.     eval_input_fn: A function that returns an iterator over the evaluation data.     vocabulary: A mapping of word's integer index to the corresponding string.
Minimize using Hessian-informed proximal gradient descent.    This function solves the regularized minimization problem    ```none   argmin{ Loss(x)             + l1_regularizer * ||x||_1             + l2_regularizer * ||x||_2**2           : x in R^n }   ```    where `Loss` is a convex C^2 function (typically, `Loss` is the negative log   likelihood of a model and `x` is a vector of model coefficients).  The `Loss`   function does not need to be supplied directly, but this optimizer does need a   way to compute the gradient and Hessian of the Loss function at a given value   of `x`.  The gradient and Hessian are often computationally expensive, and   this optimizer calls them relatively few times compared with other algorithms.    Args:     grad_and_hessian_loss_fn: callable that takes as input a (batch of) `Tensor`       of the same shape and dtype as `x_start` and returns the triple       `(gradient_unregularized_loss, hessian_unregularized_loss_outer,       hessian_unregularized_loss_middle)` as defined in the argument spec of       `minimize_one_step`.     x_start: (Batch of) vector-shaped, `float` `Tensor` representing the initial       value of the argument to the `Loss` function.     tolerance: scalar, `float` `Tensor` representing the tolerance for each       optimization step; see the `tolerance` argument of       `minimize_one_step`.     l1_regularizer: scalar, `float` `Tensor` representing the weight of the L1       regularization term (see equation above).     l2_regularizer: scalar, `float` `Tensor` representing the weight of the L2       regularization term (see equation above).       Default value: `None` (i.e., no L2 regularization).     maximum_iterations: Python integer specifying the maximum number of       iterations of the outer loop of the optimizer.  After this many iterations       of the outer loop, the algorithm will terminate even if the return value       `optimal_x` has not converged.       Default value: `1`.     maximum_full_sweeps_per_iteration: Python integer specifying the maximum       number of sweeps allowed in each iteration of the outer loop of the       optimizer.  Passed as the `maximum_full_sweeps` argument to       `minimize_one_step`.       Default value: `1`.     learning_rate: scalar, `float` `Tensor` representing a multiplicative factor       used to dampen the proximal gradient descent steps.       Default value: `None` (i.e., factor is conceptually `1`).     name: Python string representing the name of the TensorFlow operation.       The default name is `"minimize"`.    Returns:     x: `Tensor` of the same shape and dtype as `x_start`, representing the       (batches of) computed values of `x` which minimizes `Loss(x)`.     is_converged: scalar, `bool` `Tensor` indicating whether the minimization       procedure converged within the specified number of iterations across all       batches.  Here convergence means that an iteration of the inner loop       (`minimize_one_step`) returns `True` for its `is_converged` output value.     iter: scalar, `int` `Tensor` indicating the actual number of iterations of       the outer loop of the optimizer completed (i.e., number of calls to       `minimize_one_step` before achieving convergence).    #### References    [1]: Jerome Friedman, Trevor Hastie and Rob Tibshirani. Regularization Paths        for Generalized Linear Models via Coordinate Descent. _Journal of        Statistical Software_, 33(1), 2010.        https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf    [2]: Guo-Xun Yuan, Chia-Hua Ho and Chih-Jen Lin. An Improved GLMNET for        L1-regularized Logistic Regression. _Journal of Machine Learning        Research_, 13, 2012.        http://www.jmlr.org/papers/volume13/yuan12a/yuan12a.pdf
Add control dependencies to the commmitment loss to update the codebook.    Args:     vector_quantizer: An instance of the VectorQuantizer class.     one_hot_assignments: The one-hot vectors corresponding to the matched       codebook entry for each code in the batch.     codes: A `float`-like `Tensor` containing the latent vectors to be compared       to the codebook.     commitment_loss: The commitment loss from comparing the encoder outputs to       their neighboring codebook entries.     decay: Decay factor for exponential moving average.    Returns:     commitment_loss: Commitment loss with control dependencies.
Helper method to save a grid of images to a PNG file.    Args:     x: A numpy array of shape [n_images, height, width].     fname: The filename to write to (including extension).
Helper method to save images visualizing model reconstructions.    Args:     images_val: Numpy array containing a batch of input images.     reconstructed_images_val: Numpy array giving the expected output       (mean) of the decoder.     random_images_val: Optionally, a Numpy array giving the expected output       (mean) of decoding samples from the prior, or `None`.     log_dir: The directory to write images (Python `str`).     prefix: A specific label for the saved visualizations, which       determines their filenames (Python `str`).     viz_n: The number of images from each batch to visualize (Python `int`).
Returns Hugo Larochelle's binary static MNIST tf.data.Dataset.
Returns a `np.dtype` based on this `dtype`.
Returns a non-reference `dtype` based on this `dtype`.
Returns whether this is a boolean data type.
Returns whether this is a complex floating point type.
Returns the maximum representable value in this data type.
Returns the string name for this `dtype`.
Returns the number of bytes to represent this `dtype`.
r"""Asserts all items are of the same base type.    Args:     items: List of graph items (e.g., `Variable`, `Tensor`, `SparseTensor`,         `Operation`, or `IndexedSlices`). Can include `None` elements, which         will be ignored.     expected_type: Expected type. If not specified, assert all items are         of the same base type.    Returns:     Validated type, or none if neither expected_type nor items provided.    Raises:     ValueError: If any types do not match.
Validate and return float type based on `tensors` and `dtype`.    For ops such as matrix multiplication, inputs and weights must be of the   same float type. This function validates that all `tensors` are the same type,   validates that type is `dtype` (if supplied), and returns the type. Type must   be a floating point type. If neither `tensors` nor `dtype` is supplied,   the function will return `dtypes.float32`.    Args:     tensors: Tensors of input values. Can include `None` elements, which will       be ignored.     dtype: Expected type.    Returns:     Validated type.    Raises:     ValueError: if neither `tensors` nor `dtype` is supplied, or result is not       float, or the common type of the inputs is not a floating point type.
Minimum of the objective function using the Nelder Mead simplex algorithm.    Performs an unconstrained minimization of a (possibly non-smooth) function   using the Nelder Mead simplex method. Nelder Mead method does not support   univariate functions. Hence the dimensions of the domain must be 2 or greater.   For details of the algorithm, see   [Press, Teukolsky, Vetterling and Flannery(2007)][1].    Points in the domain of the objective function may be represented as a   `Tensor` of general shape but with rank at least 1. The algorithm proceeds   by modifying a full rank simplex in the domain. The initial simplex may   either be specified by the user or can be constructed using a single vertex   supplied by the user. In the latter case, if `v0` is the supplied vertex,   the simplex is the convex hull of the set:    ```None   S = {v0} + {v0 + step_i * e_i}   ```    Here `e_i` is a vector which is `1` along the `i`-th axis and zero elsewhere   and `step_i` is a characteristic length scale along the `i`-th axis. If the   step size is not supplied by the user, a unit step size is used in every axis.   Alternately, a single step size may be specified which is used for every   axis. The most flexible option is to supply a bespoke step size for every   axis.    ### Usage:    The following example demonstrates the usage of the Nelder Mead minimzation   on a two dimensional problem with the minimum located at a non-differentiable   point.    ```python     # The objective function     def sqrt_quadratic(x):       return tf.sqrt(tf.reduce_sum(x ** 2, axis=-1))      start = tf.constant([6.0, -21.0])  # Starting point for the search.     optim_results = tfp.optimizer.nelder_mead_minimize(         sqrt_quadratic, initial_vertex=start, func_tolerance=1e-8,         batch_evaluate_objective=True)      with tf.Session() as session:       results = session.run(optim_results)       # Check that the search converged       assert(results.converged)       # Check that the argmin is close to the actual value.       np.testing.assert_allclose(results.position, np.array([0.0, 0.0]),                                  atol=1e-7)       # Print out the total number of function evaluations it took.       print ("Function evaluations: %d" % results.num_objective_evaluations)   ```    ### References:   [1]: William Press, Saul Teukolsky, William Vetterling and Brian Flannery.     Numerical Recipes in C++, third edition. pp. 502-507. (2007).     http://numerical.recipes/cpppages/chap0sel.pdf    [2]: Jeffrey Lagarias, James Reeds, Margaret Wright and Paul Wright.     Convergence properties of the Nelder-Mead simplex method in low dimensions,     Siam J. Optim., Vol 9, No. 1, pp. 112-147. (1998).     http://www.math.kent.edu/~reichel/courses/Opt/reading.material.2/nelder.mead.pdf    [3]: Fuchang Gao and Lixing Han. Implementing the Nelder-Mead simplex     algorithm with adaptive parameters. Computational Optimization and     Applications, Vol 51, Issue 1, pp 259-277. (2012).     https://pdfs.semanticscholar.org/15b4/c4aa7437df4d032c6ee6ce98d6030dd627be.pdf    Args:     objective_function:  A Python callable that accepts a point as a       real `Tensor` and returns a `Tensor` of real dtype containing       the value of the function at that point. The function       to be minimized. If `batch_evaluate_objective` is `True`, the callable       may be evaluated on a `Tensor` of shape `[n+1] + s ` where `n` is       the dimension of the problem and `s` is the shape of a single point       in the domain (so `n` is the size of a `Tensor` representing a       single point).       In this case, the expected return value is a `Tensor` of shape `[n+1]`.       Note that this method does not support univariate functions so the problem       dimension `n` must be strictly greater than 1.     initial_simplex: (Optional) `Tensor` of real dtype. The initial simplex to       start the search. If supplied, should be a `Tensor` of shape `[n+1] + s`       where `n` is the dimension of the problem and `s` is the shape of a       single point in the domain. Each row (i.e. the `Tensor` with a given       value of the first index) is interpreted as a vertex of a simplex and       hence the rows must be affinely independent. If not supplied, an axes       aligned simplex is constructed using the `initial_vertex` and       `step_sizes`. Only one and at least one of `initial_simplex` and       `initial_vertex` must be supplied.     initial_vertex: (Optional) `Tensor` of real dtype and any shape that can       be consumed by the `objective_function`. A single point in the domain that       will be used to construct an axes aligned initial simplex.     step_sizes: (Optional) `Tensor` of real dtype and shape broadcasting       compatible with `initial_vertex`. Supplies the simplex scale along each       axes. Only used if `initial_simplex` is not supplied. See description       above for details on how step sizes and initial vertex are used to       construct the initial simplex.     objective_at_initial_simplex: (Optional) Rank `1` `Tensor` of real dtype       of a rank `1` `Tensor`. The value of the objective function at the       initial simplex. May be supplied only if `initial_simplex` is       supplied. If not supplied, it will be computed.     objective_at_initial_vertex: (Optional) Scalar `Tensor` of real dtype. The       value of the objective function at the initial vertex. May be supplied       only if the `initial_vertex` is also supplied.     batch_evaluate_objective: (Optional) Python `bool`. If True, the objective       function will be evaluated on all the vertices of the simplex packed       into a single tensor. If False, the objective will be mapped across each       vertex separately. Evaluating the objective function in a batch allows       use of vectorization and should be preferred if the objective function       allows it.     func_tolerance: (Optional) Scalar `Tensor` of real dtype. The algorithm       stops if the absolute difference between the largest and the smallest       function value on the vertices of the simplex is below this number.     position_tolerance: (Optional) Scalar `Tensor` of real dtype. The       algorithm stops if the largest absolute difference between the       coordinates of the vertices is below this threshold.     parallel_iterations: (Optional) Positive integer. The number of iterations       allowed to run in parallel.     max_iterations: (Optional) Scalar positive `Tensor` of dtype `int32`.       The maximum number of iterations allowed. If `None` then no limit is       applied.     reflection: (Optional) Positive Scalar `Tensor` of same dtype as       `initial_vertex`. This parameter controls the scaling of the reflected       vertex. See, [Press et al(2007)][1] for details. If not specified,       uses the dimension dependent prescription of [Gao and Han(2012)][3].     expansion: (Optional) Positive Scalar `Tensor` of same dtype as       `initial_vertex`. Should be greater than `1` and `reflection`. This       parameter controls the expanded scaling of a reflected vertex.       See, [Press et al(2007)][1] for details. If not specified, uses the       dimension dependent prescription of [Gao and Han(2012)][3].     contraction: (Optional) Positive scalar `Tensor` of same dtype as       `initial_vertex`. Must be between `0` and `1`. This parameter controls       the contraction of the reflected vertex when the objective function at       the reflected point fails to show sufficient decrease.       See, [Press et al(2007)][1] for more details. If not specified, uses       the dimension dependent prescription of [Gao and Han(2012][3].     shrinkage: (Optional) Positive scalar `Tensor` of same dtype as       `initial_vertex`. Must be between `0` and `1`. This parameter is the scale       by which the simplex is shrunk around the best point when the other       steps fail to produce improvements.       See, [Press et al(2007)][1] for more details. If not specified, uses       the dimension dependent prescription of [Gao and Han(2012][3].     name: (Optional) Python str. The name prefixed to the ops created by this       function. If not supplied, the default name 'minimize' is used.    Returns:     optimizer_results: A namedtuple containing the following items:       converged: Scalar boolean tensor indicating whether the minimum was         found within tolerance.       num_objective_evaluations: The total number of objective         evaluations performed.       position: A `Tensor` containing the last argument value found         during the search. If the search converged, then         this value is the argmin of the objective function.       objective_value: A tensor containing the value of the objective         function at the `position`. If the search         converged, then this is the (local) minimum of         the objective function.       final_simplex: The last simplex constructed before stopping.       final_objective_values: The objective function evaluated at the         vertices of the final simplex.       initial_simplex: The starting simplex.       initial_objective_values: The objective function evaluated at the         vertices of the initial simplex.       num_iterations: The number of iterations of the main algorithm body.    Raises:     ValueError: If any of the following conditions hold       1. If none or more than one of `initial_simplex` and `initial_vertex` are         supplied.       2. If `initial_simplex` and `step_sizes` are both specified.
A single iteration of the Nelder Mead algorithm.
Creates the condition function pair for a reflection to be accepted.
Creates the condition function pair for an expansion.
Creates the condition function pair for an outside contraction.
Shrinks the simplex around the best vertex.
Replaces an element at supplied index.
Returns True if the simplex has converged.    If the simplex size is smaller than the `position_tolerance` or the variation   of the function value over the vertices of the simplex is smaller than the   `func_tolerance` return True else False.    Args:     simplex: `Tensor` of real dtype. The simplex to test for convergence. For       more details, see the docstring for `initial_simplex` argument       of `minimize`.     best_vertex: `Tensor` of real dtype and rank one less than `simplex`. The       vertex with the best (i.e. smallest) objective value.     best_objective: Scalar `Tensor` of real dtype. The best (i.e. smallest)       value of the objective function at a vertex.     worst_objective: Scalar `Tensor` of same dtype as `best_objective`. The       worst (i.e. largest) value of the objective function at a vertex.     func_tolerance: Scalar positive `Tensor`. The tolerance for the variation       of the objective function value over the simplex. If the variation over       the simplex vertices is below this threshold, convergence is True.     position_tolerance: Scalar positive `Tensor`. The algorithm stops if the       lengths (under the supremum norm) of edges connecting to the best vertex       are below this threshold.    Returns:     has_converged: A scalar boolean `Tensor` indicating whether the algorithm       is deemed to have converged.
Computes the initial simplex and the objective values at the simplex.    Args:     objective_function:  A Python callable that accepts a point as a       real `Tensor` and returns a `Tensor` of real dtype containing       the value of the function at that point. The function       to be evaluated at the simplex. If `batch_evaluate_objective` is `True`,       the callable may be evaluated on a `Tensor` of shape `[n+1] + s `       where `n` is the dimension of the problem and `s` is the shape of a       single point in the domain (so `n` is the size of a `Tensor`       representing a single point).       In this case, the expected return value is a `Tensor` of shape `[n+1]`.     initial_simplex: None or `Tensor` of real dtype. The initial simplex to       start the search. If supplied, should be a `Tensor` of shape `[n+1] + s`       where `n` is the dimension of the problem and `s` is the shape of a       single point in the domain. Each row (i.e. the `Tensor` with a given       value of the first index) is interpreted as a vertex of a simplex and       hence the rows must be affinely independent. If not supplied, an axes       aligned simplex is constructed using the `initial_vertex` and       `step_sizes`. Only one and at least one of `initial_simplex` and       `initial_vertex` must be supplied.     initial_vertex: None or `Tensor` of real dtype and any shape that can       be consumed by the `objective_function`. A single point in the domain that       will be used to construct an axes aligned initial simplex.     step_sizes: None or `Tensor` of real dtype and shape broadcasting       compatible with `initial_vertex`. Supplies the simplex scale along each       axes. Only used if `initial_simplex` is not supplied. See the docstring       of `minimize` for more details.     objective_at_initial_simplex: None or rank `1` `Tensor` of real dtype.       The value of the objective function at the initial simplex.       May be supplied only if `initial_simplex` is       supplied. If not supplied, it will be computed.     objective_at_initial_vertex: None or scalar `Tensor` of real dtype. The       value of the objective function at the initial vertex. May be supplied       only if the `initial_vertex` is also supplied.     batch_evaluate_objective: Python `bool`. If True, the objective function       will be evaluated on all the vertices of the simplex packed into a       single tensor. If False, the objective will be mapped across each       vertex separately.    Returns:     prepared_args: A tuple containing the following elements:       dimension: Scalar `Tensor` of `int32` dtype. The dimension of the problem         as inferred from the supplied arguments.       num_vertices: Scalar `Tensor` of `int32` dtype. The number of vertices         in the simplex.       simplex: A `Tensor` of same dtype as `initial_simplex`         (or `initial_vertex`). The first component of the shape of the         `Tensor` is `num_vertices` and each element represents a vertex of         the simplex.       objective_at_simplex: A `Tensor` of same dtype as the dtype of the         return value of objective_function. The shape is a vector of size         `num_vertices`. The objective function evaluated at the simplex.       num_evaluations: An `int32` scalar `Tensor`. The number of points on         which the objective function was evaluated.    Raises:     ValueError: If any of the following conditions hold       1. If none or more than one of `initial_simplex` and `initial_vertex` are         supplied.       2. If `initial_simplex` and `step_sizes` are both specified.
Evaluates the objective function at the specified initial simplex.
Constructs a standard axes aligned simplex.
Evaluates the objective function on a batch of points.    If `batch_evaluate_objective` is True, returns   `objective function(arg_batch)` else it maps the `objective_function`   across the `arg_batch`.    Args:     objective_function: A Python callable that accepts a single `Tensor` of       rank 'R > 1' and any shape 's' and returns a scalar `Tensor` of real dtype       containing the value of the function at that point. If       `batch a `Tensor` of shape `[batch_size] + s ` where `batch_size` is the       size of the batch of args. In this case, the expected return value is a       `Tensor` of shape `[batch_size]`.     arg_batch: A `Tensor` of real dtype. The batch of arguments at which to       evaluate the `objective_function`. If `batch_evaluate_objective` is False,       `arg_batch` will be unpacked along the zeroth axis and the       `objective_function` will be applied to each element.     batch_evaluate_objective: `bool`. Whether the `objective_function` can       evaluate a batch of arguments at once.    Returns:     A tuple containing:       objective_values: A `Tensor` of real dtype and shape `[batch_size]`.         The value of the objective function evaluated at the supplied         `arg_batch`.       num_evaluations: An `int32` scalar `Tensor`containing the number of         points on which the objective function was evaluated (i.e `batch_size`).
Save a PNG plot with histograms of weight means and stddevs.    Args:     names: A Python `iterable` of `str` variable names.     qm_vals: A Python `iterable`, the same length as `names`,       whose elements are Numpy `array`s, of any shape, containing       posterior means of weight varibles.     qs_vals: A Python `iterable`, the same length as `names`,       whose elements are Numpy `array`s, of any shape, containing       posterior standard deviations of weight varibles.     fname: Python `str` filename to save the plot to.
Save a PNG plot visualizing posterior uncertainty on heldout data.    Args:     input_vals: A `float`-like Numpy `array` of shape       `[num_heldout] + IMAGE_SHAPE`, containing heldout input images.     probs: A `float`-like Numpy array of shape `[num_monte_carlo,       num_heldout, num_classes]` containing Monte Carlo samples of       class probabilities for each heldout sample.     fname: Python `str` filename to save the plot to.     n: Python `int` number of datapoints to vizualize.     title: Python `str` title for the plot.
Build fake MNIST-style data for unit testing.
Returns initializer configuration as a JSON-serializable dict.
Instantiates an initializer from a configuration dictionary.
Numpy matmul wrapper.
Helper to compute stddev, covariance and variance.
Compute the log of the exponentially weighted moving mean of the exp.    If `log_value` is a draw from a stationary random variable, this function   approximates `log(E[exp(log_value)])`, i.e., a weighted log-sum-exp. More   precisely, a `tf.Variable`, `log_mean_exp_var`, is updated by `log_value`   using the following identity:    ```none   log_mean_exp_var =   = log(decay exp(log_mean_exp_var) + (1 - decay) exp(log_value))   = log(exp(log_mean_exp_var + log(decay)) + exp(log_value + log1p(-decay)))   = log_mean_exp_var     + log(  exp(log_mean_exp_var   - log_mean_exp_var + log(decay))           + exp(log_value - log_mean_exp_var + log1p(-decay)))   = log_mean_exp_var     + log_sum_exp([log(decay), log_value - log_mean_exp_var + log1p(-decay)]).   ```    In addition to numerical stability, this formulation is advantageous because   `log_mean_exp_var` can be updated in a lock-free manner, i.e., using   `assign_add`. (Note: the updates are not thread-safe; it's just that the   update to the tf.Variable is presumed efficient due to being lock-free.)    Args:     log_mean_exp_var: `float`-like `Variable` representing the log of the       exponentially weighted moving mean of the exp. Same shape as `log_value`.     log_value: `float`-like `Tensor` representing a new (streaming) observation.       Same shape as `log_mean_exp_var`.     decay: A `float`-like `Tensor`. The moving mean decay. Typically close to       `1.`, e.g., `0.999`.     name: Optional name of the returned operation.    Returns:     log_mean_exp_var: A reference to the input 'Variable' tensor with the       `log_value`-updated log of the exponentially weighted moving mean of exp.    Raises:     TypeError: if `log_mean_exp_var` does not have float type `dtype`.     TypeError: if `log_mean_exp_var`, `log_value`, `decay` have different       `base_dtype`.
Ensures non-scalar input has at least one column.      Example:       If `x = [1, 2, 3]` then the output is `[[1], [2], [3]]`.        If `x = [[1, 2, 3], [4, 5, 6]]` then the output is unchanged.        If `x = 1` then the output is unchanged.      Args:       x: `Tensor`.      Returns:       columnar_x: `Tensor` with at least two dimensions.
Generates `Tensor` consisting of `-1` or `+1`, chosen uniformly at random.    For more details, see [Rademacher distribution](   https://en.wikipedia.org/wiki/Rademacher_distribution).    Args:     shape: Vector-shaped, `int` `Tensor` representing shape of output.     dtype: (Optional) TF `dtype` representing `dtype` of output.     seed: (Optional) Python integer to seed the random number generator.     name: Python `str` name prefixed to Ops created by this function.       Default value: `None` (i.e., 'random_rademacher').    Returns:     rademacher: `Tensor` with specified `shape` and `dtype` consisting of `-1`       or `+1` chosen uniformly-at-random.
Generates `Tensor` of positive reals drawn from a Rayleigh distributions.    The probability density function of a Rayleigh distribution with `scale`   parameter is given by:    ```none   f(x) = x scale**-2 exp(-x**2 0.5 scale**-2)   ```    For more details, see [Rayleigh distribution](   https://en.wikipedia.org/wiki/Rayleigh_distribution)    Args:     shape: Vector-shaped, `int` `Tensor` representing shape of output.     scale: (Optional) Positive `float` `Tensor` representing `Rayleigh` scale.       Default value: `None` (i.e., `scale = 1.`).     dtype: (Optional) TF `dtype` representing `dtype` of output.       Default value: `tf.float32`.     seed: (Optional) Python integer to seed the random number generator.       Default value: `None` (i.e., no seed).     name: Python `str` name prefixed to Ops created by this function.       Default value: `None` (i.e., 'random_rayleigh').    Returns:     rayleigh: `Tensor` with specified `shape` and `dtype` consisting of positive       real values drawn from a Rayleigh distribution with specified `scale`.
Convenience function which chooses the condition based on the predicate.
Finish computation of log_prob on one element of the inverse image.
Finish computation of prob on one element of the inverse image.
Helper which rolls left event_dims left or right event_dims right.
r"""Inverse of tf.nn.batch_normalization.    Args:     x: Input `Tensor` of arbitrary dimensionality.     mean: A mean `Tensor`.     variance: A variance `Tensor`.     offset: An offset `Tensor`, often denoted `beta` in equations, or       None. If present, will be added to the normalized tensor.     scale: A scale `Tensor`, often denoted `gamma` in equations, or       `None`. If present, the scale is applied to the normalized tensor.     variance_epsilon: A small `float` added to the minibatch `variance` to       prevent dividing by zero.     name: A name for this operation (optional).    Returns:     batch_unnormalized: The de-normalized, de-scaled, de-offset `Tensor`.
Check for valid BatchNormalization layer.      Args:       layer: Instance of `tf.layers.BatchNormalization`.     Raises:       ValueError: If batchnorm_layer argument is not an instance of       `tf.layers.BatchNormalization`, or if `batchnorm_layer.renorm=True` or       if `batchnorm_layer.virtual_batch_size` is specified.
Slices a single parameter of a distribution.    Args:     param: A `Tensor`, the original parameter to slice.     param_event_ndims: `int` event parameterization rank for this parameter.     slices: A `tuple` of normalized slices.     dist_batch_shape: The distribution's batch shape `Tensor`.    Returns:     new_param: A `Tensor`, batch-sliced according to slices.
Computes the override dictionary of sliced parameters.    Args:     dist: The tfd.Distribution being batch-sliced.     params_event_ndims: Per-event parameter ranks, a `str->int` `dict`.     slices: Slices as received by __getitem__.    Returns:     overrides: `str->Tensor` `dict` of batch-sliced parameter overrides.
Applies a single slicing step to `dist`, returning a new instance.
Applies a sequence of slice or copy-with-overrides operations to `dist`.
Slices `dist` along its batch dimensions. Helper for tfd.Distribution.    Args:     dist: A `tfd.Distribution` instance.     params_event_ndims: A `dict` of `str->int` indicating the number of       dimensions of a given parameter required to parameterize a single event.     params_overrides: A `dict` of parameter overrides. (e.g. from       `Distribution.copy`).     slices: A `slice` or `int` or `int` `Tensor` or `tf.newaxis` or `tuple`       thereof. (e.g. the argument of a `__getitem__` method).    Returns:     new_dist: A batch-sliced `tfd.Distribution`.
Runs multiple Fisher scoring steps.    Args:     model_matrix: (Batch of) `float`-like, matrix-shaped `Tensor` where each row       represents a sample's features.     response: (Batch of) vector-shaped `Tensor` where each element represents a       sample's observed response (to the corresponding row of features). Must       have same `dtype` as `model_matrix`.     model: `tfp.glm.ExponentialFamily`-like instance which implicitly       characterizes a negative log-likelihood loss by specifying the       distribuion's `mean`, `gradient_mean`, and `variance`.     model_coefficients_start: Optional (batch of) vector-shaped `Tensor`       representing the initial model coefficients, one for each column in       `model_matrix`. Must have same `dtype` as `model_matrix`.       Default value: Zeros.     predicted_linear_response_start: Optional `Tensor` with `shape`, `dtype`       matching `response`; represents `offset` shifted initial linear       predictions based on `model_coefficients_start`.       Default value: `offset` if `model_coefficients is None`, and       `tf.linalg.matvec(model_matrix, model_coefficients_start) + offset`       otherwise.     l2_regularizer: Optional scalar `Tensor` representing L2 regularization       penalty, i.e.,       `loss(w) = sum{-log p(y[i]|x[i],w) : i=1..n} + l2_regularizer ||w||_2^2`.       Default value: `None` (i.e., no L2 regularization).     dispersion: Optional (batch of) `Tensor` representing `response` dispersion,       i.e., as in, `p(y|theta) := exp((y theta - A(theta)) / dispersion)`.       Must broadcast with rows of `model_matrix`.       Default value: `None` (i.e., "no dispersion").     offset: Optional `Tensor` representing constant shift applied to       `predicted_linear_response`.  Must broadcast to `response`.       Default value: `None` (i.e., `tf.zeros_like(response)`).     convergence_criteria_fn: Python `callable` taking:       `is_converged_previous`, `iter_`, `model_coefficients_previous`,       `predicted_linear_response_previous`, `model_coefficients_next`,       `predicted_linear_response_next`, `response`, `model`, `dispersion` and       returning a `bool` `Tensor` indicating that Fisher scoring has converged.       See `convergence_criteria_small_relative_norm_weights_change` as an       example function.       Default value: `None` (i.e.,       `convergence_criteria_small_relative_norm_weights_change`).     learning_rate: Optional (batch of) scalar `Tensor` used to dampen iterative       progress. Typically only needed if optimization diverges, should be no       larger than `1` and typically very close to `1`.       Default value: `None` (i.e., `1`).     fast_unsafe_numerics: Optional Python `bool` indicating if faster, less       numerically accurate methods can be employed for computing the weighted       least-squares solution.       Default value: `True` (i.e., "fast but possibly diminished accuracy").     maximum_iterations: Optional maximum number of iterations of Fisher scoring       to run; "and-ed" with result of `convergence_criteria_fn`.       Default value: `None` (i.e., `infinity`).     name: Python `str` used as name prefix to ops created by this function.       Default value: `"fit"`.    Returns:     model_coefficients: (Batch of) vector-shaped `Tensor`; represents the       fitted model coefficients, one for each column in `model_matrix`.     predicted_linear_response: `response`-shaped `Tensor` representing linear       predictions based on new `model_coefficients`, i.e.,       `tf.linalg.matvec(model_matrix, model_coefficients) + offset`.     is_converged: `bool` `Tensor` indicating that the returned       `model_coefficients` met the `convergence_criteria_fn` criteria within the       `maximum_iterations` limit.     iter_: `int32` `Tensor` indicating the number of iterations taken.    #### Example    ```python   from __future__ import print_function   import numpy as np   import tensorflow as tf   import tensorflow_probability as tfp   tfd = tfp.distributions    def make_dataset(n, d, link, scale=1., dtype=np.float32):     model_coefficients = tfd.Uniform(         low=np.array(-1, dtype),         high=np.array(1, dtype)).sample(d, seed=42)     radius = np.sqrt(2.)     model_coefficients *= radius / tf.linalg.norm(model_coefficients)     model_matrix = tfd.Normal(         loc=np.array(0, dtype),         scale=np.array(1, dtype)).sample([n, d], seed=43)     scale = tf.convert_to_tensor(scale, dtype)     linear_response = tf.tensordot(         model_matrix, model_coefficients, axes=[[1], [0]])     if link == 'linear':       response = tfd.Normal(loc=linear_response, scale=scale).sample(seed=44)     elif link == 'probit':       response = tf.cast(           tfd.Normal(loc=linear_response, scale=scale).sample(seed=44) > 0,           dtype)     elif link == 'logit':       response = tfd.Bernoulli(logits=linear_response).sample(seed=44)     else:       raise ValueError('unrecognized true link: {}'.format(link))     return model_matrix, response, model_coefficients    X, Y, w_true = make_dataset(n=int(1e6), d=100, link='probit')    w, linear_response, is_converged, num_iter = tfp.glm.fit(       model_matrix=X,       response=Y,       model=tfp.glm.BernoulliNormalCDF())   log_likelihood = tfp.glm.BernoulliNormalCDF().log_prob(Y, linear_response)    with tf.Session() as sess:     [w_, linear_response_, is_converged_, num_iter_, Y_, w_true_,      log_likelihood_] = sess.run([         w, linear_response, is_converged, num_iter, Y, w_true,         log_likelihood])    print('is_converged: ', is_converged_)   print('    num_iter: ', num_iter_)   print('    accuracy: ', np.mean((linear_response_ > 0.) == Y_))   print('    deviance: ', 2. * np.mean(log_likelihood_))   print('||w0-w1||_2 / (1+||w0||_2): ', (np.linalg.norm(w_true_ - w_, ord=2) /                                          (1. + np.linalg.norm(w_true_, ord=2))))    # ==>   # is_converged:  True   #     num_iter:  6   #     accuracy:  0.804382   #     deviance:  -0.820746600628   # ||w0-w1||_2 / (1+||w0||_2):  0.00619245105309   ```
Returns Python `callable` which indicates fitting procedure has converged.    Writing old, new `model_coefficients` as `w0`, `w1`, this function   defines convergence as,    ```python   relative_euclidean_norm = (tf.norm(w0 - w1, ord=2, axis=-1) /                              (1. + tf.norm(w0, ord=2, axis=-1)))   reduce_all(relative_euclidean_norm < tolerance)   ```    where `tf.norm(x, ord=2)` denotes the [Euclidean norm](   https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm) of `x`.    Args:     tolerance: `float`-like `Tensor` indicating convergence, i.e., when       max relative Euclidean norm weights difference < tolerance`.       Default value: `1e-5`.     norm_order: Order of the norm. Default value: `2` (i.e., "Euclidean norm".)    Returns:     convergence_criteria_fn: Python `callable` which returns `bool` `Tensor`       indicated fitting procedure has converged. (See inner function       specification for argument signature.)       Default value: `1e-5`.
Helper to `fit` which sanitizes input args.    Args:     model_matrix: (Batch of) `float`-like, matrix-shaped `Tensor` where each row       represents a sample's features.     response: (Batch of) vector-shaped `Tensor` where each element represents a       sample's observed response (to the corresponding row of features). Must       have same `dtype` as `model_matrix`.     model_coefficients: Optional (batch of) vector-shaped `Tensor` representing       the model coefficients, one for each column in `model_matrix`. Must have       same `dtype` as `model_matrix`.       Default value: `tf.zeros(tf.shape(model_matrix)[-1], model_matrix.dtype)`.     predicted_linear_response: Optional `Tensor` with `shape`, `dtype` matching       `response`; represents `offset` shifted initial linear predictions based       on current `model_coefficients`.       Default value: `offset` if `model_coefficients is None`, and       `tf.linalg.matvec(model_matrix, model_coefficients_start) + offset`       otherwise.     offset: Optional `Tensor` with `shape`, `dtype` matching `response`;       represents constant shift applied to `predicted_linear_response`.       Default value: `None` (i.e., `tf.zeros_like(response)`).     name: Python `str` used as name prefix to ops created by this function.       Default value: `"prepare_args"`.    Returns:     model_matrix: A `Tensor` with `shape`, `dtype` and values of the       `model_matrix` argument.     response: A `Tensor` with `shape`, `dtype` and values of the       `response` argument.     model_coefficients_start: A `Tensor` with `shape`, `dtype` and       values of the `model_coefficients_start` argument if specified.       A (batch of) vector-shaped `Tensors` with `dtype` matching `model_matrix`       containing the default starting point otherwise.     predicted_linear_response:  A `Tensor` with `shape`, `dtype` and       values of the `predicted_linear_response` argument if specified.       A `Tensor` with `shape`, `dtype` matching `response` containing the       default value otherwise.     offset: A `Tensor` with `shape`, `dtype` and values of the `offset` argument       if specified or `None` otherwise.
Returns number of cols in a given `Tensor`.
Wraps original_fn, preferring to call static_fn when inputs are static.
Wraps new_fn with the doc of original_fn.
Helper function for statically evaluating predicates in `cond`.
Computes `rank` given a `Tensor`'s `shape`.
Like tf.case, except attempts to statically evaluate predicates.    If any predicate in `pred_fn_pairs` is a bool or has a constant value, the   associated callable will be called or omitted depending on its value.   Otherwise this functions like tf.case.    Args:     pred_fn_pairs: Dict or list of pairs of a boolean scalar tensor and a                    callable which returns a list of tensors.     default: Optional callable that returns a list of tensors.     exclusive: True iff at most one predicate is allowed to evaluate to `True`.     name: A name for this operation (optional).    Returns:     The tensors returned by the first pair whose predicate evaluated to True, or     those returned by `default` if none does.    Raises:     TypeError: If `pred_fn_pairs` is not a list/dictionary.     TypeError: If `pred_fn_pairs` is a list but does not contain 2-tuples.     TypeError: If `fns[i]` is not callable for any i, or `default` is not                callable.
Helper function to standardize op scope.
Computes the standard deviation of a mixture distribution.    This function works regardless of the component distribution, so long as   each component's mean and standard deviation can be provided.    Args:     mixture_weight_vector: A 2D tensor with shape [batch_size, num_components]     mean_vector: A 2D tensor of mixture component means. Has shape `[batch_size,       num_components]`.     stddev_vector: A 2D tensor of mixture component standard deviations. Has       shape `[batch_size, num_components]`.    Returns:     A 1D tensor of shape `[batch_size]` representing the standard deviation of     the mixture distribution with given weights and component means and standard     deviations.   Raises:     ValueError: If the shapes of the input tensors are not as expected.
Creates a LinearOperator representing a lower triangular matrix.    Args:     loc: Floating-point `Tensor`. This is used for inferring shape in the case       where only `scale_identity_multiplier` is set.     scale_tril: Floating-point `Tensor` representing the diagonal matrix.       `scale_diag` has shape [N1, N2, ...  k, k], which represents a k x k lower       triangular matrix. When `None` no `scale_tril` term is added to the       LinearOperator. The upper triangular elements above the diagonal are       ignored.     scale_diag: Floating-point `Tensor` representing the diagonal matrix.       `scale_diag` has shape [N1, N2, ...  k], which represents a k x k diagonal       matrix. When `None` no diagonal term is added to the LinearOperator.     scale_identity_multiplier: floating point rank 0 `Tensor` representing a       scaling done to the identity matrix. When `scale_identity_multiplier =       scale_diag = scale_tril = None` then `scale += IdentityMatrix`. Otherwise       no scaled-identity-matrix is added to `scale`.     shape_hint: scalar integer `Tensor` representing a hint at the dimension of       the identity matrix when only `scale_identity_multiplier` is set.     validate_args: Python `bool` indicating whether arguments should be checked       for correctness.     assert_positive: Python `bool` indicating whether LinearOperator should be       checked for being positive definite.     name: Python `str` name given to ops managed by this object.    Returns:     `LinearOperator` representing a lower triangular matrix.    Raises:     ValueError:  If only `scale_identity_multiplier` is set and `loc` and       `shape_hint` are both None.
Creates a LinearOperator representing a diagonal matrix.    Args:     loc: Floating-point `Tensor`. This is used for inferring shape in the case       where only `scale_identity_multiplier` is set.     scale_diag: Floating-point `Tensor` representing the diagonal matrix.       `scale_diag` has shape [N1, N2, ...  k], which represents a k x k diagonal       matrix. When `None` no diagonal term is added to the LinearOperator.     scale_identity_multiplier: floating point rank 0 `Tensor` representing a       scaling done to the identity matrix. When `scale_identity_multiplier =       scale_diag = scale_tril = None` then `scale += IdentityMatrix`. Otherwise       no scaled-identity-matrix is added to `scale`.     shape_hint: scalar integer `Tensor` representing a hint at the dimension of       the identity matrix when only `scale_identity_multiplier` is set.     validate_args: Python `bool` indicating whether arguments should be checked       for correctness.     assert_positive: Python `bool` indicating whether LinearOperator should be       checked for being positive definite.     name: Python `str` name given to ops managed by this object.     dtype: TF `DType` to prefer when converting args to `Tensor`s. Else, we fall       back to a compatible dtype across all of `loc`, `scale_diag`, and       `scale_identity_multiplier`.    Returns:     `LinearOperator` representing a lower triangular matrix.    Raises:     ValueError:  If only `scale_identity_multiplier` is set and `loc` and       `shape_hint` are both None.
Infer distribution batch and event shapes from a location and scale.    Location and scale family distributions determine their batch/event shape by   broadcasting the `loc` and `scale` args.  This helper does that broadcast,   statically if possible.    Batch shape broadcasts as per the normal rules.   We allow the `loc` event shape to broadcast up to that of `scale`.  We do not   allow `scale`'s event shape to change.  Therefore, the last dimension of `loc`   must either be size `1`, or the same as `scale.range_dimension`.    See `MultivariateNormalLinearOperator` for a usage example.    Args:     loc: `Tensor` (already converted to tensor) or `None`. If `None`, or       `rank(loc)==0`, both batch and event shape are determined by `scale`.     scale:  A `LinearOperator` instance.     name:  A string name to prepend to created ops.    Returns:     batch_shape:  `TensorShape` (if broadcast is done statically), or `Tensor`.     event_shape:  `TensorShape` (if broadcast is done statically), or `Tensor`.    Raises:     ValueError:  If the last dimension of `loc` is determined statically to be       different than the range of `scale`.
Returns `True` if `scale` is a `LinearOperator` that is known to be diag.    Args:     scale:  `LinearOperator` instance.    Returns:     Python `bool`.    Raises:     TypeError:  If `scale` is not a `LinearOperator`.
Helper which checks validity of a scalar `distribution` init arg.    Valid here means:    * `distribution` has scalar batch and event shapes.   * `distribution` is `FULLY_REPARAMETERIZED`   * `distribution` has expected dtype.    Args:     distribution:  `Distribution`-like object.     expected_base_dtype:  `TensorFlow` `dtype`.     validate_args:  Python `bool`.  Whether to do additional checks: (i)  check       that reparameterization_type is `FULLY_REPARAMETERIZED`. (ii) add       `tf.Assert` ops to the graph to enforce that distribution is scalar in the       event that this cannot be determined statically.    Returns:     List of `tf.Assert` ops to run to enforce validity checks that could not       be statically determined.  Empty if `not validate_args`.    Raises:     ValueError:  If validate_args and distribution is not FULLY_REPARAMETERIZED     ValueError:  If distribution is statically determined to not have both       scalar batch and scalar event shapes.
Pad dimensions of event tensors for mixture distributions.    See `Mixture._sample_n` and `MixtureSameFamily._sample_n` for usage examples.    Args:     x: event tensor to pad.     mixture_distribution: Base distribution of the mixture.     categorical_distribution: `Categorical` distribution that mixes the base       distribution.     event_ndims: Integer specifying the number of event dimensions in the event       tensor.    Returns:     A padded version of `x` that can broadcast with `categorical_distribution`.
Convenience function that chooses one of two values based on the predicate.    This utility is equivalent to a version of `tf.where` that accepts only a   scalar predicate and computes its result statically when possible. It may also   be used in place of `tf.cond` when both branches yield a `Tensor` of the same   shape; the operational difference is that `tf.cond` uses control flow to   evaluate only the branch that's needed, while `tf.where` (and thus   this method) may evaluate both branches before the predicate's truth is known.   This means that `tf.cond` is preferred when one of the branches is expensive   to evaluate (like performing a large matmul), while this method is preferred   when both branches are cheap, e.g., constants. In the latter case, we expect   this method to be substantially faster than `tf.cond` on GPU and to give   similar performance on CPU.    Args:     pred: Scalar `bool` `Tensor` predicate.     true_value: `Tensor` to return if `pred` is `True`.     false_value: `Tensor` to return if `pred` is `False`. Must have the same       shape as `true_value`.     name: Python `str` name given to ops managed by this object.    Returns:     result: a `Tensor` (or `Tensor`-convertible Python value) equal to       `true_value` if `pred` evaluates to `True` and `false_value` otherwise.       If the condition can be evaluated statically, the result returned is one       of the input Python values, with no graph side effects.
Move a single tensor dimension within its shape.    This is a special case of `tf.transpose()`, which applies   arbitrary permutations to tensor dimensions.    Args:     x: Tensor of rank `ndims`.     source_idx: Integer index into `x.shape` (negative indexing is supported).     dest_idx: Integer index into `x.shape` (negative indexing is supported).    Returns:     x_perm: Tensor of rank `ndims`, in which the dimension at original      index `source_idx` has been moved to new index `dest_idx`, with      all other dimensions retained in their original order.    Example:    ```python   x = tf.placeholder(shape=[200, 30, 4, 1, 6])   x_perm = _move_dimension(x, 1, 1) # no-op   x_perm = _move_dimension(x, 0, 3) # result shape [30, 4, 1, 200, 6]   x_perm = _move_dimension(x, 0, -2) # equivalent to previous   x_perm = _move_dimension(x, 4, 2) # result shape [200, 30, 6, 4, 1]   ```
Assert x is a non-negative tensor, and optionally of integers.
Returns whether a and b have the same dynamic shape.    Args:     a: `Tensor`     b: `Tensor`    Returns:     `bool` `Tensor` representing if both tensors have the same shape.
Helper which tries to return a static value.    Given `x`, extract it's value statically, optionally casting to a specific   dtype. If this is not possible, None is returned.    Args:     x: `Tensor` for which to extract a value statically.     dtype: Optional dtype to cast to.    Returns:     Statically inferred value if possible, otherwise None.
Helper returning True if dtype is known to be unsigned.
Helper returning True if dtype is known to be signed.
Helper returning the largest integer exactly representable by dtype.
Helper returning the smallest integer exactly representable by dtype.
Helper returning True if dtype.is_integer or is `bool`.
Embeds checks that categorical distributions don't have too many classes.    A categorical-type distribution is one which, e.g., returns the class label   rather than a one-hot encoding.  E.g., `Categorical(probs)`.    Since distributions output samples in the same dtype as the parameters, we   must ensure that casting doesn't lose precision. That is, the   `parameter.dtype` implies a maximum number of classes. However, since shape is   `int32` and categorical variables are presumed to be indexes into a `Tensor`,   we must also ensure that the number of classes is no larger than the largest   possible `int32` index, i.e., `2**31-1`.    In other words the number of classes, `K`, must satisfy the following   condition:    ```python   K <= min(       int(2**31 - 1),  # Largest float as an index.       {           tf.float16: int(2**11),   # Largest int as a float16.           tf.float32: int(2**24),           tf.float64: int(2**53),       }.get(dtype_util.base_dtype(categorical_param.dtype), 0))   ```    Args:     categorical_param: Floating-point `Tensor` representing parameters of       distribution over categories. The rightmost shape is presumed to be the       number of categories.     name: A name for this operation (optional).    Returns:     categorical_param: Input `Tensor` with appropriate assertions embedded.    Raises:     TypeError: if `categorical_param` has an unknown `dtype`.     ValueError: if we can statically identify `categorical_param` as being too       large (for being closed under int32/float casting).
Multinomial coefficient.    Given `n` and `counts`, where `counts` has last dimension `k`, we compute   the multinomial coefficient as:    ```n! / sum_i n_i!```    where `i` runs over all `k` classes.    Args:     n: Floating-point `Tensor` broadcastable with `counts`. This represents `n`       outcomes.     counts: Floating-point `Tensor` broadcastable with `n`. This represents       counts in `k` classes, where `k` is the last dimension of the tensor.     name: A name for this operation (optional).    Returns:     `Tensor` representing the multinomial coefficient between `n` and `counts`.
Circularly moves dims left or right.    Effectively identical to:    ```python   numpy.transpose(x, numpy.roll(numpy.arange(len(x.shape)), shift))   ```    When `validate_args=False` additional graph-runtime checks are   performed. These checks entail moving data from to GPU to CPU.    Example:    ```python   x = tf.random_normal([1, 2, 3, 4])  # Tensor of shape [1, 2, 3, 4].   rotate_transpose(x, -1).shape == [2, 3, 4, 1]   rotate_transpose(x, -2).shape == [3, 4, 1, 2]   rotate_transpose(x,  1).shape == [4, 1, 2, 3]   rotate_transpose(x,  2).shape == [3, 4, 1, 2]   rotate_transpose(x,  7).shape == rotate_transpose(x, 3).shape  # [2, 3, 4, 1]   rotate_transpose(x, -7).shape == rotate_transpose(x, -3).shape  # [4, 1, 2, 3]   ```    Args:     x: `Tensor`.     shift: `Tensor`. Number of dimensions to transpose left (shift<0) or       transpose right (shift>0).     name: Python `str`. The name to give this op.    Returns:     rotated_x: Input `Tensor` with dimensions circularly rotated by shift.    Raises:     TypeError: if shift is not integer type.
Picks possibly different length row `Tensor`s based on condition.    Value `Tensor`s should have exactly one dimension.    If `cond` is a python Boolean or `tf.constant` then either `true_vector` or   `false_vector` is immediately returned. I.e., no graph nodes are created and   no validation happens.    Args:     cond: `Tensor`. Must have `dtype=tf.bool` and be scalar.     true_vector: `Tensor` of one dimension. Returned when cond is `True`.     false_vector: `Tensor` of one dimension. Returned when cond is `False`.     name: Python `str`. The name to give this op.   Example:  ```python pick_vector(tf.less(0, 5), tf.range(10, 12), tf.range(15,     18))  # [10, 11] pick_vector(tf.less(5, 0), tf.range(10, 12), tf.range(15,     18))  # [15, 16, 17] ```    Returns:     true_or_false_vector: `Tensor`.    Raises:     TypeError: if `cond.dtype != tf.bool`     TypeError: if `cond` is not a constant and       `true_vector.dtype != false_vector.dtype`
Convenience function which statically broadcasts shape when possible.    Args:     shape1:  `1-D` integer `Tensor`.  Already converted to tensor!     shape2:  `1-D` integer `Tensor`.  Already converted to tensor!     name:  A string name to prepend to created ops.    Returns:     The broadcast shape, either as `TensorShape` (if broadcast can be done       statically), or as a `Tensor`.
Generate a new seed, from the given seed and salt.
Creates a matrix with values set above, below, and on the diagonal.    Example:    ```python   tridiag(below=[1., 2., 3.],           diag=[4., 5., 6., 7.],           above=[8., 9., 10.])   # ==> array([[  4.,   8.,   0.,   0.],   #            [  1.,   5.,   9.,   0.],   #            [  0.,   2.,   6.,  10.],   #            [  0.,   0.,   3.,   7.]], dtype=float32)   ```    Warning: This Op is intended for convenience, not efficiency.    Args:     below: `Tensor` of shape `[B1, ..., Bb, d-1]` corresponding to the below       diagonal part. `None` is logically equivalent to `below = 0`.     diag: `Tensor` of shape `[B1, ..., Bb, d]` corresponding to the diagonal       part.  `None` is logically equivalent to `diag = 0`.     above: `Tensor` of shape `[B1, ..., Bb, d-1]` corresponding to the above       diagonal part.  `None` is logically equivalent to `above = 0`.     name: Python `str`. The name to give this op.    Returns:     tridiag: `Tensor` with values set above, below and on the diagonal.    Raises:     ValueError: if all inputs are `None`.
Returns the size of a specific dimension.
Validates quadrature grid, probs or computes them as necessary.    Args:     quadrature_grid_and_probs: Python pair of `float`-like `Tensor`s       representing the sample points and the corresponding (possibly       normalized) weight.  When `None`, defaults to:         `np.polynomial.hermite.hermgauss(deg=8)`.     dtype: The expected `dtype` of `grid` and `probs`.     validate_args: Python `bool`, default `False`. When `True` distribution       parameters are checked for validity despite possibly degrading runtime       performance. When `False` invalid inputs may silently render incorrect       outputs.     name: Python `str` name prefixed to Ops created by this class.    Returns:      quadrature_grid_and_probs: Python pair of `float`-like `Tensor`s       representing the sample points and the corresponding (possibly       normalized) weight.    Raises:     ValueError: if `quadrature_grid_and_probs is not None` and       `len(quadrature_grid_and_probs[0]) != len(quadrature_grid_and_probs[1])`
Returns parent frame arguments.    When called inside a function, returns a dictionary with the caller's function   arguments. These are positional arguments and keyword arguments (**kwargs),   while variable arguments (*varargs) are excluded.    When called at global scope, this will return an empty dictionary, since there   are no arguments.    WARNING: If caller function argument names are overloaded before invoking   this method, then values will reflect the overloaded value. For this reason,   we recommend calling `parent_frame_arguments` at the beginning of the   function.
Transform a 0-D or 1-D `Tensor` to be 1-D.    For user convenience, many parts of the TensorFlow Probability API accept   inputs of rank 0 or 1 -- i.e., allowing an `event_shape` of `[5]` to be passed   to the API as either `5` or `[5]`.  This function can be used to transform   such an argument to always be 1-D.    NOTE: Python or NumPy values will be converted to `Tensor`s with standard type   inference/conversion.  In particular, an empty list or tuple will become an   empty `Tensor` with dtype `float32`.  Callers should convert values to   `Tensor`s before calling this function if different behavior is desired   (e.g. converting empty lists / other values to `Tensor`s with dtype `int32`).    Args:     x: A 0-D or 1-D `Tensor`.     tensor_name: Python `str` name for `Tensor`s created by this function.     op_name: Python `str` name for `Op`s created by this function.     validate_args: Python `bool, default `False`.  When `True`, arguments may be       checked for validity at execution time, possibly degrading runtime       performance.  When `False`, invalid inputs may silently render incorrect         outputs.   Returns:     vector: a 1-D `Tensor`.
Produces the content of `output_tensor` only after `dependencies`.    In some cases, a user may want the output of an operation to be consumed   externally only after some other dependencies have run first. This function   returns `output_tensor`, but only after all operations in `dependencies` have   run. Note that this means that there is no guarantee that `output_tensor` will   be evaluated after any `dependencies` have run.    See also `tf.tuple` and `tf.group`.    Args:     dependencies: Iterable of operations to run before this op finishes.     output_tensor: A `Tensor` or `IndexedSlices` that will be returned.     name: (Optional) A name for this operation.    Returns:     output_with_deps: Same as `output_tensor` but with embedded dependencies.    Raises:     TypeError: if `output_tensor` is not a `Tensor` or `IndexedSlices`.
Checks that `rightmost_transposed_ndims` is valid.
Checks that `perm` is valid.
Helper for _forward and _inverse_event_shape.
Returns the concatenation of the dimension in `x` and `other`.    *Note:* If either `x` or `other` is completely unknown, concatenation will   discard information about the other shape. In future, we might support   concatenation that preserves this information for use with slicing.    For more details, see `help(tf.TensorShape.concatenate)`.    Args:     x: object representing a shape; convertible to `tf.TensorShape`.     other: object representing a shape; convertible to `tf.TensorShape`.    Returns:     new_shape: an object like `x` whose elements are the concatenation of the       dimensions in `x` and `other`.
Returns a list of dimension sizes, or `None` if `rank` is unknown.    For more details, see `help(tf.TensorShape.dims)`.    Args:     x: object representing a shape; convertible to `tf.TensorShape`.    Returns:     shape_as_list: list of sizes or `None` values representing each       dimensions size if known. A size is `tf.Dimension` if input is a       `tf.TensorShape` and an `int` otherwise.
Returns a shape combining the information in `x` and `other`.    The dimensions in `x` and `other` are merged elementwise, according to the   rules defined for `tf.Dimension.merge_with()`.    For more details, see `help(tf.TensorShape.merge_with)`.    Args:     x: object representing a shape; convertible to `tf.TensorShape`.     other: object representing a shape; convertible to `tf.TensorShape`.    Returns:     merged_shape: shape having `type(x)` containing the combined information of       `x` and `other`.    Raises:     ValueError: If `x` and `other` are not compatible.
Returns a shape based on `x` with at least the given `rank`.    For more details, see `help(tf.TensorShape.with_rank_at_least)`.    Args:     x: object representing a shape; convertible to `tf.TensorShape`.     rank: An `int` representing the minimum rank of `x` or else an assertion is       raised.    Returns:     shape: a shape having `type(x)` but guaranteed to have at least the given       rank (or else an assertion was raised).    Raises:     ValueError: If `x` does not represent a shape with at least the given       `rank`.
Check that source and target shape match, statically if possible.
Augment a sample shape to broadcast batch dimensions.    Computes an augmented sample shape, so that any batch dimensions not   part of the distribution `partial_batch_dist` are treated as identical   distributions.    # partial_batch_dist.batch_shape  = [      7]   # full_sample_and_batch_shape     = [3, 4, 7]   # => return an augmented sample shape of [3, 4] so that   #    partial_batch_dist.sample(augmented_sample_shape) has combined   #    sample and batch shape of [3, 4, 7].    Args:     partial_batch_dist: `tfd.Distribution` instance with batch shape a       prefix of `full_sample_and_batch_shape`.     full_sample_and_batch_shape: a Tensor or Tensor-like shape.     validate_args: if True, check for shape errors at runtime.   Returns:     augmented_sample_shape: sample shape such that       `partial_batch_dist.sample(augmented_sample_shape)` has combined       sample and batch shape of `full_sample_and_batch_shape`.    Raises:     ValueError: if `partial_batch_dist.batch_shape` has more dimensions than       `full_sample_and_batch_shape`.     NotImplementedError: if broadcasting would be required to make       `partial_batch_dist.batch_shape` into a prefix of       `full_sample_and_batch_shape` .
Build a callable that perform one step for backward smoothing.    Args:     get_transition_matrix_for_timestep: callable taking a timestep       as an integer `Tensor` argument, and returning a `LinearOperator`       of shape `[latent_size, latent_size]`.    Returns:     backward_pass_step: a callable that updates a BackwardPassState       from timestep `t` to `t-1`.
Backward update for a Kalman smoother.    Give the `filtered_mean` mu(t | t), `filtered_cov` sigma(t | t),   `predicted_mean` mu(t+1 | t) and `predicted_cov` sigma(t+1 | t),   as returns from the `forward_filter` function, as well as   `next_posterior_mean` mu(t+1 | 1:T) and `next_posterior_cov` sigma(t+1 | 1:T),   if the `transition_matrix` of states from time t to time t+1   is given as A(t+1), the 1 step backward smoothed distribution parameter   could be calculated as:   p(z(t) | Obs(1:T)) = N( mu(t | 1:T), sigma(t | 1:T)),   mu(t | 1:T) = mu(t | t) + J(t) * (mu(t+1 | 1:T) - mu(t+1 | t)),   sigma(t | 1:T) = sigma(t | t)                    + J(t) * (sigma(t+1 | 1:T) - sigma(t+1 | t) * J(t)',   J(t) = sigma(t | t) * A(t+1)' / sigma(t+1 | t),   where all the multiplications are matrix multiplication, and `/` is   the matrix inverse. J(t) is the backward Kalman gain matrix.    The algorithm can be intialized from mu(T | 1:T) and sigma(T | 1:T),   which are the last step parameters returned by forward_filter.     Args:     filtered_mean: `Tensor` with event shape `[latent_size, 1]` and       batch shape `B`, containing mu(t | t).     filtered_cov: `Tensor` with event shape `[latent_size, latent_size]` and       batch shape `B`, containing sigma(t | t).     predicted_mean: `Tensor` with event shape `[latent_size, 1]` and       batch shape `B`, containing mu(t+1 | t).     predicted_cov: `Tensor` with event shape `[latent_size, latent_size]` and       batch shape `B`, containing sigma(t+1 | t).     next_posterior_mean: `Tensor` with event shape `[latent_size, 1]` and       batch shape `B`, containing mu(t+1 | 1:T).     next_posterior_cov: `Tensor` with event shape `[latent_size, latent_size]`       and batch shape `B`, containing sigma(t+1 | 1:T).     transition_matrix: `LinearOperator` with shape       `[latent_size, latent_size]` and batch shape broadcastable       to `B`.    Returns:     posterior_mean: `Tensor` with event shape `[latent_size, 1]` and       batch shape `B`, containing mu(t | 1:T).     posterior_cov: `Tensor` with event shape `[latent_size, latent_size]` and       batch shape `B`, containing sigma(t | 1:T).
Build a callable that performs one step of Kalman filtering.    Args:     get_transition_matrix_for_timestep: callable taking a timestep       as an integer `Tensor` argument, and returning a `LinearOperator`       of shape `[latent_size, latent_size]`.     get_transition_noise_for_timestep: callable taking a timestep as       an integer `Tensor` argument, and returning a       `MultivariateNormalLinearOperator` of event shape       `[latent_size]`.     get_observation_matrix_for_timestep: callable taking a timestep       as an integer `Tensor` argument, and returning a `LinearOperator`       of shape `[observation_size, observation_size]`.     get_observation_noise_for_timestep: callable taking a timestep as       an integer `Tensor` argument, and returning a       `MultivariateNormalLinearOperator` of event shape       `[observation_size]`.    Returns:     kalman_filter_step: a callable that updates a KalmanFilterState       from timestep `t-1` to `t`.
Conjugate update for a linear Gaussian model.    Given a normal prior on a latent variable `z`,     `p(z) = N(prior_mean, prior_cov) = N(u, P)`,   for which we observe a linear Gaussian transformation `x`,     `p(x|z) = N(H * z + c, R)`,   the posterior is also normal:     `p(z|x) = N(u*, P*)`.    We can write this update as      x_expected = H * u + c # pushforward prior mean      S = R + H * P * H'  # pushforward prior cov      K = P * H' * S^{-1} # optimal Kalman gain      u* = u + K * (x_observed - x_expected) # posterior mean      P* = (I - K * H) * P (I - K * H)' + K * R * K' # posterior cov   (see, e.g., https://en.wikipedia.org/wiki/Kalman_filter#Update)    Args:     prior_mean: `Tensor` with event shape `[latent_size, 1]` and       potential batch shape `B = [b1, ..., b_n]`.     prior_cov: `Tensor` with event shape `[latent_size, latent_size]`       and batch shape `B` (matching `prior_mean`).     observation_matrix: `LinearOperator` with shape       `[observation_size, latent_size]` and batch shape broadcastable       to `B`.     observation_noise: potentially-batched       `MultivariateNormalLinearOperator` instance with event shape       `[observation_size]` and batch shape broadcastable to `B`.     x_observed: potentially batched `Tensor` with event shape       `[observation_size, 1]` and batch shape `B`.    Returns:     posterior_mean: `Tensor` with event shape `[latent_size, 1]` and       batch shape `B`.     posterior_cov: `Tensor` with event shape `[latent_size,       latent_size]` and batch shape `B`.     predictive_dist: the prior predictive distribution `p(x|z)`,       as a `Distribution` instance with event       shape `[observation_size]` and batch shape `B`. This will       typically be `tfd.MultivariateNormalTriL`, but when       `observation_size=1` we return a `tfd.Independent(tfd.Normal)`       instance as an optimization.
Propagate a filtered distribution through a transition model.
Build a callable that performs one step of Kalman mean recursion.    Args:     get_transition_matrix_for_timestep: callable taking a timestep       as an integer `Tensor` argument, and returning a `LinearOperator`       of shape `[latent_size, latent_size]`.     get_transition_noise_for_timestep: callable taking a timestep as       an integer `Tensor` argument, and returning a       `MultivariateNormalLinearOperator` of event shape       `[latent_size]`.     get_observation_matrix_for_timestep: callable taking a timestep       as an integer `Tensor` argument, and returning a `LinearOperator`       of shape `[observation_size, observation_size]`.     get_observation_noise_for_timestep: callable taking a timestep as       an integer `Tensor` argument, and returning a       `MultivariateNormalLinearOperator` of event shape       `[observation_size]`.    Returns:     kalman_mean_step: a callable that computes latent state and       observation means at time `t`, given latent mean at time `t-1`.
Build a callable for one step of Kalman covariance recursion.    Args:     get_transition_matrix_for_timestep: callable taking a timestep       as an integer `Tensor` argument, and returning a `LinearOperator`       of shape `[latent_size, latent_size]`.     get_transition_noise_for_timestep: callable taking a timestep as       an integer `Tensor` argument, and returning a       `MultivariateNormalLinearOperator` of event shape       `[latent_size]`.     get_observation_matrix_for_timestep: callable taking a timestep       as an integer `Tensor` argument, and returning a `LinearOperator`       of shape `[observation_size, observation_size]`.     get_observation_noise_for_timestep: callable taking a timestep as       an integer `Tensor` argument, and returning a       `MultivariateNormalLinearOperator` of event shape       `[observation_size]`.    Returns:     cov_step: a callable that computes latent state and observation       covariance at time `t`, given latent covariance at time `t-1`.
Build a callable for one step of Kalman sampling recursion.    Args:     get_transition_matrix_for_timestep: callable taking a timestep       as an integer `Tensor` argument, and returning a `LinearOperator`       of shape `[latent_size, latent_size]`.     get_transition_noise_for_timestep: callable taking a timestep as       an integer `Tensor` argument, and returning a       `MultivariateNormalLinearOperator` of event shape       `[latent_size]`.     get_observation_matrix_for_timestep: callable taking a timestep       as an integer `Tensor` argument, and returning a `LinearOperator`       of shape `[observation_size, observation_size]`.     get_observation_noise_for_timestep: callable taking a timestep as       an integer `Tensor` argument, and returning a       `MultivariateNormalLinearOperator` of event shape       `[observation_size]`.     full_sample_and_batch_shape: Desired sample and batch shape of the       returned samples, concatenated in a single `Tensor`.     stream: `tfd.SeedStream` instance used to generate a       sequence of random seeds.     validate_args: if True, perform error checking at runtime.    Returns:     sample_step: a callable that samples the latent state and       observation at time `t`, given latent state at time `t-1`.
Propagate a mean through linear Gaussian transformation.
Propagate covariance through linear Gaussian transformation.
Run the backward pass in Kalman smoother.      The backward smoothing is using Rauch, Tung and Striebel smoother as     as discussed in section 18.3.2 of Kevin P. Murphy, 2012, Machine Learning:     A Probabilistic Perspective, The MIT Press. The inputs are returned by     `forward_filter` function.      Args:       filtered_means: Means of the per-timestep filtered marginal         distributions p(z_t | x_{:t}), as a Tensor of shape         `sample_shape(x) + batch_shape + [num_timesteps, latent_size]`.       filtered_covs: Covariances of the per-timestep filtered marginal         distributions p(z_t | x_{:t}), as a Tensor of shape         `batch_shape + [num_timesteps, latent_size, latent_size]`.       predicted_means: Means of the per-timestep predictive          distributions over latent states, p(z_{t+1} | x_{:t}), as a          Tensor of shape `sample_shape(x) + batch_shape +          [num_timesteps, latent_size]`.       predicted_covs: Covariances of the per-timestep predictive          distributions over latent states, p(z_{t+1} | x_{:t}), as a          Tensor of shape `batch_shape + [num_timesteps, latent_size,          latent_size]`.      Returns:       posterior_means: Means of the smoothed marginal distributions         p(z_t | x_{1:T}), as a Tensor of shape         `sample_shape(x) + batch_shape + [num_timesteps, latent_size]`,         which is of the same shape as filtered_means.       posterior_covs: Covariances of the smoothed marginal distributions         p(z_t | x_{1:T}), as a Tensor of shape         `batch_shape + [num_timesteps, latent_size, latent_size]`.         which is of the same shape as filtered_covs.
Draw a joint sample from the prior over latents and observations.
Run a Kalman smoother to return posterior mean and cov.      Note that the returned values `smoothed_means` depend on the observed     time series `x`, while the `smoothed_covs` are independent     of the observed series; i.e., they depend only on the model itself.     This means that the mean values have shape `concat([sample_shape(x),     batch_shape, [num_timesteps, {latent/observation}_size]])`,     while the covariances have shape `concat[(batch_shape, [num_timesteps,     {latent/observation}_size, {latent/observation}_size]])`, which     does not depend on the sample shape.      This function only performs smoothing. If the user wants the     intermediate values, which are returned by filtering pass `forward_filter`,     one could get it by:     ```     (log_likelihoods,      filtered_means, filtered_covs,      predicted_means, predicted_covs,      observation_means, observation_covs) = model.forward_filter(x)     smoothed_means, smoothed_covs = model.backward_smoothing_pass(x)     ```     where `x` is an observation sequence.      Args:       x: a float-type `Tensor` with rightmost dimensions         `[num_timesteps, observation_size]` matching         `self.event_shape`. Additional dimensions must match or be         broadcastable to `self.batch_shape`; any further dimensions         are interpreted as a sample shape.       mask: optional bool-type `Tensor` with rightmost dimension         `[num_timesteps]`; `True` values specify that the value of `x`         at that timestep is masked, i.e., not conditioned on. Additional         dimensions must match or be broadcastable to `self.batch_shape`; any         further dimensions must match or be broadcastable to the sample         shape of `x`.         Default value: `None`.      Returns:       smoothed_means: Means of the per-timestep smoothed          distributions over latent states, p(x_{t} | x_{:T}), as a          Tensor of shape `sample_shape(x) + batch_shape +          [num_timesteps, observation_size]`.       smoothed_covs: Covariances of the per-timestep smoothed          distributions over latent states, p(x_{t} | x_{:T}), as a          Tensor of shape `sample_shape(mask) + batch_shape + [num_timesteps,          observation_size, observation_size]`. Note that the covariances depend          only on the model and the mask, not on the data, so this may have fewer          dimensions than `filtered_means`.
Compute prior means for all variables via dynamic programming.      Returns:       latent_means: Prior means of latent states `z_t`, as a `Tensor`         of shape `batch_shape + [num_timesteps, latent_size]`       observation_means: Prior covariance matrices of observations         `x_t`, as a `Tensor` of shape `batch_shape + [num_timesteps,         observation_size]`
Compute prior covariances for all variables via dynamic programming.      Returns:       latent_covs: Prior covariance matrices of latent states `z_t`, as         a `Tensor` of shape `batch_shape + [num_timesteps,         latent_size, latent_size]`       observation_covs: Prior covariance matrices of observations         `x_t`, as a `Tensor` of shape `batch_shape + [num_timesteps,         observation_size, observation_size]`
Push latent means and covariances forward through the observation model.      Args:       latent_means: float `Tensor` of shape `[..., num_timesteps, latent_size]`       latent_covs: float `Tensor` of shape         `[..., num_timesteps, latent_size, latent_size]`.      Returns:       observation_means: float `Tensor` of shape         `[..., num_timesteps, observation_size]`       observation_covs: float `Tensor` of shape         `[..., num_timesteps, observation_size, observation_size]`
Computes the log-normalizer of the distribution.
The mode of the von Mises-Fisher distribution is the mean direction.
Applies a Householder rotation to `samples`.
Specialized inversion sampler for 3D.
Create a deep copy of fn.    Args:     fn: a callable    Returns:     A `FunctionType`: a deep copy of fn.    Raises:     TypeError: if `fn` is not a callable.
Removes `dict` keys which have have `self` as value.
Recursively replace `dict`s with `_PrettyDict`.
Check args and return samples.
Helper which returns `True` if input is `collections.namedtuple`-like.
Helper to `choose` which expand_dims `is_accepted` and applies tf.where.
Helper which expand_dims `is_accepted` then applies tf.where.
Elementwise adds list members, replacing non-finite results with alt_value.    Typically the `alt_value` is chosen so the `MetropolisHastings`   `TransitionKernel` always rejects the proposal.    Args:     x: Python `list` of `Tensors` to elementwise add.     alt_value: Python scalar used to replace any elementwise sums which would       otherwise be non-finite.     name: Python `str` name prefixed to Ops created by this function.       Default value: `None` (i.e., "safe_sum").    Returns:     safe_sum: `Tensor` representing the elementwise sum of list of `Tensor`s       `x` or `alt_value` where sums are non-finite.    Raises:     TypeError: if `x` is not list-like.     ValueError: if `x` is empty.
Helper to `maybe_call_fn_and_grads`.
Calls `fn` and computes the gradient of the result wrt `args_list`.
Construct a for loop, preferring a python loop if `n` is staticaly known.    Given `loop_num_iter` and `body_fn`, return an op corresponding to executing   `body_fn` `loop_num_iter` times, feeding previous outputs of `body_fn` into   the next iteration.    If `loop_num_iter` is statically known, the op is constructed via python for   loop, and otherwise a `tf.while_loop` is used.    Args:     loop_num_iter: `Integer` `Tensor` representing the number of loop       iterations.     body_fn: Callable to be executed `loop_num_iter` times.     initial_loop_vars: Listlike object of `Tensors` to be passed in to       `body_fn`'s first execution.     parallel_iterations: The number of iterations allowed to run in parallel.       It must be a positive integer. See `tf.while_loop` for more details.       Default value: `10`.     name: Python `str` name prefixed to Ops created by this function.       Default value: `None` (i.e., "smart_for_loop").   Returns:     result: `Tensor` representing applying `body_fn` iteratively `n` times.
A simplified version of `tf.scan` that has configurable tracing.    This function repeatedly calls `loop_fn(state, elem)`, where `state` is the   `initial_state` during the first iteration, and the return value of `loop_fn`   for every iteration thereafter. `elem` is a slice of `elements` along the   first dimension, accessed in order. Additionally, it calls `trace_fn` on the   return value of `loop_fn`. The `Tensor`s in return values of `trace_fn` are   stacked and returned from this function, such that the first dimension of   those `Tensor`s matches the size of `elems`.    Args:     loop_fn: A callable that takes in a `Tensor` or a nested collection of       `Tensor`s with the same structure as `initial_state`, a slice of `elems`       and returns the same structure as `initial_state`.     initial_state: A `Tensor` or a nested collection of `Tensor`s passed to       `loop_fn` in the first iteration.     elems: A `Tensor` that is split along the first dimension and each element       of which is passed to `loop_fn`.     trace_fn: A callable that takes in the return value of `loop_fn` and returns       a `Tensor` or a nested collection of `Tensor`s.     parallel_iterations: Passed to the internal `tf.while_loop`.     name: Name scope used in this function. Default: 'trace_scan'.    Returns:     final_state: The final return value of `loop_fn`.     trace: The same structure as the return value of `trace_fn`, but with each       `Tensor` being a stack of the corresponding `Tensors` in the return value       of `trace_fn` for each slice of `elems`.
Wraps a setter so it applies to the inner-most results in `kernel_results`.    The wrapped setter unwraps `kernel_results` and applies `setter` to the first   results without an `inner_results` attribute.    Args:     setter: A callable that takes the kernel results as well as some `*args` and       `**kwargs` and returns a modified copy of those kernel results.    Returns:     new_setter: A wrapped `setter`.
Wraps a getter so it applies to the inner-most results in `kernel_results`.    The wrapped getter unwraps `kernel_results` and returns the return value of   `getter` called with the first results without an `inner_results` attribute.    Args:     getter: A callable that takes Kernel results and returns some value.    Returns:     new_getter: A wrapped `getter`.
Enables the `store_parameters_in_results` parameter in a chain of kernels.    This is a temporary utility for use during the transition period of the   parameter storage methods.    Args:     kernel: A TransitionKernel.    Returns:     kernel: The same kernel, but recreated with `store_parameters_in_results`         recursively set to `True` in its parameters and its inner kernels (as         appropriate).
Replaces the rightmost dims in a `Tensor` representing a shape.    Args:     input_shape: a rank-1 `Tensor` of integers     event_shape_in: the event shape expected to be present in rightmost dims       of `shape_in`.     event_shape_out: the event shape with which to replace `event_shape_in` in       the rightmost dims of `input_shape`.     validate_args: Python `bool` indicating whether arguments should       be checked for correctness.    Returns:     output_shape: A rank-1 integer `Tensor` with the same contents as       `input_shape` except for the event dims, which are replaced with       `event_shape_out`.
Replaces the event shape dims of a `TensorShape`.    Args:     input_tensorshape: a `TensorShape` instance in which to attempt replacing       event shape.     event_shape_in: `Tensor` shape representing the event shape expected to       be present in (rightmost dims of) `tensorshape_in`. Must be compatible       with the rightmost dims of `tensorshape_in`.     event_shape_out: `Tensor` shape representing the new event shape, i.e.,       the replacement of `event_shape_in`,    Returns:     output_tensorshape: `TensorShape` with the rightmost `event_shape_in`       replaced by `event_shape_out`. Might be partially defined, i.e.,       `TensorShape(None)`.     is_validated: Python `bool` indicating static validation happened.    Raises:     ValueError: if we can determine the event shape portion of       `tensorshape_in` as well as `event_shape_in` both statically, and they       are not compatible. "Compatible" here means that they are identical on       any dims that are not -1 in `event_shape_in`.
Check that a shape Tensor is int-type and otherwise sane.
Condition to stop when any batch member converges, or all have failed.
Returns a dictionary to populate the initial state of the search procedure.    Performs an initial convergence check and the first evaluation of the   objective function.    Args:     value_and_gradients_function: A Python callable that accepts a tensor and       returns a tuple of two tensors: the objective function value and its       derivative.     initial_position: The starting point of the search procedure.     grad_tolerance: The gradient tolerance for the procedure.     control_inputs: Optional ops used to assert the validity of inputs, these       are added as control dependencies to execute before the objective       function is evaluated for the first time.    Returns:     An dictionary with values for the following keys:       converged: True if the convergence check finds that the initial position         is already an argmin of the objective function.       failed: Initialized to False.       num_objective_evaluations: Initialized to 1.       position: Initialized to the initial position.       objective_value: Initialized to the value of the objective function at         the initial position.       objective_gradient: Initialized to the gradient of the objective         function at the initial position.
Performs the line search step of the BFGS search procedure.    Uses hager_zhang line search procedure to compute a suitable step size   to advance the current `state.position` along the given `search_direction`.   Also, if the line search is successful, updates the `state.position` by   taking the corresponding step.    Args:     state: A namedtuple instance holding values for the current state of the       search procedure. The state must include the fields: `position`,       `objective_value`, `objective_gradient`, `num_iterations`,       `num_objective_evaluations`, `converged` and `failed`.     value_and_gradients_function: A Python callable that accepts a point as a       real `Tensor` of shape `[..., n]` and returns a tuple of two tensors of       the same dtype: the objective function value, a real `Tensor` of shape       `[...]`, and its derivative, another real `Tensor` of shape `[..., n]`.     search_direction: A real `Tensor` of shape `[..., n]`. The direction along       which to perform line search.     grad_tolerance: Scalar `Tensor` of real dtype. Specifies the gradient       tolerance for the procedure.     f_relative_tolerance: Scalar `Tensor` of real dtype. Specifies the       tolerance for the relative change in the objective value.     x_tolerance: Scalar `Tensor` of real dtype. Specifies the tolerance for the       change in the position.     stopping_condition: A Python function that takes as input two Boolean       tensors of shape `[...]`, and returns a Boolean scalar tensor. The input       tensors are `converged` and `failed`, indicating the current status of       each respective batch member; the return value states whether the       algorithm should stop.   Returns:     A copy of the input state with the following fields updated:       converged: a Boolean `Tensor` of shape `[...]` indicating whether the         convergence criteria has been met.       failed: a Boolean `Tensor` of shape `[...]` indicating whether the line         search procedure failed to converge, or if either the updated gradient         or objective function are no longer finite.       num_iterations: Increased by 1.       num_objective_evaluations: Increased by the number of times that the         objective function got evaluated.       position, objective_value, objective_gradient: If line search succeeded,         updated by computing the new position and evaluating the objective         function at that position.
Restricts a function in n-dimensions to a given direction.    Suppose f: R^n -> R. Then given a point x0 and a vector p0 in R^n, the   restriction of the function along that direction is defined by:    ```None   g(t) = f(x0 + t * p0)   ```    This function performs this restriction on the given function. In addition, it   also computes the gradient of the restricted function along the restriction   direction. This is equivalent to computing `dg/dt` in the definition above.    Args:     value_and_gradients_function: Callable accepting a single real `Tensor`       argument of shape `[..., n]` and returning a tuple of a real `Tensor` of       shape `[...]` and a real `Tensor` of shape `[..., n]`. The multivariate       function whose restriction is to be computed. The output values of the       callable are the function value and the gradients at the input argument.     position: `Tensor` of real dtype and shape consumable by       `value_and_gradients_function`. Corresponds to `x0` in the definition       above.     direction: `Tensor` of the same dtype and shape as `position`. The direction       along which to restrict the function. Note that the direction need not       be a unit vector.    Returns:     restricted_value_and_gradients_func: A callable accepting a tensor of shape       broadcastable to `[...]` and same dtype as `position` and returning a       namedtuple of `Tensors`. The input tensor is the parameter along the       direction labelled `t` above. The return value contains fields:         x: A real `Tensor` of shape `[...]`. The input value `t` where the line           function was evaluated, after any necessary broadcasting.         f: A real `Tensor` of shape `[...]` containing the value of the           function at the point `position + t * direction`.         df: A real `Tensor` of shape `[...]` containing the derivative at           `position + t * direction`.         full_gradient: A real `Tensor` of shape `[..., n]`, the full gradient           of the original `value_and_gradients_function`.
Updates the state advancing its position by a given position_delta.
Checks if the algorithm satisfies the convergence criteria.
Broadcast a value to match the batching dimensions of a target.    If necessary the value is converted into a tensor. Both value and target   should be of the same dtype.    Args:     value: A value to broadcast.     target: A `Tensor` of shape [b1, ..., bn, d].    Returns:     A `Tensor` of shape [b1, ..., bn] and same dtype as the target.
Compute the harmonic number from its analytic continuation.    Derivation from [here](   https://en.wikipedia.org/wiki/Digamma_function#Relation_to_harmonic_numbers)   and [Euler's constant](   https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant).    Args:     x: input float.    Returns:     z: The analytic continuation of the harmonic number for the input.
Default exchange proposal function, for replica exchange MC.    With probability `prob_exchange` propose combinations of replica for exchange.   When exchanging, create combinations of adjacent replicas in   [Replica Exchange Monte Carlo](   https://en.wikipedia.org/wiki/Parallel_tempering)    ```   exchange_fn = default_exchange_proposed_fn(prob_exchange=0.5)   exchange_proposed = exchange_fn(num_replica=3)    exchange_proposed.eval()   ==> [[0, 1]]  # 1 exchange, 0 <--> 1    exchange_proposed.eval()   ==> []  # 0 exchanges   ```    Args:     prob_exchange: Scalar `Tensor` giving probability that any exchanges will       be generated.    Returns:     default_exchange_proposed_fn_: Python callable which take a number of       replicas (a Python integer), and return combinations of replicas for       exchange as an [n, 2] integer `Tensor`, `0 <= n <= num_replica // 2`,       with *unique* values in the set `{0, ..., num_replica}`.
field_name from kernel_results or kernel_results.accepted_results.
Get list of TensorArrays holding exchanged states, and zeros.
Helper to `_covariance` and `_variance` which computes a shared scale.
Makes a function which applies a list of Bijectors' `log_det_jacobian`s.
Makes a function which applies a list of Bijectors' `forward`s.
Makes a function which applies a list of Bijectors' `inverse`s.
Runs one iteration of the Transformed Kernel.      Args:       current_state: `Tensor` or Python `list` of `Tensor`s         representing the current state(s) of the Markov chain(s),         _after_ application of `bijector.forward`. The first `r`         dimensions index independent chains,         `r = tf.rank(target_log_prob_fn(*current_state))`. The         `inner_kernel.one_step` does not actually use `current_state`,         rather it takes as input         `previous_kernel_results.transformed_state` (because         `TransformedTransitionKernel` creates a copy of the input         inner_kernel with a modified `target_log_prob_fn` which         internally applies the `bijector.forward`).       previous_kernel_results: `collections.namedtuple` containing `Tensor`s         representing values from previous calls to this function (or from the         `bootstrap_results` function.)      Returns:       next_state: Tensor or Python list of `Tensor`s representing the state(s)         of the Markov chain(s) after taking exactly one step. Has same type and         shape as `current_state`.       kernel_results: `collections.namedtuple` of internal calculations used to         advance the chain.
Like tf.where but works on namedtuples.
Performs the secant square procedure of Hager Zhang.    Given an interval that brackets a root, this procedure performs an update of   both end points using two intermediate points generated using the secant   interpolation. For details see the steps S1-S4 in [Hager and Zhang (2006)][2].    The interval [a, b] must satisfy the opposite slope conditions described in   the documentation for `update`.    Args:     value_and_gradients_function: A Python callable that accepts a real scalar       tensor and returns an object that can be converted to a namedtuple.       The namedtuple should have fields 'f' and 'df' that correspond to scalar       tensors of real dtype containing the value of the function and its       derivative at that point. The other namedtuple fields, if present,       should be tensors or sequences (possibly nested) of tensors.       In usual optimization application, this function would be generated by       projecting the multivariate objective function along some specific       direction. The direction is determined by some other procedure but should       be a descent direction (i.e. the derivative of the projected univariate       function must be negative at 0.).       Alternatively, the function may represent the batching of `n` such line       functions (e.g. projecting a single multivariate objective function along       `n` distinct directions at once) accepting n points as input, i.e. a       tensor of shape [n], and the fields 'f' and 'df' in the returned       namedtuple should each be a tensor of shape [n], with the corresponding       function values and derivatives at the input points.     val_0: A namedtuple, as returned by value_and_gradients_function evaluated       at `0.`.     search_interval: A namedtuple describing the current search interval,       must include the fields:       - converged: Boolean `Tensor` of shape [n], indicating batch members           where search has already converged. Interval for these batch members           won't be modified.       - failed: Boolean `Tensor` of shape [n], indicating batch members           where search has already failed. Interval for these batch members           wont be modified.       - iterations: Scalar int32 `Tensor`. Number of line search iterations           so far.       - func_evals: Scalar int32 `Tensor`. Number of function evaluations           so far.       - left: A namedtuple, as returned by value_and_gradients_function,           of the left end point of the current search interval.       - right: A namedtuple, as returned by value_and_gradients_function,           of the right end point of the current search interval.     f_lim: Scalar `Tensor` of real dtype. The function value threshold for       the approximate Wolfe conditions to be checked.     sufficient_decrease_param: Positive scalar `Tensor` of real dtype.       Bounded above by the curvature param. Corresponds to 'delta' in the       terminology of [Hager and Zhang (2006)][2].     curvature_param: Positive scalar `Tensor` of real dtype. Bounded above       by `1.`. Corresponds to 'sigma' in the terminology of       [Hager and Zhang (2006)][2].     name: (Optional) Python str. The name prefixed to the ops created by this       function. If not supplied, the default name 'secant2' is used.    Returns:     A namedtuple containing the following fields.       active: A boolean `Tensor` of shape [n]. Used internally by the procedure         to indicate batch members on which there is work left to do.       converged: A boolean `Tensor` of shape [n]. Indicates whether a point         satisfying the Wolfe conditions has been found. If this is True, the         interval will be degenerate (i.e. `left` and `right` below will be         identical).       failed: A boolean `Tensor` of shape [n]. Indicates if invalid function or         gradient values were encountered (i.e. infinity or NaNs).       num_evals: A scalar int32 `Tensor`. The total number of function         evaluations made.       left: Return value of value_and_gradients_function at the updated left         end point of the interval.       right: Return value of value_and_gradients_function at the updated right         end point of the interval.
Helper function for secant square.
Helper function for secant-square step.
Squeezes a bracketing interval containing the minimum.    Given an interval which brackets a minimum and a point in that interval,   finds a smaller nested interval which also brackets the minimum. If the   supplied point does not lie in the bracketing interval, the current interval   is returned.    The following description is given in terms of individual points evaluated on   a line function to be minimized. Note, however, the implementation also   accepts batches of points allowing to minimize multiple line functions at   once. See details on the docstring of `value_and_gradients_function` below.    The requirement of the interval bracketing a minimum is expressed through the   opposite slope conditions. Assume the left end point is 'a', the right   end point is 'b', the function to be minimized is 'f' and the derivative is   'df'. The update procedure relies on the following conditions being satisfied:    '''     f(a) <= f(0) + epsilon   (1)     df(a) < 0                (2)     df(b) > 0                (3)   '''    In the first condition, epsilon is a small positive constant. The condition   demands that the function at the left end point be not much bigger than the   starting point (i.e. 0). This is an easy to satisfy condition because by   assumption, we are in a direction where the function value is decreasing.   The second and third conditions together demand that there is at least one   zero of the derivative in between a and b.    In addition to the interval, the update algorithm requires a third point to   be supplied. Usually, this point would lie within the interval [a, b]. If the   point is outside this interval, the current interval is returned. If the   point lies within the interval, the behaviour of the function and derivative   value at this point is used to squeeze the original interval in a manner that   preserves the opposite slope conditions.    For further details of this component, see the procedure U0-U3 on page 123 of   the [Hager and Zhang (2006)][2] article.    Note that this function does not explicitly verify whether the opposite slope   conditions are satisfied for the supplied interval. It is assumed that this   is so.    Args:     value_and_gradients_function: A Python callable that accepts a real scalar       tensor and returns an object that can be converted to a namedtuple.       The namedtuple should have fields 'f' and 'df' that correspond to scalar       tensors of real dtype containing the value of the function and its       derivative at that point. The other namedtuple fields, if present,       should be tensors or sequences (possibly nested) of tensors.       In usual optimization application, this function would be generated by       projecting the multivariate objective function along some specific       direction. The direction is determined by some other procedure but should       be a descent direction (i.e. the derivative of the projected univariate       function must be negative at 0.).       Alternatively, the function may represent the batching of `n` such line       functions (e.g. projecting a single multivariate objective function along       `n` distinct directions at once) accepting n points as input, i.e. a       tensor of shape [n], and the fields 'f' and 'df' in the returned       namedtuple should each be a tensor of shape [n], with the corresponding       function values and derivatives at the input points.     val_left: Return value of value_and_gradients_function at the left       end point of the bracketing interval (labelles 'a' above).     val_right: Return value of value_and_gradients_function at the right       end point of the bracketing interval (labelles 'b' above).     val_trial: Return value of value_and_gradients_function at the trial point       to be used to shrink the interval (labelled 'c' above).     f_lim: real `Tensor` of shape [n]. The function value threshold for       the approximate Wolfe conditions to be checked for each batch member.     active: optional boolean `Tensor` of shape [n]. Relevant in batching mode       only, indicates batch members on which the update procedure should be       applied. On non-active members the current left/right interval is returned       unmodified.    Returns:     A namedtuple containing the following fields:       iteration: An int32 scalar `Tensor`. The number of iterations performed         by the bisect algorithm.       stopped: A boolean `Tensor` of shape [n]. True for those batch members         where the bisection algorithm terminated.       failed: A boolean `Tensor` of shape [n]. True for those batch members         where an error was encountered.       num_evals: An int32 scalar `Tensor`. The number of times the objective         function was evaluated.       left: Return value of value_and_gradients_function at the updated left         end point of the interval found.       right: Return value of value_and_gradients_function at the updated right         end point of the interval found.
Brackets the minimum given an initial starting point.    Applies the Hager Zhang bracketing algorithm to find an interval containing   a region with points satisfying Wolfe conditions. Uses the supplied initial   step size 'c', the right end point of the provided search interval, to find   such an interval. The only condition on 'c' is that it should be positive.   For more details see steps B0-B3 in [Hager and Zhang (2006)][2].    Args:     value_and_gradients_function: A Python callable that accepts a real scalar       tensor and returns a namedtuple containing the value filed `f` of the       function and its derivative value field `df` at that point.       Alternatively, the function may representthe batching of `n` such line       functions (e.g. projecting a single multivariate objective function along       `n` distinct directions at once) accepting n points as input, i.e. a       tensor of shape [n], and return a tuple of two tensors of shape [n], the       function values and the corresponding derivatives at the input points.     search_interval: A namedtuple describing the current search interval,       must include the fields:       - converged: Boolean `Tensor` of shape [n], indicating batch members           where search has already converged. Interval for these batch members           wont be modified.       - failed: Boolean `Tensor` of shape [n], indicating batch members           where search has already failed. Interval for these batch members           wont be modified.       - iterations: Scalar int32 `Tensor`. Number of line search iterations           so far.       - func_evals: Scalar int32 `Tensor`. Number of function evaluations           so far.       - left: A namedtuple, as returned by value_and_gradients_function           evaluated at 0, the left end point of the current interval.       - right: A namedtuple, as returned by value_and_gradients_function,           of the right end point of the current interval (labelled 'c' above).     f_lim: real `Tensor` of shape [n]. The function value threshold for       the approximate Wolfe conditions to be checked for each batch member.     max_iterations: Int32 scalar `Tensor`. The maximum number of iterations       permitted. The limit applies equally to all batch members.     expansion_param: Scalar positive `Tensor` of real dtype. Must be greater       than `1.`. Used to expand the initial interval in case it does not bracket       a minimum.    Returns:     A namedtuple with the following fields.       iteration: An int32 scalar `Tensor`. The number of iterations performed.         Bounded above by `max_iterations` parameter.       stopped: A boolean `Tensor` of shape [n]. True for those batch members         where the algorithm terminated before reaching `max_iterations`.       failed: A boolean `Tensor` of shape [n]. True for those batch members         where an error was encountered during bracketing.       num_evals: An int32 scalar `Tensor`. The number of times the objective         function was evaluated.       left: Return value of value_and_gradients_function at the updated left         end point of the interval found.       right: Return value of value_and_gradients_function at the updated right         end point of the interval found.
Bisects an interval and updates to satisfy opposite slope conditions.    Corresponds to the step U3 in [Hager and Zhang (2006)][2].    Args:     value_and_gradients_function: A Python callable that accepts a real scalar       tensor and returns a namedtuple containing the value filed `f` of the       function and its derivative value field `df` at that point.       Alternatively, the function may representthe batching of `n` such line       functions (e.g. projecting a single multivariate objective function along       `n` distinct directions at once) accepting n points as input, i.e. a       tensor of shape [n], and return a tuple of two tensors of shape [n], the       function values and the corresponding derivatives at the input points.     initial_left: Return value of value_and_gradients_function at the left end       point of the current bracketing interval.     initial_right: Return value of value_and_gradients_function at the right end       point of the current bracketing interval.     f_lim: real `Tensor` of shape [n]. The function value threshold for       the approximate Wolfe conditions to be checked for each batch member.    Returns:     A namedtuple containing the following fields:       iteration: An int32 scalar `Tensor`. The number of iterations performed.         Bounded above by `max_iterations` parameter.       stopped: A boolean scalar `Tensor`. True if the bisect algorithm         terminated.       failed: A scalar boolean tensor. Indicates whether the objective function         failed to produce a finite value.       num_evals: A scalar int32 tensor. The number of value and gradients         function evaluations.       left: Return value of value_and_gradients_function at the left end         point of the bracketing interval found.       right: Return value of value_and_gradients_function at the right end         point of the bracketing interval found.
Actual implementation of bisect given initial_args in a _BracketResult.
Checks if the supplied values are finite.    Args:     val_1: A namedtuple instance with the function value and derivative,       as returned e.g. by value_and_gradients_function evaluations.     val_2: (Optional) A namedtuple instance with the function value and       derivative, as returned e.g. by value_and_gradients_function evaluations.    Returns:     is_finite: Scalar boolean `Tensor` indicating whether the function value       and the derivative in `val_1` (and optionally in `val_2`) are all finite.
Checks whether the Wolfe or approx Wolfe conditions are satisfied.    The Wolfe conditions are a set of stopping criteria for an inexact line search   algorithm. Let f(a) be the function value along the search direction and   df(a) the derivative along the search direction evaluated a distance 'a'.   Here 'a' is the distance along the search direction. The Wolfe conditions are:      ```None       f(a) <= f(0) + delta * a * df(0)   (Armijo/Sufficient decrease condition)       df(a) >= sigma * df(0)             (Weak curvature condition)     ```   `delta` and `sigma` are two user supplied parameters satisfying:    `0 < delta < sigma <= 1.`. In the following, delta is called    `sufficient_decrease_param` and sigma is called `curvature_param`.    On a finite precision machine, the Wolfe conditions are difficult to satisfy   when one is close to the minimum. Hence, Hager-Zhang propose replacing   the sufficient decrease condition with the following condition on the   derivative in the vicinity of a minimum.      ```None       df(a) <= (2 * delta - 1) * df(0)  (Approx Wolfe sufficient decrease)     ```   This condition is only used if one is near the minimum. This is tested using      ```None       f(a) <= f(0) + epsilon * |f(0)|     ```   The following function checks both the Wolfe and approx Wolfe conditions.   Here, `epsilon` is a small positive constant. In the following, the argument   `f_lim` corresponds to the product: epsilon * |f(0)|.    Args:     val_0: A namedtuple, as returned by value_and_gradients_function       evaluated at 0.     val_c: A namedtuple, as returned by value_and_gradients_function       evaluated at the point to be tested.     f_lim: Scalar `Tensor` of real dtype. The function value threshold for       the approximate Wolfe conditions to be checked.     sufficient_decrease_param: Positive scalar `Tensor` of real dtype.       Bounded above by the curvature param. Corresponds to 'delta' in the       terminology of [Hager and Zhang (2006)][2].     curvature_param: Positive scalar `Tensor` of real dtype. Bounded above       by `1.`. Corresponds to 'sigma' in the terminology of       [Hager Zhang (2005)][1].    Returns:     is_satisfied: A scalar boolean `Tensor` which is True if either the       Wolfe or approximate Wolfe conditions are satisfied.
Returns the secant interpolation for the minimum.    The secant method is a technique for finding roots of nonlinear functions.   When finding the minimum, one applies the secant method to the derivative   of the function.   For an arbitrary function and a bounding interval, the secant approximation   can produce the next point which is outside the bounding interval. However,   with the assumption of opposite slope condtion on the interval [a,b] the new   point c is always bracketed by [a,b]. Note that by assumption,   f'(a) < 0 and f'(b) > 0.   Hence c is a weighted average of a and b and thus always in [a, b].    Args:     val_a: A namedtuple with the left end point, function value and derivative,       of the current interval (i.e. a).     val_b: A namedtuple with the right end point, function value and derivative,       of the current interval (i.e. b).    Returns:     approx_minimum: A scalar real `Tensor`. An approximation to the point       at which the derivative vanishes.
Create a function implementing a step-size update policy.    The simple policy increases or decreases the `step_size_var` based on the   average of `exp(minimum(0., log_accept_ratio))`. It is based on   [Section 4.2 of Andrieu and Thoms (2008)](   https://people.eecs.berkeley.edu/~jordan/sail/readings/andrieu-thoms.pdf).    The `num_adaptation_steps` argument is set independently of any burnin   for the overall chain. In general, adaptation prevents the chain from   reaching a stationary distribution, so obtaining consistent samples requires   `num_adaptation_steps` be set to a value [somewhat smaller](   http://andrewgelman.com/2017/12/15/burn-vs-warm-iterative-simulation-algorithms/#comment-627745)   than the number of burnin steps. However, it may sometimes be helpful to set   `num_adaptation_steps` to a larger value during development in order to   inspect the behavior of the chain during adaptation.    Args:     num_adaptation_steps: Scalar `int` `Tensor` number of initial steps to       during which to adjust the step size. This may be greater, less than, or       equal to the number of burnin steps. If `None`, the step size is adapted       on every step (note this breaks stationarity of the chain!).     target_rate: Scalar `Tensor` representing desired `accept_ratio`.       Default value: `0.75` (i.e., [center of asymptotically optimal       rate](https://arxiv.org/abs/1411.6669)).     decrement_multiplier: `Tensor` representing amount to downscale current       `step_size`.       Default value: `0.01`.     increment_multiplier: `Tensor` representing amount to upscale current       `step_size`.       Default value: `0.01`.     step_counter: Scalar `int` `Variable` specifying the current step. The step       size is adapted iff `step_counter < num_adaptation_steps`.       Default value: if `None`, an internal variable         `step_size_adaptation_step_counter` is created and initialized to `-1`.    Returns:     step_size_simple_update_fn: Callable that takes args       `step_size_var, kernel_results` and returns updated step size(s).
Applies `num_leapfrog_steps` of the leapfrog integrator.    Assumes a simple quadratic kinetic energy function: `0.5 ||momentum||**2`.    #### Examples:    ##### Simple quadratic potential.    ```python   import matplotlib.pyplot as plt   %matplotlib inline   import numpy as np   import tensorflow as tf   from tensorflow_probability.python.mcmc.hmc import _leapfrog_integrator_one_step  # pylint: disable=line-too-long   tfd = tfp.distributions    dims = 10   num_iter = int(1e3)   dtype = np.float32    position = tf.placeholder(np.float32)   momentum = tf.placeholder(np.float32)    target_log_prob_fn = tfd.MultivariateNormalDiag(       loc=tf.zeros(dims, dtype)).log_prob    def _leapfrog_one_step(*args):     # Closure representing computation done during each leapfrog step.     return _leapfrog_integrator_one_step(         target_log_prob_fn=target_log_prob_fn,         independent_chain_ndims=0,         step_sizes=[0.1],         current_momentum_parts=args[0],         current_state_parts=args[1],         current_target_log_prob=args[2],         current_target_log_prob_grad_parts=args[3])    # Do leapfrog integration.   [       [next_momentum],       [next_position],       next_target_log_prob,       next_target_log_prob_grad_parts,   ] = tf.while_loop(       cond=lambda *args: True,       body=_leapfrog_one_step,       loop_vars=[         [momentum],         [position],         target_log_prob_fn(position),         tf.gradients(target_log_prob_fn(position), position),       ],       maximum_iterations=3)    momentum_ = np.random.randn(dims).astype(dtype)   position_ = np.random.randn(dims).astype(dtype)   positions = np.zeros([num_iter, dims], dtype)    with tf.Session() as sess:     for i in xrange(num_iter):       position_, momentum_ = sess.run(           [next_momentum, next_position],           feed_dict={position: position_, momentum: momentum_})       positions[i] = position_    plt.plot(positions[:, 0]);  # Sinusoidal.   ```    Args:     target_log_prob_fn: Python callable which takes an argument like       `*current_state_parts` and returns its (possibly unnormalized) log-density       under the target distribution.     independent_chain_ndims: Scalar `int` `Tensor` representing the number of       leftmost `Tensor` dimensions which index independent chains.     step_sizes: Python `list` of `Tensor`s representing the step size for the       leapfrog integrator. Must broadcast with the shape of       `current_state_parts`.  Larger step sizes lead to faster progress, but       too-large step sizes make rejection exponentially more likely. When       possible, it's often helpful to match per-variable step sizes to the       standard deviations of the target distribution in each variable.     current_momentum_parts: Tensor containing the value(s) of the momentum       variable(s) to update.     current_state_parts: Python `list` of `Tensor`s representing the current       state(s) of the Markov chain(s). The first `independent_chain_ndims` of       the `Tensor`(s) index different chains.     current_target_log_prob: `Tensor` representing the value of       `target_log_prob_fn(*current_state_parts)`. The only reason to specify       this argument is to reduce TF graph size.     current_target_log_prob_grad_parts: Python list of `Tensor`s representing       gradient of `target_log_prob_fn(*current_state_parts`) wrt       `current_state_parts`. Must have same shape as `current_state_parts`. The       only reason to specify this argument is to reduce TF graph size.     state_gradients_are_stopped: Python `bool` indicating that the proposed new       state be run through `tf.stop_gradient`. This is particularly useful when       combining optimization over samples from the HMC chain.       Default value: `False` (i.e., do not apply `stop_gradient`).     name: Python `str` name prefixed to Ops created by this function.       Default value: `None` (i.e., 'hmc_leapfrog_integrator').    Returns:     proposed_momentum_parts: Updated value of the momentum.     proposed_state_parts: Tensor or Python list of `Tensor`s representing the       state(s) of the Markov chain(s) at each result step. Has same shape as       input `current_state_parts`.     proposed_target_log_prob: `Tensor` representing the value of       `target_log_prob_fn` at `next_state`.     proposed_target_log_prob_grad_parts: Gradient of `proposed_target_log_prob`       wrt `next_state`.    Raises:     ValueError: if `len(momentum_parts) != len(state_parts)`.     ValueError: if `len(state_parts) != len(step_sizes)`.     ValueError: if `len(state_parts) != len(grads_target_log_prob)`.     TypeError: if `not target_log_prob.dtype.is_floating`.
Helper to `kernel` which computes the log acceptance-correction.    A sufficient but not necessary condition for the existence of a stationary   distribution, `p(x)`, is "detailed balance", i.e.:    ```none   p(x'|x) p(x) = p(x|x') p(x')   ```    In the Metropolis-Hastings algorithm, a state is proposed according to   `g(x'|x)` and accepted according to `a(x'|x)`, hence   `p(x'|x) = g(x'|x) a(x'|x)`.    Inserting this into the detailed balance equation implies:    ```none       g(x'|x) a(x'|x) p(x) = g(x|x') a(x|x') p(x')   ==> a(x'|x) / a(x|x') = p(x') / p(x) [g(x|x') / g(x'|x)]    (*)   ```    One definition of `a(x'|x)` which satisfies (*) is:    ```none   a(x'|x) = min(1, p(x') / p(x) [g(x|x') / g(x'|x)])   ```    (To see that this satisfies (*), notice that under this definition only at   most one `a(x'|x)` and `a(x|x') can be other than one.)    We call the bracketed term the "acceptance correction".    In the case of UncalibratedHMC, the log acceptance-correction is not the log   proposal-ratio. UncalibratedHMC augments the state-space with momentum, z.   Assuming a standard Gaussian distribution for momentums, the chain eventually   converges to:    ```none   p([x, z]) propto= target_prob(x) exp(-0.5 z**2)   ```    Relating this back to Metropolis-Hastings parlance, for HMC we have:    ```none   p([x, z]) propto= target_prob(x) exp(-0.5 z**2)   g([x, z] | [x', z']) = g([x', z'] | [x, z])   ```    In other words, the MH bracketed term is `1`. However, because we desire to   use a general MH framework, we can place the momentum probability ratio inside   the metropolis-correction factor thus getting an acceptance probability:    ```none                        target_prob(x')   accept_prob(x'|x) = -----------------  [exp(-0.5 z**2) / exp(-0.5 z'**2)]                        target_prob(x)   ```    (Note: we actually need to handle the kinetic energy change at each leapfrog   step, but this is the idea.)    Args:     current_momentums: `Tensor` representing the value(s) of the current       momentum(s) of the state (parts).     proposed_momentums: `Tensor` representing the value(s) of the proposed       momentum(s) of the state (parts).     independent_chain_ndims: Scalar `int` `Tensor` representing the number of       leftmost `Tensor` dimensions which index independent chains.     name: Python `str` name prefixed to Ops created by this function.       Default value: `None` (i.e., 'compute_log_acceptance_correction').    Returns:     log_acceptance_correction: `Tensor` representing the `log`       acceptance-correction.  (See docstring for mathematical definition.)
Runs one iteration of Hamiltonian Monte Carlo.      Args:       current_state: `Tensor` or Python `list` of `Tensor`s representing the         current state(s) of the Markov chain(s). The first `r` dimensions index         independent chains, `r = tf.rank(target_log_prob_fn(*current_state))`.       previous_kernel_results: `collections.namedtuple` containing `Tensor`s         representing values from previous calls to this function (or from the         `bootstrap_results` function.)      Returns:       next_state: Tensor or Python list of `Tensor`s representing the state(s)         of the Markov chain(s) after taking exactly one step. Has same type and         shape as `current_state`.       kernel_results: `collections.namedtuple` of internal calculations used to         advance the chain.      Raises:       ValueError: if there isn't one `step_size` or a list with same length as         `current_state`.
Creates initial `previous_kernel_results` using a supplied `state`.
Constructs a ResNet18 model.    Args:     input_shape: A `tuple` indicating the Tensor shape.     num_classes: `int` representing the number of class labels.     kernel_posterior_scale_mean: Python `int` number for the kernel       posterior's scale (log variance) mean. The smaller the mean the closer       is the initialization to a deterministic network.     kernel_posterior_scale_stddev: Python `float` number for the initial kernel       posterior's scale stddev.       ```       q(W|x) ~ N(mu, var),       log_var ~ N(kernel_posterior_scale_mean, kernel_posterior_scale_stddev)       ````     kernel_posterior_scale_constraint: Python `float` number for the log value       to constrain the log variance throughout training.       i.e. log_var <= log(kernel_posterior_scale_constraint).    Returns:     tf.keras.Model.
Network block for ResNet.
Create the encoder function.    Args:     activation: Activation function to use.     num_topics: The number of topics.     layer_sizes: The number of hidden units per layer in the encoder.    Returns:     encoder: A `callable` mapping a bag-of-words `Tensor` to a       `tfd.Distribution` instance over topics.
Create the decoder function.    Args:     num_topics: The number of topics.     num_words: The number of words.    Returns:     decoder: A `callable` mapping a `Tensor` of encodings to a       `tfd.Distribution` instance over words.
Create the prior distribution.    Args:     num_topics: Number of topics.     initial_value: The starting value for the prior parameters.    Returns:     prior: A `callable` that returns a `tf.distribution.Distribution`         instance, the prior distribution.     prior_variables: A `list` of `Variable` objects, the trainable parameters         of the prior.
Implements Markov chain Monte Carlo via repeated `TransitionKernel` steps.    This function samples from an Markov chain at `current_state` and whose   stationary distribution is governed by the supplied `TransitionKernel`   instance (`kernel`).    This function can sample from multiple chains, in parallel. (Whether or not   there are multiple chains is dictated by the `kernel`.)    The `current_state` can be represented as a single `Tensor` or a `list` of   `Tensors` which collectively represent the current state.    Since MCMC states are correlated, it is sometimes desirable to produce   additional intermediate states, and then discard them, ending up with a set of   states with decreased autocorrelation.  See [Owen (2017)][1]. Such "thinning"   is made possible by setting `num_steps_between_results > 0`. The chain then   takes `num_steps_between_results` extra steps between the steps that make it   into the results. The extra steps are never materialized (in calls to   `sess.run`), and thus do not increase memory requirements.    Warning: when setting a `seed` in the `kernel`, ensure that `sample_chain`'s   `parallel_iterations=1`, otherwise results will not be reproducible.    In addition to returning the chain state, this function supports tracing of   auxiliary variables used by the kernel. The traced values are selected by   specifying `trace_fn`. By default, all kernel results are traced but in the   future the default will be changed to no results being traced, so plan   accordingly. See below for some examples of this feature.    Args:     num_results: Integer number of Markov chain draws.     current_state: `Tensor` or Python `list` of `Tensor`s representing the       current state(s) of the Markov chain(s).     previous_kernel_results: A `Tensor` or a nested collection of `Tensor`s       representing internal calculations made within the previous call to this       function (or as returned by `bootstrap_results`).     kernel: An instance of `tfp.mcmc.TransitionKernel` which implements one step       of the Markov chain.     num_burnin_steps: Integer number of chain steps to take before starting to       collect results.       Default value: 0 (i.e., no burn-in).     num_steps_between_results: Integer number of chain steps between collecting       a result. Only one out of every `num_steps_between_samples + 1` steps is       included in the returned results.  The number of returned chain states is       still equal to `num_results`.  Default value: 0 (i.e., no thinning).     trace_fn: A callable that takes in the current chain state and the previous       kernel results and return a `Tensor` or a nested collection of `Tensor`s       that is then traced along with the chain state.     return_final_kernel_results: If `True`, then the final kernel results are       returned alongside the chain state and the trace specified by the       `trace_fn`.     parallel_iterations: The number of iterations allowed to run in parallel. It       must be a positive integer. See `tf.while_loop` for more details.     name: Python `str` name prefixed to Ops created by this function.       Default value: `None` (i.e., "mcmc_sample_chain").    Returns:     checkpointable_states_and_trace: if `return_final_kernel_results` is       `True`. The return value is an instance of       `CheckpointableStatesAndTrace`.     all_states: if `return_final_kernel_results` is `False` and `trace_fn` is       `None`. The return value is a `Tensor` or Python list of `Tensor`s       representing the state(s) of the Markov chain(s) at each result step. Has       same shape as input `current_state` but with a prepended       `num_results`-size dimension.     states_and_trace: if `return_final_kernel_results` is `False` and       `trace_fn` is not `None`. The return value is an instance of       `StatesAndTrace`.    #### Examples    ##### Sample from a diagonal-variance Gaussian.    I.e.,    ```none   for i=1..n:     x[i] ~ MultivariateNormal(loc=0, scale=diag(true_stddev))  # likelihood   ```    ```python   import tensorflow as tf   import tensorflow_probability as tfp   tfd = tfp.distributions    dims = 10   true_stddev = np.sqrt(np.linspace(1., 3., dims))   likelihood = tfd.MultivariateNormalDiag(loc=0., scale_diag=true_stddev)    states = tfp.mcmc.sample_chain(       num_results=1000,       num_burnin_steps=500,       current_state=tf.zeros(dims),       kernel=tfp.mcmc.HamiltonianMonteCarlo(         target_log_prob_fn=likelihood.log_prob,         step_size=0.5,         num_leapfrog_steps=2),       trace_fn=None)    sample_mean = tf.reduce_mean(states, axis=0)   # ==> approx all zeros    sample_stddev = tf.sqrt(tf.reduce_mean(       tf.squared_difference(states, sample_mean),       axis=0))   # ==> approx equal true_stddev   ```    ##### Sampling from factor-analysis posteriors with known factors.    I.e.,    ```none   # prior   w ~ MultivariateNormal(loc=0, scale=eye(d))   for i=1..n:     # likelihood     x[i] ~ Normal(loc=w^T F[i], scale=1)   ```    where `F` denotes factors.    ```python   import tensorflow as tf   import tensorflow_probability as tfp   tfd = tfp.distributions    # Specify model.   def make_prior(dims):     return tfd.MultivariateNormalDiag(         loc=tf.zeros(dims))    def make_likelihood(weights, factors):     return tfd.MultivariateNormalDiag(         loc=tf.matmul(weights, factors, adjoint_b=True))    def joint_log_prob(num_weights, factors, x, w):     return (make_prior(num_weights).log_prob(w) +             make_likelihood(w, factors).log_prob(x))    def unnormalized_log_posterior(w):     # Posterior is proportional to: `p(W, X=x | factors)`.     return joint_log_prob(num_weights, factors, x, w)    # Setup data.   num_weights = 10 # == d   num_factors = 40 # == n   num_chains = 100    weights = make_prior(num_weights).sample(1)   factors = tf.random_normal([num_factors, num_weights])   x = make_likelihood(weights, factors).sample()    # Sample from Hamiltonian Monte Carlo Markov Chain.    # Get `num_results` samples from `num_chains` independent chains.   chains_states, kernels_results = tfp.mcmc.sample_chain(       num_results=1000,       num_burnin_steps=500,       current_state=tf.zeros([num_chains, num_weights], name='init_weights'),       kernel=tfp.mcmc.HamiltonianMonteCarlo(         target_log_prob_fn=unnormalized_log_posterior,         step_size=0.1,         num_leapfrog_steps=2))    # Compute sample stats.   sample_mean = tf.reduce_mean(chains_states, axis=[0, 1])   # ==> approx equal to weights    sample_var = tf.reduce_mean(       tf.squared_difference(chains_states, sample_mean),       axis=[0, 1])   # ==> less than 1   ```    ##### Custom tracing functions.    ```python   import tensorflow as tf   import tensorflow_probability as tfp   tfd = tfp.distributions    likelihood = tfd.Normal(loc=0., scale=1.)    def sample_chain(trace_fn):     return tfp.mcmc.sample_chain(       num_results=1000,       num_burnin_steps=500,       current_state=0.,       kernel=tfp.mcmc.HamiltonianMonteCarlo(         target_log_prob_fn=likelihood.log_prob,         step_size=0.5,         num_leapfrog_steps=2),       trace_fn=trace_fn)    def trace_log_accept_ratio(states, previous_kernel_results):     return previous_kernel_results.log_accept_ratio    def trace_everything(states, previous_kernel_results):     return previous_kernel_results    _, log_accept_ratio = sample_chain(trace_fn=trace_log_accept_ratio)   _, kernel_results = sample_chain(trace_fn=trace_everything)    acceptance_prob = tf.exp(tf.minimum(log_accept_ratio_, 0.))   # Equivalent to, but more efficient than:   acceptance_prob = tf.exp(tf.minimum(kernel_results.log_accept_ratio_, 0.))   ```    #### References    [1]: Art B. Owen. Statistically efficient thinning of a Markov chain sampler.        _Technical Report_, 2017.        http://statweb.stanford.edu/~owen/reports/bestthinning.pdf
A multi-layered topic model over a documents-by-terms matrix.
Learnable Deterministic distribution over positive reals.
Learnable Gamma via concentration and scale parameterization.
Loads NIPS 2011 conference papers.    The NIPS 1987-2015 data set is in the form of a 11,463 x 5,812 matrix of   per-paper word counts, containing 11,463 words and 5,811 NIPS conference   papers (Perrone et al., 2016). We subset to papers in 2011 and words appearing   in at least two documents and having a total word count of at least 10.    Built from the Observations Python package.    Args:     path: str.       Path to directory which either stores file or otherwise file will       be downloaded and extracted there. Filename is `NIPS_1987-2015.csv`.    Returns:     bag_of_words: np.ndarray of shape [num_documents, num_words]. Each element       denotes the number of occurrences of a specific word in a specific       document.     words: List of strings, denoting the words for `bag_of_words`'s columns.
Shared init logic for `amplitude` and `length_scale` params.      Args:       amplitude: `Tensor` (or convertible) or `None` to convert, validate.       length_scale: `Tensor` (or convertible) or `None` to convert, validate.       validate_args: If `True`, parameters are checked for validity despite         possibly degrading runtime performance      Returns:       dtype: The common `DType` of the parameters.
Get the KL function registered for classes a and b.
Returns an image tensor.
Downloads the sprites data and returns the saved filepath.
Creates a character sprite from a set of attribute sprites.
Creates a sequence.    Args:     character: A character sprite tensor.     action_metadata: An action metadata tuple.     direction: An integer representing the direction, i.e., the row       offset within each action group corresponding to a particular       direction.     length: Desired length of the sequence. If this is longer than       the number of available frames, it will roll over to the       beginning.     start: Index of possible frames at which to start the sequence.    Returns:     A sequence tensor.
Creates a random sequence.
Creates a tf.data pipeline for the sprites dataset.    Args:     characters: A list of (skin, hair, top, pants) tuples containing       relative paths to the sprite png image for each attribute.     actions: A list of Actions.     directions: A list of Directions.     channels: Number of image channels to yield.     length: Desired length of the sequences.     shuffle: Whether or not to shuffle the characters and sequences       start frame.     fake_data: Boolean for whether or not to yield synthetic data.    Returns:     A tf.data.Dataset yielding (seq, skin label index, hair label index,     top label index, pants label index, action label index, skin label     name, hair label_name, top label name, pants label name, action     label name) tuples.
Checks that `distributions` satisfies all assumptions.
Flatten a list of kernels which may contain _SumKernel instances.    Args:     kernels: Python list of `PositiveSemidefiniteKernel` instances    Returns:     Python list containing the elements of kernels, with any _SumKernel     instances replaced by their `kernels` property contents.
Flatten a list of kernels which may contain _ProductKernel instances.    Args:     kernels: Python list of `PositiveSemidefiniteKernel` instances    Returns:     Python list containing the elements of kernels, with any _ProductKernel     instances replaced by their `kernels` property contents.
Build fake CIFAR10-style data for unit testing.
Counts the number of occurrences of each value in an integer array `arr`.    Works like `tf.math.bincount`, but provides an `axis` kwarg that specifies   dimensions to reduce over.  With     `~axis = [i for i in range(arr.ndim) if i not in axis]`,   this function returns a `Tensor` of shape `[K] + arr.shape[~axis]`.    If `minlength` and `maxlength` are not given, `K = tf.reduce_max(arr) + 1`   if `arr` is non-empty, and 0 otherwise.   If `weights` are non-None, then index `i` of the output stores the sum of the   value in `weights` at each index where the corresponding value in `arr` is   `i`.    Args:     arr: An `int32` `Tensor` of non-negative values.     weights: If non-None, must be the same shape as arr. For each value in       `arr`, the bin will be incremented by the corresponding weight instead of       1.     minlength: If given, ensures the output has length at least `minlength`,       padding with zeros at the end if necessary.     maxlength: If given, skips values in `arr` that are equal or greater than       `maxlength`, ensuring that the output has length at most `maxlength`.     axis: A `0-D` or `1-D` `int32` `Tensor` (with static values) designating       dimensions in `arr` to reduce over.       `Default value:` `None`, meaning reduce over all dimensions.     dtype: If `weights` is None, determines the type of the output bins.     name: A name scope for the associated operations (optional).    Returns:     A vector with the same dtype as `weights` or the given `dtype`. The bin     values.
Bin values into discrete intervals.    Given `edges = [c0, ..., cK]`, defining intervals   `I0 = [c0, c1)`, `I1 = [c1, c2)`, ..., `I_{K-1} = [c_{K-1}, cK]`,   This function returns `bins`, such that:   `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.    Args:     x:  Numeric `N-D` `Tensor` with `N > 0`.     edges:  `Tensor` of same `dtype` as `x`.  The first dimension indexes edges       of intervals.  Must either be `1-D` or have       `x.shape[1:] == edges.shape[1:]`.  If `rank(edges) > 1`, `edges[k]`       designates a shape `edges.shape[1:]` `Tensor` of bin edges for the       corresponding dimensions of `x`.     extend_lower_interval:  Python `bool`.  If `True`, extend the lowest       interval `I0` to `(-inf, c1]`.     extend_upper_interval:  Python `bool`.  If `True`, extend the upper       interval `I_{K-1}` to `[c_{K-1}, +inf)`.     dtype: The output type (`int32` or `int64`). `Default value:` `x.dtype`.       This effects the output values when `x` is below/above the intervals,       which will be `-1/K+1` for `int` types and `NaN` for `float`s.       At indices where `x` is `NaN`, the output values will be `0` for `int`       types and `NaN` for floats.     name:  A Python string name to prepend to created ops. Default: 'find_bins'    Returns:     bins: `Tensor` with same `shape` as `x` and `dtype`.       Has whole number values.  `bins[i] = k` means the `x[i]` falls into the       `kth` bin, ie, `edges[bins[i]] <= x[i] < edges[bins[i] + 1]`.    Raises:     ValueError:  If `edges.shape[0]` is determined to be less than 2.    #### Examples    Cut a `1-D` array    ```python   x = [0., 5., 6., 10., 20.]   edges = [0., 5., 10.]   tfp.stats.find_bins(x, edges)   ==> [0., 0., 1., 1., np.nan]   ```    Cut `x` into its deciles    ```python   x = tf.random_uniform(shape=(100, 200))   decile_edges = tfp.stats.quantiles(x, num_quantiles=10)   bins = tfp.stats.find_bins(x, edges=decile_edges)   bins.shape   ==> (100, 200)   tf.reduce_mean(bins == 0.)   ==> approximately 0.1   tf.reduce_mean(bins == 1.)   ==> approximately 0.1   ```
Count how often `x` falls in intervals defined by `edges`.    Given `edges = [c0, ..., cK]`, defining intervals   `I0 = [c0, c1)`, `I1 = [c1, c2)`, ..., `I_{K-1} = [c_{K-1}, cK]`,   This function counts how often `x` falls into each interval.    Values of `x` outside of the intervals cause errors.  Consider using   `extend_lower_interval`, `extend_upper_interval` to deal with this.    Args:     x:  Numeric `N-D` `Tensor` with `N > 0`.  If `axis` is not       `None`, must have statically known number of dimensions. The       `axis` kwarg determines which dimensions index iid samples.       Other dimensions of `x` index "events" for which we will compute different       histograms.     edges:  `Tensor` of same `dtype` as `x`.  The first dimension indexes edges       of intervals.  Must either be `1-D` or have `edges.shape[1:]` the same       as the dimensions of `x` excluding `axis`.       If `rank(edges) > 1`, `edges[k]` designates a shape `edges.shape[1:]`       `Tensor` of interval edges for the corresponding dimensions of `x`.     axis:  Optional `0-D` or `1-D` integer `Tensor` with constant       values. The axis in `x` that index iid samples.       `Default value:` `None` (treat every dimension as sample dimension).     extend_lower_interval:  Python `bool`.  If `True`, extend the lowest       interval `I0` to `(-inf, c1]`.     extend_upper_interval:  Python `bool`.  If `True`, extend the upper       interval `I_{K-1}` to `[c_{K-1}, +inf)`.     dtype: The output type (`int32` or `int64`). `Default value:` `x.dtype`.     name:  A Python string name to prepend to created ops.       `Default value:` 'histogram'    Returns:     counts: `Tensor` of type `dtype` and, with       `~axis = [i for i in range(arr.ndim) if i not in axis]`,       `counts.shape = [edges.shape[0]] + x.shape[~axis]`.       With `I` a multi-index into `~axis`, `counts[k][I]` is the number of times       event(s) fell into the `kth` interval of `edges`.    #### Examples    ```python   # x.shape = [1000, 2]   # x[:, 0] ~ Uniform(0, 1), x[:, 1] ~ Uniform(1, 2).   x = tf.stack([tf.random_uniform([1000]), 1 + tf.random_uniform([1000])],                axis=-1)    # edges ==> bins [0, 0.5), [0.5, 1.0), [1.0, 1.5), [1.5, 2.0].   edges = [0., 0.5, 1.0, 1.5, 2.0]    tfp.stats.histogram(x, edges)   ==> approximately [500, 500, 500, 500]    tfp.stats.histogram(x, edges, axis=0)   ==> approximately [[500, 500, 0, 0], [0, 0, 500, 500]]   ```
Compute quantiles of `x` along `axis`.    The quantiles of a distribution are cut points dividing the range into   intervals with equal probabilities.    Given a vector `x` of samples, this function estimates the cut points by   returning `num_quantiles + 1` cut points, `(c0, ..., cn)`, such that, roughly   speaking, equal number of sample points lie in the `num_quantiles` intervals   `[c0, c1), [c1, c2), ..., [c_{n-1}, cn]`.  That is,    * About `1 / n` fraction of the data lies in `[c_{k-1}, c_k)`, `k = 1, ..., n`   * About `k / n` fraction of the data lies below `c_k`.   * `c0` is the sample minimum and `cn` is the maximum.    The exact number of data points in each interval depends on the size of   `x` (e.g. whether the size is divisible by `n`) and the `interpolation` kwarg.    Args:     x:  Numeric `N-D` `Tensor` with `N > 0`.  If `axis` is not `None`,       `x` must have statically known number of dimensions.     num_quantiles:  Scalar `integer` `Tensor`.  The number of intervals the       returned `num_quantiles + 1` cut points divide the range into.     axis:  Optional `0-D` or `1-D` integer `Tensor` with constant values. The       axis that index independent samples over which to return the desired       percentile.  If `None` (the default), treat every dimension as a sample       dimension, returning a scalar.     interpolation : {'nearest', 'linear', 'lower', 'higher', 'midpoint'}.       Default value: 'nearest'.  This specifies the interpolation method to       use when the fractions `k / n` lie between two data points `i < j`:         * linear: i + (j - i) * fraction, where fraction is the fractional part           of the index surrounded by i and j.         * lower: `i`.         * higher: `j`.         * nearest: `i` or `j`, whichever is nearest.         * midpoint: (i + j) / 2. `linear` and `midpoint` interpolation do not           work with integer dtypes.     keep_dims:  Python `bool`. If `True`, the last dimension is kept with size 1       If `False`, the last dimension is removed from the output shape.     validate_args:  Whether to add runtime checks of argument validity. If       False, and arguments are incorrect, correct behavior is not guaranteed.     name:  A Python string name to give this `Op`.  Default is 'percentile'    Returns:     cut_points:  A `rank(x) + 1 - len(axis)` dimensional `Tensor` with same     `dtype` as `x` and shape `[num_quantiles + 1, ...]` where the trailing shape     is that of `x` without the dimensions in `axis` (unless `keep_dims is True`)    Raises:     ValueError:  If argument 'interpolation' is not an allowed type.     ValueError:  If interpolation type not compatible with `dtype`.    #### Examples    ```python   # Get quartiles of x with various interpolation choices.   x = [0.,  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.]    tfp.stats.quantiles(x, num_quantiles=4, interpolation='nearest')   ==> [  0.,   2.,   5.,   8.,  10.]    tfp.stats.quantiles(x, num_quantiles=4, interpolation='linear')   ==> [  0. ,   2.5,   5. ,   7.5,  10. ]    tfp.stats.quantiles(x, num_quantiles=4, interpolation='lower')   ==> [  0.,   2.,   5.,   7.,  10.]    # Get deciles of columns of an R x C data set.   data = load_my_columnar_data(...)   tfp.stats.quantiles(data, num_quantiles=10)   ==> Shape [11, C] Tensor   ```
Get static number of dimensions and assert that some expectations are met.    This function returns the number of dimensions 'ndims' of x, as a Python int.    The optional expect arguments are used to check the ndims of x, but this is   only done if the static ndims of x is not None.    Args:     x:  A Tensor.     expect_static:  Expect `x` to have statically defined `ndims`.     expect_ndims:  Optional Python integer.  If provided, assert that x has       number of dimensions equal to this.     expect_ndims_no_more_than:  Optional Python integer.  If provided, assert       that x has no more than this many dimensions.     expect_ndims_at_least:  Optional Python integer.  If provided, assert that x       has at least this many dimensions.    Returns:     ndims:  A Python integer.    Raises:     ValueError:  If any of the expectations above are violated.
Insert the dims in `axis` back as singletons after being removed.    Args:     x:  `Tensor`.     axis:  Python list of integers.    Returns:     `Tensor` with same values as `x`, but additional singleton dimensions.
Convert possibly negatively indexed axis to non-negative list of ints.    Args:     axis:  Integer Tensor.     ndims:  Number of dimensions into which axis indexes.    Returns:     A list of non-negative Python integers.    Raises:     ValueError: If `axis` is not statically defined.
Move dims corresponding to `axis` in `x` to the end, then flatten.    Args:     x: `Tensor` with shape `[B0,B1,...,Bb]`.     axis:  Python list of indices into dimensions of `x`.     x_ndims:  Python integer holding number of dimensions in `x`.     right_end:  Python bool.  Whether to move dims to the right end (else left).    Returns:     `Tensor` with value from `x` and dims in `axis` moved to end into one single       dimension.
Use `top_k` to sort a `Tensor` along the last dimension.
Build an ordered list of Distribution instances for component models.      Args:       num_timesteps: Python `int` number of timesteps to model.       param_vals: a list of `Tensor` parameter values in order corresponding to         `self.parameters`, or a dict mapping from parameter names to values.       initial_step: optional `int` specifying the initial timestep to model.         This is relevant when the model contains time-varying components,         e.g., holidays or seasonality.      Returns:       component_ssms: a Python list of `LinearGaussianStateSpaceModel`         Distribution objects, in order corresponding to `self.components`.
The Amari-alpha Csiszar-function in log-space.    A Csiszar-function is a member of,    ```none   F = { f:R_+ to R : f convex }.   ```    When `self_normalized = True`, the Amari-alpha Csiszar-function is:    ```none   f(u) = { -log(u) + (u - 1),     alpha = 0          { u log(u) - (u - 1),    alpha = 1          { [(u**alpha - 1) - alpha (u - 1)] / (alpha (alpha - 1)),    otherwise   ```    When `self_normalized = False` the `(u - 1)` terms are omitted.    Warning: when `alpha != 0` and/or `self_normalized = True` this function makes   non-log-space calculations and may therefore be numerically unstable for   `|logu| >> 0`.    For more information, see:     A. Cichocki and S. Amari. "Families of Alpha-Beta-and GammaDivergences:     Flexible and Robust Measures of Similarities." Entropy, vol. 12, no. 6, pp.     1532-1568, 2010.    Args:     logu: `float`-like `Tensor` representing `log(u)` from above.     alpha: `float`-like Python scalar. (See Mathematical Details for meaning.)     self_normalized: Python `bool` indicating whether `f'(u=1)=0`. When       `f'(u=1)=0` the implied Csiszar f-Divergence remains non-negative even       when `p, q` are unnormalized measures.     name: Python `str` name prefixed to Ops created by this function.    Returns:     amari_alpha_of_u: `float`-like `Tensor` of the Csiszar-function evaluated       at `u = exp(logu)`.    Raises:     TypeError: if `alpha` is `None` or a `Tensor`.     TypeError: if `self_normalized` is `None` or a `Tensor`.
The reverse Kullback-Leibler Csiszar-function in log-space.    A Csiszar-function is a member of,    ```none   F = { f:R_+ to R : f convex }.   ```    When `self_normalized = True`, the KL-reverse Csiszar-function is:    ```none   f(u) = -log(u) + (u - 1)   ```    When `self_normalized = False` the `(u - 1)` term is omitted.    Observe that as an f-Divergence, this Csiszar-function implies:    ```none   D_f[p, q] = KL[q, p]   ```    The KL is "reverse" because in maximum likelihood we think of minimizing `q`   as in `KL[p, q]`.    Warning: when self_normalized = True` this function makes non-log-space   calculations and may therefore be numerically unstable for `|logu| >> 0`.    Args:     logu: `float`-like `Tensor` representing `log(u)` from above.     self_normalized: Python `bool` indicating whether `f'(u=1)=0`. When       `f'(u=1)=0` the implied Csiszar f-Divergence remains non-negative even       when `p, q` are unnormalized measures.     name: Python `str` name prefixed to Ops created by this function.    Returns:     kl_reverse_of_u: `float`-like `Tensor` of the Csiszar-function evaluated at       `u = exp(logu)`.    Raises:     TypeError: if `self_normalized` is `None` or a `Tensor`.
The Jensen-Shannon Csiszar-function in log-space.    A Csiszar-function is a member of,    ```none   F = { f:R_+ to R : f convex }.   ```    When `self_normalized = True`, the Jensen-Shannon Csiszar-function is:    ```none   f(u) = u log(u) - (1 + u) log(1 + u) + (u + 1) log(2)   ```    When `self_normalized = False` the `(u + 1) log(2)` term is omitted.    Observe that as an f-Divergence, this Csiszar-function implies:    ```none   D_f[p, q] = KL[p, m] + KL[q, m]   m(x) = 0.5 p(x) + 0.5 q(x)   ```    In a sense, this divergence is the "reverse" of the Arithmetic-Geometric   f-Divergence.    This Csiszar-function induces a symmetric f-Divergence, i.e.,   `D_f[p, q] = D_f[q, p]`.    Warning: this function makes non-log-space calculations and may therefore be   numerically unstable for `|logu| >> 0`.    For more information, see:     Lin, J. "Divergence measures based on the Shannon entropy." IEEE Trans.     Inf. Th., 37, 145-151, 1991.    Args:     logu: `float`-like `Tensor` representing `log(u)` from above.     self_normalized: Python `bool` indicating whether `f'(u=1)=0`. When       `f'(u=1)=0` the implied Csiszar f-Divergence remains non-negative even       when `p, q` are unnormalized measures.     name: Python `str` name prefixed to Ops created by this function.    Returns:     jensen_shannon_of_u: `float`-like `Tensor` of the Csiszar-function       evaluated at `u = exp(logu)`.
The Pearson Csiszar-function in log-space.    A Csiszar-function is a member of,    ```none   F = { f:R_+ to R : f convex }.   ```    The Pearson Csiszar-function is:    ```none   f(u) = (u - 1)**2   ```    Warning: this function makes non-log-space calculations and may therefore be   numerically unstable for `|logu| >> 0`.    Args:     logu: `float`-like `Tensor` representing `log(u)` from above.     name: Python `str` name prefixed to Ops created by this function.    Returns:     pearson_of_u: `float`-like `Tensor` of the Csiszar-function evaluated at       `u = exp(logu)`.
The Squared-Hellinger Csiszar-function in log-space.    A Csiszar-function is a member of,    ```none   F = { f:R_+ to R : f convex }.   ```    The Squared-Hellinger Csiszar-function is:    ```none   f(u) = (sqrt(u) - 1)**2   ```    This Csiszar-function induces a symmetric f-Divergence, i.e.,   `D_f[p, q] = D_f[q, p]`.    Warning: this function makes non-log-space calculations and may therefore be   numerically unstable for `|logu| >> 0`.    Args:     logu: `float`-like `Tensor` representing `log(u)` from above.     name: Python `str` name prefixed to Ops created by this function.    Returns:     squared_hellinger_of_u: `float`-like `Tensor` of the Csiszar-function       evaluated at `u = exp(logu)`.
The Triangular Csiszar-function in log-space.    A Csiszar-function is a member of,    ```none   F = { f:R_+ to R : f convex }.   ```    The Triangular Csiszar-function is:    ```none   f(u) = (u - 1)**2 / (1 + u)   ```    This Csiszar-function induces a symmetric f-Divergence, i.e.,   `D_f[p, q] = D_f[q, p]`.    Warning: this function makes non-log-space calculations and may therefore be   numerically unstable for `|logu| >> 0`.    Args:     logu: `float`-like `Tensor` representing `log(u)` from above.     name: Python `str` name prefixed to Ops created by this function.    Returns:     triangular_of_u: `float`-like `Tensor` of the Csiszar-function evaluated       at `u = exp(logu)`.
The T-Power Csiszar-function in log-space.    A Csiszar-function is a member of,    ```none   F = { f:R_+ to R : f convex }.   ```    When `self_normalized = True` the T-Power Csiszar-function is:    ```none   f(u) = s [ u**t - 1 - t(u - 1) ]   s = { -1   0 < t < 1       { +1   otherwise   ```    When `self_normalized = False` the `- t(u - 1)` term is omitted.    This is similar to the `amari_alpha` Csiszar-function, with the associated   divergence being the same up to factors depending only on `t`.    Args:     logu: `float`-like `Tensor` representing `log(u)` from above.     t:  `Tensor` of same `dtype` as `logu` and broadcastable shape.     self_normalized: Python `bool` indicating whether `f'(u=1)=0`.     name: Python `str` name prefixed to Ops created by this function.    Returns:     t_power_of_u: `float`-like `Tensor` of the Csiszar-function evaluated       at `u = exp(logu)`.
The log1p-abs Csiszar-function in log-space.    A Csiszar-function is a member of,    ```none   F = { f:R_+ to R : f convex }.   ```    The Log1p-Abs Csiszar-function is:    ```none   f(u) = u**(sign(u-1)) - 1   ```    This function is so-named because it was invented from the following recipe.   Choose a convex function g such that g(0)=0 and solve for f:    ```none   log(1 + f(u)) = g(log(u)).     <=>   f(u) = exp(g(log(u))) - 1   ```    That is, the graph is identically `g` when y-axis is `log1p`-domain and x-axis   is `log`-domain.    Warning: this function makes non-log-space calculations and may therefore be   numerically unstable for `|logu| >> 0`.    Args:     logu: `float`-like `Tensor` representing `log(u)` from above.     name: Python `str` name prefixed to Ops created by this function.    Returns:     log1p_abs_of_u: `float`-like `Tensor` of the Csiszar-function evaluated       at `u = exp(logu)`.
The Jeffreys Csiszar-function in log-space.    A Csiszar-function is a member of,    ```none   F = { f:R_+ to R : f convex }.   ```    The Jeffreys Csiszar-function is:    ```none   f(u) = 0.5 ( u log(u) - log(u) )        = 0.5 kl_forward + 0.5 kl_reverse        = symmetrized_csiszar_function(kl_reverse)        = symmetrized_csiszar_function(kl_forward)   ```    This Csiszar-function induces a symmetric f-Divergence, i.e.,   `D_f[p, q] = D_f[q, p]`.    Warning: this function makes non-log-space calculations and may therefore be   numerically unstable for `|logu| >> 0`.    Args:     logu: `float`-like `Tensor` representing `log(u)` from above.     name: Python `str` name prefixed to Ops created by this function.    Returns:     jeffreys_of_u: `float`-like `Tensor` of the Csiszar-function evaluated       at `u = exp(logu)`.
The Modified-GAN Csiszar-function in log-space.    A Csiszar-function is a member of,    ```none   F = { f:R_+ to R : f convex }.   ```    When `self_normalized = True` the modified-GAN (Generative/Adversarial   Network) Csiszar-function is:    ```none   f(u) = log(1 + u) - log(u) + 0.5 (u - 1)   ```    When `self_normalized = False` the `0.5 (u - 1)` is omitted.    The unmodified GAN Csiszar-function is identical to Jensen-Shannon (with   `self_normalized = False`).    Warning: this function makes non-log-space calculations and may therefore be   numerically unstable for `|logu| >> 0`.    Args:     logu: `float`-like `Tensor` representing `log(u)` from above.     self_normalized: Python `bool` indicating whether `f'(u=1)=0`. When       `f'(u=1)=0` the implied Csiszar f-Divergence remains non-negative even       when `p, q` are unnormalized measures.     name: Python `str` name prefixed to Ops created by this function.    Returns:     chi_square_of_u: `float`-like `Tensor` of the Csiszar-function evaluated       at `u = exp(logu)`.
Calculates the dual Csiszar-function in log-space.    A Csiszar-function is a member of,    ```none   F = { f:R_+ to R : f convex }.   ```    The Csiszar-dual is defined as:    ```none   f^*(u) = u f(1 / u)   ```    where `f` is some other Csiszar-function.    For example, the dual of `kl_reverse` is `kl_forward`, i.e.,    ```none   f(u) = -log(u)   f^*(u) = u f(1 / u) = -u log(1 / u) = u log(u)   ```    The dual of the dual is the original function:    ```none   f^**(u) = {u f(1/u)}^*(u) = u (1/u) f(1/(1/u)) = f(u)   ```    Warning: this function makes non-log-space calculations and may therefore be   numerically unstable for `|logu| >> 0`.    Args:     logu: `float`-like `Tensor` representing `log(u)` from above.     csiszar_function: Python `callable` representing a Csiszar-function over       log-domain.     name: Python `str` name prefixed to Ops created by this function.    Returns:     dual_f_of_u: `float`-like `Tensor` of the result of calculating the dual of       `f` at `u = exp(logu)`.
Symmetrizes a Csiszar-function in log-space.    A Csiszar-function is a member of,    ```none   F = { f:R_+ to R : f convex }.   ```    The symmetrized Csiszar-function is defined as:    ```none   f_g(u) = 0.5 g(u) + 0.5 u g (1 / u)   ```    where `g` is some other Csiszar-function.    We say the function is "symmetrized" because:    ```none   D_{f_g}[p, q] = D_{f_g}[q, p]   ```    for all `p << >> q` (i.e., `support(p) = support(q)`).    There exists alternatives for symmetrizing a Csiszar-function. For example,    ```none   f_g(u) = max(f(u), f^*(u)),   ```    where `f^*` is the dual Csiszar-function, also implies a symmetric   f-Divergence.    Example:    When either of the following functions are symmetrized, we obtain the   Jensen-Shannon Csiszar-function, i.e.,    ```none   g(u) = -log(u) - (1 + u) log((1 + u) / 2) + u - 1   h(u) = log(4) + 2 u log(u / (1 + u))   ```    implies,    ```none   f_g(u) = f_h(u) = u log(u) - (1 + u) log((1 + u) / 2)          = jensen_shannon(log(u)).   ```    Warning: this function makes non-log-space calculations and may therefore be   numerically unstable for `|logu| >> 0`.    Args:     logu: `float`-like `Tensor` representing `log(u)` from above.     csiszar_function: Python `callable` representing a Csiszar-function over       log-domain.     name: Python `str` name prefixed to Ops created by this function.    Returns:     symmetrized_g_of_u: `float`-like `Tensor` of the result of applying the       symmetrization of `g` evaluated at `u = exp(logu)`.
Monte-Carlo approximation of the Csiszar f-Divergence.    A Csiszar-function is a member of,    ```none   F = { f:R_+ to R : f convex }.   ```    The Csiszar f-Divergence for Csiszar-function f is given by:    ```none   D_f[p(X), q(X)] := E_{q(X)}[ f( p(X) / q(X) ) ]                   ~= m**-1 sum_j^m f( p(x_j) / q(x_j) ),                              where x_j ~iid q(X)   ```    Tricks: Reparameterization and Score-Gradient    When q is "reparameterized", i.e., a diffeomorphic transformation of a   parameterless distribution (e.g.,   `Normal(Y; m, s) <=> Y = sX + m, X ~ Normal(0,1)`), we can swap gradient and   expectation, i.e.,   `grad[Avg{ s_i : i=1...n }] = Avg{ grad[s_i] : i=1...n }` where `S_n=Avg{s_i}`   and `s_i = f(x_i), x_i ~iid q(X)`.    However, if q is not reparameterized, TensorFlow's gradient will be incorrect   since the chain-rule stops at samples of unreparameterized distributions. In   this circumstance using the Score-Gradient trick results in an unbiased   gradient, i.e.,    ```none   grad[ E_q[f(X)] ]   = grad[ int dx q(x) f(x) ]   = int dx grad[ q(x) f(x) ]   = int dx [ q'(x) f(x) + q(x) f'(x) ]   = int dx q(x) [q'(x) / q(x) f(x) + f'(x) ]   = int dx q(x) grad[ f(x) q(x) / stop_grad[q(x)] ]   = E_q[ grad[ f(x) q(x) / stop_grad[q(x)] ] ]   ```    Unless `q.reparameterization_type != tfd.FULLY_REPARAMETERIZED` it is   usually preferable to set `use_reparametrization = True`.    Example Application:    The Csiszar f-Divergence is a useful framework for variational inference.   I.e., observe that,    ```none   f(p(x)) =  f( E_{q(Z | x)}[ p(x, Z) / q(Z | x) ] )           <= E_{q(Z | x)}[ f( p(x, Z) / q(Z | x) ) ]           := D_f[p(x, Z), q(Z | x)]   ```    The inequality follows from the fact that the "perspective" of `f`, i.e.,   `(s, t) |-> t f(s / t))`, is convex in `(s, t)` when `s/t in domain(f)` and   `t` is a real. Since the above framework includes the popular Evidence Lower   BOund (ELBO) as a special case, i.e., `f(u) = -log(u)`, we call this framework   "Evidence Divergence Bound Optimization" (EDBO).    Args:     f: Python `callable` representing a Csiszar-function in log-space, i.e.,       takes `p_log_prob(q_samples) - q.log_prob(q_samples)`.     p_log_prob: Python `callable` taking (a batch of) samples from `q` and       returning the natural-log of the probability under distribution `p`.       (In variational inference `p` is the joint distribution.)     q: `tf.Distribution`-like instance; must implement:       `reparameterization_type`, `sample(n, seed)`, and `log_prob(x)`.       (In variational inference `q` is the approximate posterior distribution.)     num_draws: Integer scalar number of draws used to approximate the       f-Divergence expectation.     use_reparametrization: Python `bool`. When `None` (the default),       automatically set to:       `q.reparameterization_type == tfd.FULLY_REPARAMETERIZED`.       When `True` uses the standard Monte-Carlo average. When `False` uses the       score-gradient trick. (See above for details.)  When `False`, consider       using `csiszar_vimco`.     seed: Python `int` seed for `q.sample`.     name: Python `str` name prefixed to Ops created by this function.    Returns:     monte_carlo_csiszar_f_divergence: `float`-like `Tensor` Monte Carlo       approximation of the Csiszar f-Divergence.    Raises:     ValueError: if `q` is not a reparameterized distribution and       `use_reparametrization = True`. A distribution `q` is said to be       "reparameterized" when its samples are generated by transforming the       samples of another distribution which does not depend on the       parameterization of `q`. This property ensures the gradient (with respect       to parameters) is valid.     TypeError: if `p_log_prob` is not a Python `callable`.
Helper to `csiszar_vimco`; computes `log_avg_u`, `log_sooavg_u`.    `axis = 0` of `logu` is presumed to correspond to iid samples from `q`, i.e.,    ```none   logu[j] = log(u[j])   u[j] = p(x, h[j]) / q(h[j] | x)   h[j] iid~ q(H | x)   ```    Args:     logu: Floating-type `Tensor` representing `log(p(x, h) / q(h | x))`.     name: Python `str` name prefixed to Ops created by this function.    Returns:     log_avg_u: `logu.dtype` `Tensor` corresponding to the natural-log of the       average of `u`. The sum of the gradient of `log_avg_u` is `1`.     log_sooavg_u: `logu.dtype` `Tensor` characterized by the natural-log of the       average of `u`` except that the average swaps-out `u[i]` for the       leave-`i`-out Geometric-average. The mean of the gradient of       `log_sooavg_u` is `1`. Mathematically `log_sooavg_u` is,       ```none       log_sooavg_u[i] = log(Avg{h[j ; i] : j=0, ..., m-1})       h[j ; i] = { u[j]                              j!=i                  { GeometricAverage{u[k] : k != i}   j==i       ```
Assert that Tensor x has expected number of dimensions.
Like batch_gather, but broadcasts to the left of axis.
Broadcasts the event or distribution parameters.
r"""Importance sampling with a positive function, in log-space.    With \\(p(z) := exp^{log_p(z)}\\), and \\(f(z) = exp{log_f(z)}\\),   this `Op` returns    \\(Log[ n^{-1} sum_{i=1}^n [ f(z_i) p(z_i) / q(z_i) ] ],  z_i ~ q,\\)   \\(\approx Log[ E_q[ f(Z) p(Z) / q(Z) ] ]\\)   \\(=       Log[E_p[f(Z)]]\\)    This integral is done in log-space with max-subtraction to better handle the   often extreme values that `f(z) p(z) / q(z)` can take on.    In contrast to `expectation_importance_sampler`, this `Op` returns values in   log-space.     User supplies either `Tensor` of samples `z`, or number of samples to draw `n`    Args:     log_f: Callable mapping samples from `sampling_dist_q` to `Tensors` with       shape broadcastable to `q.batch_shape`.       For example, `log_f` works "just like" `sampling_dist_q.log_prob`.     log_p:  Callable mapping samples from `sampling_dist_q` to `Tensors` with       shape broadcastable to `q.batch_shape`.       For example, `log_p` works "just like" `q.log_prob`.     sampling_dist_q:  The sampling distribution.       `tfp.distributions.Distribution`.       `float64` `dtype` recommended.       `log_p` and `q` should be supported on the same set.     z:  `Tensor` of samples from `q`, produced by `q.sample` for some `n`.     n:  Integer `Tensor`.  Number of samples to generate if `z` is not provided.     seed:  Python integer to seed the random number generator.     name:  A name to give this `Op`.    Returns:     Logarithm of the importance sampling estimate.  `Tensor` with `shape` equal       to batch shape of `q`, and `dtype` = `q.dtype`.
Broadcasts the event or samples.
Applies the BFGS algorithm to minimize a differentiable function.    Performs unconstrained minimization of a differentiable function using the   BFGS scheme. For details of the algorithm, see [Nocedal and Wright(2006)][1].    ### Usage:    The following example demonstrates the BFGS optimizer attempting to find the   minimum for a simple two dimensional quadratic objective function.    ```python     minimum = np.array([1.0, 1.0])  # The center of the quadratic bowl.     scales = np.array([2.0, 3.0])  # The scales along the two axes.      # The objective function and the gradient.     def quadratic(x):       value = tf.reduce_sum(scales * (x - minimum) ** 2)       return value, tf.gradients(value, x)[0]      start = tf.constant([0.6, 0.8])  # Starting point for the search.     optim_results = tfp.optimizer.bfgs_minimize(         quadratic, initial_position=start, tolerance=1e-8)      with tf.Session() as session:       results = session.run(optim_results)       # Check that the search converged       assert(results.converged)       # Check that the argmin is close to the actual value.       np.testing.assert_allclose(results.position, minimum)       # Print out the total number of function evaluations it took. Should be 6.       print ("Function evaluations: %d" % results.num_objective_evaluations)   ```    ### References:   [1]: Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series in     Operations Research. pp 136-140. 2006     http://pages.mtu.edu/~struther/Courses/OLD/Sp2013/5630/Jorge_Nocedal_Numerical_optimization_267490.pdf    Args:     value_and_gradients_function:  A Python callable that accepts a point as a       real `Tensor` and returns a tuple of `Tensor`s of real dtype containing       the value of the function and its gradient at that point. The function       to be minimized. The input should be of shape `[..., n]`, where `n` is       the size of the domain of input points, and all others are batching       dimensions. The first component of the return value should be a real       `Tensor` of matching shape `[...]`. The second component (the gradient)       should also be of shape `[..., n]` like the input value to the function.     initial_position: real `Tensor` of shape `[..., n]`. The starting point, or       points when using batching dimensions, of the search procedure. At these       points the function value and the gradient norm should be finite.     tolerance: Scalar `Tensor` of real dtype. Specifies the gradient tolerance       for the procedure. If the supremum norm of the gradient vector is below       this number, the algorithm is stopped.     x_tolerance: Scalar `Tensor` of real dtype. If the absolute change in the       position between one iteration and the next is smaller than this number,       the algorithm is stopped.     f_relative_tolerance: Scalar `Tensor` of real dtype. If the relative change       in the objective value between one iteration and the next is smaller       than this value, the algorithm is stopped.     initial_inverse_hessian_estimate: Optional `Tensor` of the same dtype       as the components of the output of the `value_and_gradients_function`.       If specified, the shape should broadcastable to shape `[..., n, n]`; e.g.       if a single `[n, n]` matrix is provided, it will be automatically       broadcasted to all batches. Alternatively, one can also specify a       different hessian estimate for each batch member.       For the correctness of the algorithm, it is required that this parameter       be symmetric and positive definite. Specifies the starting estimate for       the inverse of the Hessian at the initial point. If not specified,       the identity matrix is used as the starting estimate for the       inverse Hessian.     max_iterations: Scalar positive int32 `Tensor`. The maximum number of       iterations for BFGS updates.     parallel_iterations: Positive integer. The number of iterations allowed to       run in parallel.     stopping_condition: (Optional) A Python function that takes as input two       Boolean tensors of shape `[...]`, and returns a Boolean scalar tensor.       The input tensors are `converged` and `failed`, indicating the current       status of each respective batch member; the return value states whether       the algorithm should stop. The default is tfp.optimizer.converged_all       which only stops when all batch members have either converged or failed.       An alternative is tfp.optimizer.converged_any which stops as soon as one       batch member has converged, or when all have failed.     name: (Optional) Python str. The name prefixed to the ops created by this       function. If not supplied, the default name 'minimize' is used.    Returns:     optimizer_results: A namedtuple containing the following items:       converged: boolean tensor of shape `[...]` indicating for each batch         member whether the minimum was found within tolerance.       failed:  boolean tensor of shape `[...]` indicating for each batch         member whether a line search step failed to find a suitable step size         satisfying Wolfe conditions. In the absence of any constraints on the         number of objective evaluations permitted, this value will         be the complement of `converged`. However, if there is         a constraint and the search stopped due to available         evaluations being exhausted, both `failed` and `converged`         will be simultaneously False.       num_objective_evaluations: The total number of objective         evaluations performed.       position: A tensor of shape `[..., n]` containing the last argument value         found during the search from each starting point. If the search         converged, then this value is the argmin of the objective function.       objective_value: A tensor of shape `[...]` with the value of the         objective function at the `position`. If the search converged, then         this is the (local) minimum of the objective function.       objective_gradient: A tensor of shape `[..., n]` containing the gradient         of the objective function at the `position`. If the search converged         the max-norm of this tensor should be below the tolerance.       inverse_hessian_estimate: A tensor of shape `[..., n, n]` containing the         inverse of the estimated Hessian.
Computes control inputs to validate a provided inverse Hessian.    These ensure that the provided inverse Hessian is positive definite and   symmetric.    Args:     inv_hessian: The starting estimate for the inverse of the Hessian at the       initial point.    Returns:     A list of tf.Assert ops suitable for use with tf.control_dependencies.
Update the BGFS state by computing the next inverse hessian estimate.
Applies the BFGS update to the inverse Hessian estimate.    The BFGS update rule is (note A^T denotes the transpose of a vector/matrix A).    ```None     rho = 1/(grad_delta^T * position_delta)     U = (I - rho * position_delta * grad_delta^T)     H_1 =  U * H_0 * U^T + rho * position_delta * position_delta^T   ```    Here, `H_0` is the inverse Hessian estimate at the previous iteration and   `H_1` is the next estimate. Note that `*` should be interpreted as the   matrix multiplication (with the understanding that matrix multiplication for   scalars is usual multiplication and for matrix with vector is the action of   the matrix on the vector.).    The implementation below utilizes an expanded version of the above formula   to avoid the matrix multiplications that would be needed otherwise. By   expansion it is easy to see that one only needs matrix-vector or   vector-vector operations. The expanded version is:    ```None     f = 1 + rho * (grad_delta^T * H_0 * grad_delta)     H_1 - H_0 = - rho * [position_delta * (H_0 * grad_delta)^T +                         (H_0 * grad_delta) * position_delta^T] +                   rho * f * [position_delta * position_delta^T]   ```    All the terms in square brackets are matrices and are constructed using   vector outer products. All the other terms on the right hand side are scalars.   Also worth noting that the first and second lines are both rank 1 updates   applied to the current inverse Hessian estimate.    Args:     grad_delta: Real `Tensor` of shape `[..., n]`. The difference between the       gradient at the new position and the old position.     position_delta: Real `Tensor` of shape `[..., n]`. The change in position       from the previous iteration to the current one.     normalization_factor: Real `Tensor` of shape `[...]`. Should be equal to       `grad_delta^T * position_delta`, i.e. `1/rho` as defined above.     inv_hessian_estimate: Real `Tensor` of shape `[..., n, n]`. The previous       estimate of the inverse Hessian. Should be positive definite and       symmetric.    Returns:     A tuple containing the following fields       is_valid: A Boolean `Tensor` of shape `[...]` indicating batch members         where the update succeeded. The update can fail if the position change         becomes orthogonal to the gradient change.       next_inv_hessian_estimate: A `Tensor` of shape `[..., n, n]`. The next         Hessian estimate updated using the BFGS update scheme. If the         `inv_hessian_estimate` is symmetric and positive definite, the         `next_inv_hessian_estimate` is guaranteed to satisfy the same         conditions.
Computes the product of a matrix with a vector on the right.    Note this supports dynamic shapes and batched computation.    Examples:      M = tf.reshape(tf.range(6), shape=(3, 2))     # => [[0, 1],     #     [2, 3],     #     [4, 5]]     v = tf.constant([1, 2])  # Shape: (2,)     _mul_right(M, v)     # => [ 2,  8, 14]  # Shape: (3,)      M = tf.reshape(tf.range(30), shape=(2, 3, 5))     # => [[[ 0,  1,  2,  3,  4],     #     [ 5,  6,  7,  8,  9],     #     [10, 11, 12, 13, 14]],     #     #    [[15, 16, 17, 18, 19],     #     [20, 21, 22, 23, 24],     #     [25, 26, 27, 28, 29]]]     v = tf.reshape(tf.range(10), shape=(2, 5))     # => [[0, 1, 2, 3, 4],     #     [5, 6, 7, 8, 9]]     _mul_right(M, v)     # => [[ 30,  80, 130],     #     [605, 780, 955]]  # Shape: (2, 3)    Args:     mat: A `tf.Tensor` of shape `[..., n, m]`.     vec: A `tf.Tensor` of shape `[..., m]`.    Returns:     A tensor of shape `[..., n]` with matching batch dimensions.
Computes the outer product of two possibly batched vectors.    Args:     t1: A `tf.Tensor` of shape `[..., n]`.     t2: A `tf.Tensor` of shape `[..., m]`.    Returns:     A tensor of shape `[..., n, m]` with matching batch dimensions, let's call     it `r`, whose components are:      ```None       r[..., i, j] = t1[..., i] * t2[..., j]     ```
Transpose a possibly batched matrix.    Args:     mat: A `tf.Tensor` of shape `[..., n, m]`.    Returns:     A tensor of shape `[..., m, n]` with matching batch dimensions.
Maybe add `ndims` ones to `x.shape` on the right.    If `ndims` is zero, this is a no-op; otherwise, we will create and return a   new `Tensor` whose shape is that of `x` with `ndims` ones concatenated on the   right side. If the shape of `x` is known statically, the shape of the return   value will be as well.    Args:     x: The `Tensor` we'll return a reshaping of.     ndims: Python `integer` number of ones to pad onto `x.shape`.   Returns:     If `ndims` is zero, `x`; otherwise, a `Tensor` whose shape is that of `x`     with `ndims` ones concatenated on the right side. If possible, returns a     `Tensor` whose shape is known statically.   Raises:     ValueError: if `ndims` is not a Python `integer` greater than or equal to     zero.
Return `Tensor` with right-most ndims summed.    Args:     x: the `Tensor` whose right-most `ndims` dimensions to sum     ndims: number of right-most dimensions to sum.    Returns:     A `Tensor` resulting from calling `reduce_sum` on the `ndims` right-most     dimensions. If the shape of `x` is statically known, the result will also     have statically known shape. Otherwise, the resulting shape will only be     known at runtime.
A sqrt function whose gradient at zero is very large but finite.    Args:     x: a `Tensor` whose sqrt is to be computed.     name: a Python `str` prefixed to all ops created by this function.       Default `None` (i.e., "sqrt_with_finite_grads").    Returns:     sqrt: the square root of `x`, with an overridden gradient at zero     grad: a gradient function, which is the same as sqrt's gradient everywhere       except at zero, where it is given a large finite value, instead of `inf`.    Raises:     TypeError: if `tf.convert_to_tensor(x)` is not a `float` type.    Often in kernel functions, we need to compute the L2 norm of the difference   between two vectors, `x` and `y`: `sqrt(sum_i((x_i - y_i) ** 2))`. In the   case where `x` and `y` are identical, e.g., on the diagonal of a kernel   matrix, we get `NaN`s when we take gradients with respect to the inputs. To   see, this consider the forward pass:      ```     [x_1 ... x_N]  -->  [x_1 ** 2 ... x_N ** 2]  -->         (x_1 ** 2 + ... + x_N ** 2)  -->  sqrt((x_1 ** 2 + ... + x_N ** 2))     ```    When we backprop through this forward pass, the `sqrt` yields an `inf` because   `grad_z(sqrt(z)) = 1 / (2 * sqrt(z))`. Continuing the backprop to the left, at   the `x ** 2` term, we pick up a `2 * x`, and when `x` is zero, we get   `0 * inf`, which is `NaN`.    We'd like to avoid these `NaN`s, since they infect the rest of the connected   computation graph. Practically, when two inputs to a kernel function are   equal, we are in one of two scenarios:     1. We are actually computing k(x, x), in which case norm(x - x) is        identically zero, independent of x. In this case, we'd like the        gradient to reflect this independence: it should be zero.     2. We are computing k(x, y), and x just *happens* to have the same value        as y. The gradient at such inputs is in fact ill-defined (there is a        cusp in the sqrt((x - y) ** 2) surface along the line x = y). There are,        however, an infinite number of sub-gradients, all of which are valid at        all such inputs. By symmetry, there is exactly one which is "special":        zero, and we elect to use that value here. In practice, having two        identical inputs to a kernel matrix is probably a pathological        situation to be avoided, but that is better resolved at a higher level        than this.    To avoid the infinite gradient at zero, we use tf.custom_gradient to redefine   the gradient at zero. We assign it to be a very large value, specifically   the sqrt of the max value of the floating point dtype of the input. We use   the sqrt (as opposed to just using the max floating point value) to avoid   potential overflow when combining this value with others downstream.
Return common dtype of arg_list, or None.    Args:     arg_list: an iterable of items which are either `None` or have a `dtype`       property.    Returns:     dtype: The common dtype of items in `arg_list`, or `None` if the list is       empty or all items are `None`.
Applies the L-BFGS algorithm to minimize a differentiable function.    Performs unconstrained minimization of a differentiable function using the   L-BFGS scheme. See [Nocedal and Wright(2006)][1] for details of the algorithm.    ### Usage:    The following example demonstrates the L-BFGS optimizer attempting to find the   minimum for a simple high-dimensional quadratic objective function.    ```python     # A high-dimensional quadratic bowl.     ndims = 60     minimum = np.ones([ndims], dtype='float64')     scales = np.arange(ndims, dtype='float64') + 1.0      # The objective function and the gradient.     def quadratic(x):       value = tf.reduce_sum(scales * (x - minimum) ** 2)       return value, tf.gradients(value, x)[0]      start = np.arange(ndims, 0, -1, dtype='float64')     optim_results = tfp.optimizer.lbfgs_minimize(         quadratic, initial_position=start, num_correction_pairs=10,         tolerance=1e-8)      with tf.Session() as session:       results = session.run(optim_results)       # Check that the search converged       assert(results.converged)       # Check that the argmin is close to the actual value.       np.testing.assert_allclose(results.position, minimum)   ```    ### References:    [1] Jorge Nocedal, Stephen Wright. Numerical Optimization. Springer Series       in Operations Research. pp 176-180. 2006    http://pages.mtu.edu/~struther/Courses/OLD/Sp2013/5630/Jorge_Nocedal_Numerical_optimization_267490.pdf    Args:     value_and_gradients_function:  A Python callable that accepts a point as a       real `Tensor` and returns a tuple of `Tensor`s of real dtype containing       the value of the function and its gradient at that point. The function       to be minimized. The input is of shape `[..., n]`, where `n` is the size       of the domain of input points, and all others are batching dimensions.       The first component of the return value is a real `Tensor` of matching       shape `[...]`. The second component (the gradient) is also of shape       `[..., n]` like the input value to the function.     initial_position: Real `Tensor` of shape `[..., n]`. The starting point, or       points when using batching dimensions, of the search procedure. At these       points the function value and the gradient norm should be finite.     num_correction_pairs: Positive integer. Specifies the maximum number of       (position_delta, gradient_delta) correction pairs to keep as implicit       approximation of the Hessian matrix.     tolerance: Scalar `Tensor` of real dtype. Specifies the gradient tolerance       for the procedure. If the supremum norm of the gradient vector is below       this number, the algorithm is stopped.     x_tolerance: Scalar `Tensor` of real dtype. If the absolute change in the       position between one iteration and the next is smaller than this number,       the algorithm is stopped.     f_relative_tolerance: Scalar `Tensor` of real dtype. If the relative change       in the objective value between one iteration and the next is smaller       than this value, the algorithm is stopped.     initial_inverse_hessian_estimate: None. Option currently not supported.     max_iterations: Scalar positive int32 `Tensor`. The maximum number of       iterations for L-BFGS updates.     parallel_iterations: Positive integer. The number of iterations allowed to       run in parallel.     stopping_condition: (Optional) A Python function that takes as input two       Boolean tensors of shape `[...]`, and returns a Boolean scalar tensor.       The input tensors are `converged` and `failed`, indicating the current       status of each respective batch member; the return value states whether       the algorithm should stop. The default is tfp.optimizer.converged_all       which only stops when all batch members have either converged or failed.       An alternative is tfp.optimizer.converged_any which stops as soon as one       batch member has converged, or when all have failed.     name: (Optional) Python str. The name prefixed to the ops created by this       function. If not supplied, the default name 'minimize' is used.    Returns:     optimizer_results: A namedtuple containing the following items:       converged: Scalar boolean tensor indicating whether the minimum was         found within tolerance.       failed:  Scalar boolean tensor indicating whether a line search         step failed to find a suitable step size satisfying Wolfe         conditions. In the absence of any constraints on the         number of objective evaluations permitted, this value will         be the complement of `converged`. However, if there is         a constraint and the search stopped due to available         evaluations being exhausted, both `failed` and `converged`         will be simultaneously False.       num_objective_evaluations: The total number of objective         evaluations performed.       position: A tensor containing the last argument value found         during the search. If the search converged, then         this value is the argmin of the objective function.       objective_value: A tensor containing the value of the objective         function at the `position`. If the search converged, then this is         the (local) minimum of the objective function.       objective_gradient: A tensor containing the gradient of the objective         function at the `position`. If the search converged the         max-norm of this tensor should be below the tolerance.       position_deltas: A tensor encoding information about the latest         changes in `position` during the algorithm execution.       gradient_deltas: A tensor encoding information about the latest         changes in `objective_gradient` during the algorithm execution.
Create LBfgsOptimizerResults with initial state of search procedure.
Computes the search direction to follow at the current state.    On the `k`-th iteration of the main L-BFGS algorithm, the state has collected   the most recent `m` correction pairs in position_deltas and gradient_deltas,   where `k = state.num_iterations` and `m = min(k, num_correction_pairs)`.    Assuming these, the code below is an implementation of the L-BFGS two-loop   recursion algorithm given by [Nocedal and Wright(2006)][1]:    ```None     q_direction = objective_gradient     for i in reversed(range(m)):  # First loop.       inv_rho[i] = gradient_deltas[i]^T * position_deltas[i]       alpha[i] = position_deltas[i]^T * q_direction / inv_rho[i]       q_direction = q_direction - alpha[i] * gradient_deltas[i]      kth_inv_hessian_factor = (gradient_deltas[-1]^T * position_deltas[-1] /                               gradient_deltas[-1]^T * gradient_deltas[-1])     r_direction = kth_inv_hessian_factor * I * q_direction      for i in range(m):  # Second loop.       beta = gradient_deltas[i]^T * r_direction / inv_rho[i]       r_direction = r_direction + position_deltas[i] * (alpha[i] - beta)      return -r_direction  # Approximates - H_k * objective_gradient.   ```    Args:     state: A `LBfgsOptimizerResults` tuple with the current state of the       search procedure.    Returns:     A real `Tensor` of the same shape as the `state.position`. The direction     along which to perform line search.
Creates a `tf.Tensor` suitable to hold `k` element-shaped tensors.    For example:    ```python     element = tf.constant([[0., 1., 2., 3., 4.],                            [5., 6., 7., 8., 9.]])      # A queue capable of holding 3 elements.     _make_empty_queue_for(3, element)     # => [[[ 0.,  0.,  0.,  0.,  0.],     #      [ 0.,  0.,  0.,  0.,  0.]],     #     #     [[ 0.,  0.,  0.,  0.,  0.],     #      [ 0.,  0.,  0.,  0.,  0.]],     #     #     [[ 0.,  0.,  0.,  0.,  0.],     #      [ 0.,  0.,  0.,  0.,  0.]]]   ```    Args:     k: A positive scalar integer, number of elements that each queue will hold.     element: A `tf.Tensor`, only its shape and dtype information are relevant.    Returns:     A zero-filed `tf.Tensor` of shape `(k,) + tf.shape(element)` and same dtype     as `element`.
Conditionally push new vectors into a batch of first-in-first-out queues.    The `queue` of shape `[k, ..., n]` can be thought of as a batch of queues,   each holding `k` n-D vectors; while `new_vecs` of shape `[..., n]` is a   fresh new batch of n-D vectors. The `should_update` batch of Boolean scalars,   i.e. shape `[...]`, indicates batch members whose corresponding n-D vector in   `new_vecs` should be added at the back of its queue, pushing out the   corresponding n-D vector from the front. Batch members in `new_vecs` for   which `should_update` is False are ignored.    Note: the choice of placing `k` at the dimension 0 of the queue is   constrained by the L-BFGS two-loop algorithm above. The algorithm uses   tf.scan to iterate over the `k` correction pairs simulatneously across all   batches, and tf.scan itself can only iterate over dimension 0.    For example:    ```python     k, b, n = (3, 2, 5)     queue = tf.reshape(tf.range(30), (k, b, n))     # => [[[ 0,  1,  2,  3,  4],     #      [ 5,  6,  7,  8,  9]],     #     #     [[10, 11, 12, 13, 14],     #      [15, 16, 17, 18, 19]],     #     #     [[20, 21, 22, 23, 24],     #      [25, 26, 27, 28, 29]]]      element = tf.reshape(tf.range(30, 40), (b, n))     # => [[30, 31, 32, 33, 34],           [35, 36, 37, 38, 39]]      should_update = tf.constant([True, False])  # Shape: (b,)      _queue_add(should_update, queue, element)     # => [[[10, 11, 12, 13, 14],     #      [ 5,  6,  7,  8,  9]],     #     #     [[20, 21, 22, 23, 24],     #      [15, 16, 17, 18, 19]],     #     #     [[30, 31, 32, 33, 34],     #      [25, 26, 27, 28, 29]]]   ```    Args:     queue: A `tf.Tensor` of shape `[k, ..., n]`; a batch of queues each with       `k` n-D vectors.     should_update: A Boolean `tf.Tensor` of shape `[...]` indicating batch       members where new vectors should be added to their queues.     new_vecs: A `tf.Tensor` of shape `[..., n]`; a batch of n-D vectors to add       at the end of their respective queues, pushing out the first element from       each.    Returns:     A new `tf.Tensor` of shape `[k, ..., n]`.
Computes whether each square matrix in the input is positive semi-definite.    Args:     x: A floating-point `Tensor` of shape `[B1, ..., Bn, M, M]`.    Returns:     mask: A floating-point `Tensor` of shape `[B1, ... Bn]`.  Each       scalar is 1 if the corresponding matrix was PSD, otherwise 0.
Returns whether the input matches the given determinant limit.    Args:     x: A floating-point `Tensor` of shape `[B1, ..., Bn, M, M]`.     det_bounds: A floating-point `Tensor` that must broadcast to shape       `[B1, ..., Bn]`, giving the desired lower bound on the       determinants in `x`.    Returns:     mask: A floating-point `Tensor` of shape [B1, ..., Bn].  Each       scalar is 1 if the corresponding matrix had determinant above       the corresponding bound, otherwise 0.
Returns a uniformly random `Tensor` of "correlation-like" matrices.    A "correlation-like" matrix is a symmetric square matrix with all entries   between -1 and 1 (inclusive) and 1s on the main diagonal.  Of these,   the ones that are positive semi-definite are exactly the correlation   matrices.    Args:     num_rows: Python `int` dimension of the correlation-like matrices.     batch_shape: `Tensor` or Python `tuple` of `int` shape of the       batch to return.     dtype: `dtype` of the `Tensor` to return.     seed: Random seed.    Returns:     matrices: A `Tensor` of shape `batch_shape + [num_rows, num_rows]`       and dtype `dtype`.  Each entry is in [-1, 1], and each matrix       along the bottom two dimensions is symmetric and has 1s on the       main diagonal.
Returns rejection samples from trying to get good correlation matrices.    The proposal being rejected from is the uniform distribution on   "correlation-like" matrices.  We say a matrix is "correlation-like"   if it is a symmetric square matrix with all entries between -1 and 1   (inclusive) and 1s on the main diagonal.  Of these, the ones that   are positive semi-definite are exactly the correlation matrices.    The rejection algorithm, then, is to sample a `Tensor` of   `sample_shape` correlation-like matrices of dimensions `dim` by   `dim`, and check each one for (i) being a correlation matrix (i.e.,   PSD), and (ii) having determinant at least the corresponding entry   of `det_bounds`.    Args:     det_bounds: A `Tensor` of lower bounds on the determinants of       acceptable matrices.  The shape must broadcast with `sample_shape`.     dim: A Python `int` dimension of correlation matrices to sample.     sample_shape: Python `tuple` of `int` shape of the samples to       compute, excluding the two matrix dimensions.     dtype: The `dtype` in which to do the computation.     seed: Random seed.    Returns:     weights: A `Tensor` of shape `sample_shape`.  Each entry is 0 if the       corresponding matrix was not a correlation matrix, or had too       small of a determinant.  Otherwise, the entry is the       multiplicative inverse of the density of proposing that matrix       uniformly, i.e., the volume of the set of `dim` by `dim`       correlation-like matrices.     volume: The volume of the set of `dim` by `dim` correlation-like       matrices.
Computes a confidence interval for the mean of the given 1-D distribution.    Assumes (and checks) that the given distribution is Bernoulli, i.e.,   takes only two values.  This licenses using the CDF of the binomial   distribution for the confidence, which is tighter (for extreme   probabilities) than the DKWM inequality.  The method is known as the   [Clopper-Pearson method]   (https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval).    Assumes:    - The given samples were drawn iid from the distribution of interest.    - The given distribution is a Bernoulli, i.e., supported only on     low and high.    Guarantees:    - The probability (over the randomness of drawing the given sample)     that the true mean is outside the returned interval is no more     than the given error_rate.    Args:     samples: `np.ndarray` of samples drawn iid from the distribution       of interest.     error_rate: Python `float` admissible rate of mistakes.    Returns:     low: Lower bound of confidence interval.     high: Upper bound of confidence interval.    Raises:     ValueError: If `samples` has rank other than 1 (batch semantics       are not implemented), or if `samples` contains values other than       `low` or `high` (as that makes the distribution not Bernoulli).
Returns confidence intervals for the desired correlation matrix volumes.    The confidence intervals are computed by the [Clopper-Pearson method]   (https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval).    Args:     det_bounds: A rank-1 numpy array of lower bounds on the       determinants of acceptable matrices.  Entries must be unique.     dim: A Python `int` dimension of correlation matrices to sample.     num_samples: The number of samples to draw.     error_rate: The statistical significance of the returned       confidence intervals.  The significance is broadcast: Each       returned interval separately may be incorrect with probability       (under the sample of correlation-like matrices drawn internally)       at most `error_rate`.     seed: Random seed.    Returns:     bounds: A Python `dict` mapping each determinant bound to the low, high       tuple giving the confidence interval.
Computes the von Mises CDF and its derivative via series expansion.
Computes the von Mises CDF and its derivative via Normal approximation.
Performs one step of the differential evolution algorithm.    Args:     objective_function:  A Python callable that accepts a batch of possible       solutions and returns the values of the objective function at those       arguments as a rank 1 real `Tensor`. This specifies the function to be       minimized. The input to this callable may be either a single `Tensor`       or a Python `list` of `Tensor`s. The signature must match the format of       the argument `population`. (i.e. objective_function(*population) must       return the value of the function to be minimized).     population:  `Tensor` or Python `list` of `Tensor`s representing the       current population vectors. Each `Tensor` must be of the same real dtype.       The first dimension indexes individual population members while the       rest of the dimensions are consumed by the value function. For example,       if the population is a single `Tensor` of shape [n, m1, m2], then `n` is       the population size and the output of `objective_function` applied to the       population is a `Tensor` of shape [n]. If the population is a python       list of `Tensor`s then each `Tensor` in the list should have the first       axis of a common size, say `n` and `objective_function(*population)`       should return a `Tensor of shape [n]. The population must have at least       4 members for the algorithm to work correctly.     population_values: A `Tensor` of rank 1 and real dtype. The result of       applying `objective_function` to the `population`. If not supplied it is       computed using the `objective_function`.       Default value: None.     differential_weight: Real scalar `Tensor`. Must be positive and less than       2.0. The parameter controlling the strength of mutation.       Default value: 0.5     crossover_prob: Real scalar `Tensor`. Must be between 0 and 1. The       probability of recombination per site.       Default value: 0.9     seed: `int` or None. The random seed for this `Op`. If `None`, no seed is       applied.       Default value: None.     name: (Optional) Python str. The name prefixed to the ops created by this       function. If not supplied, the default name 'one_step' is       used.       Default value: None    Returns:     A sequence containing the following elements (in order):     next_population: A `Tensor` or Python `list` of `Tensor`s of the same       structure as the input population. The population at the next generation.     next_population_values: A `Tensor` of same shape and dtype as input       `population_values`. The function values for the `next_population`.
Applies the Differential evolution algorithm to minimize a function.    Differential Evolution is an evolutionary optimization algorithm which works   on a set of candidate solutions called the population. It iteratively   improves the population by applying genetic operators of mutation and   recombination. The objective function `f` supplies the fitness of each   candidate. A candidate `s_1` is considered better than `s_2` if   `f(s_1) < f(s_2)`.    This method allows the user to either specify an initial population or a   single candidate solution. If a single solution is specified, a population   of the specified size is initialized by adding independent normal noise   to the candidate solution.    The implementation also supports a multi-part specification of the state. For   example, consider the objective function:    ```python   # x is a tensor of shape [n, m] while y is of shape [n].   def objective(x, y):     return tf.math.reduce_sum(x ** 2, axis=-1) + y ** 2   ```   The state in this case is specified by two input tensors `x` and `y`. To   apply the algorithm to this objective function, one would need to specify   either an initial population as a list of two tensors of shapes   `[population_size, k]` and `[population_size]`. The following code shows the   complete example:    ```python     population_size = 40     # With an initial population and a multi-part state.     initial_population = (tf.random.normal([population_size]),                           tf.random.normal([population_size]))     def easom_fn(x, y):       return -(tf.math.cos(x) * tf.math.cos(y) *                tf.math.exp(-(x-np.pi)**2 - (y-np.pi)**2))      optim_results = tfp.optimizers.differential_evolution_minimize(         easom_fn,         initial_population=initial_population,         seed=43210)      print (optim_results.converged)     print (optim_results.position)  # Should be (close to) [pi, pi].     print (optim_results.objective_value)    # Should be -1.       # With a single starting point     initial_position = (tf.constant(1.0), tf.constant(1.0))      optim_results = tfp.optimizers.differential_evolution_minimize(         easom_fn,         initial_position=initial_position,         population_size=40,         population_stddev=2.0,         seed=43210)   ```    Args:     objective_function: A Python callable that accepts a batch of possible       solutions and returns the values of the objective function at those       arguments as a rank 1 real `Tensor`. This specifies the function to be       minimized. The input to this callable may be either a single `Tensor`       or a Python `list` of `Tensor`s. The signature must match the format of       the argument `population`. (i.e. objective_function(*population) must       return the value of the function to be minimized).     initial_population: A real `Tensor` or Python list of `Tensor`s.       If a list, each `Tensor` must be of rank at least 1 and with a common       first dimension. The first dimension indexes into the candidate solutions       while the rest of the dimensions (if any) index into an individual       solution. The size of the population must be at least 4. This is a       requirement of the DE algorithm.     initial_position: A real `Tensor` of any shape. The seed solution used       to initialize the population of solutions. If this parameter is specified       then `initial_population` must not be specified.     population_size: A positive scalar int32 `Tensor` greater than 4. The       size of the population to evolve. This parameter is ignored if       `initial_population` is specified.       Default value: 50.     population_stddev: A positive scalar real `Tensor` of the same dtype       as `initial_position`. This parameter is ignored if `initial_population`       is specified. Used to generate the population from the `initial_position`       by adding random normal noise with zero mean and the specified standard       deviation.       Default value: 1.0     max_iterations: Positive scalar int32 `Tensor`. The maximum number of       generations to evolve the population for.       Default value: 100     func_tolerance: Scalar `Tensor` of the same dtype as the output of the       `objective_function`. The algorithm stops if the absolute difference       between the largest and the smallest objective function value in the       population is below this number.       Default value: 0     position_tolerance: Scalar `Tensor` of the same real dtype as       `initial_position` or `initial_population`. The algorithm terminates if       the largest absolute difference between the coordinates of the population       members is below this threshold.       Default value: 1e-8     differential_weight: Real scalar `Tensor`. Must be positive and less than       2.0. The parameter controlling the strength of mutation in the algorithm.       Default value: 0.5     crossover_prob: Real scalar `Tensor`. Must be between 0 and 1. The       probability of recombination per site.       Default value: 0.9     seed: `int` or None. The random seed for this `Op`. If `None`, no seed is       applied.       Default value: None.     name: (Optional) Python str. The name prefixed to the ops created by this       function. If not supplied, the default name       'differential_evolution_minimize' is used.       Default value: None    Returns:     optimizer_results: An object containing the following attributes:       converged: Scalar boolean `Tensor` indicating whether the minimum was         found within the specified tolerances.       num_objective_evaluations: The total number of objective         evaluations performed.       position: A `Tensor` containing the best point found during the search.         If the search converged, then this value is the argmin of the         objective function within the specified tolerances.       objective_value: A `Tensor` containing the value of the objective         function at the `position`. If the search         converged, then this is the (local) minimum of         the objective function.       final_population: The final state of the population.       final_objective_values: The objective function evaluated at the         final population.       initial_population: The starting population.       initial_objective_values: The objective function evaluated at the         initial population.       num_iterations: The number of iterations of the main algorithm body.    Raises:     ValueError: If neither the initial population, nor the initial position       are specified or if both are specified.
Processes initial args.
Finds the population member with the lowest value.
Checks whether the convergence criteria have been met.
Constructs the initial population.    If an initial population is not already provided, this function constructs   a population by adding random normal noise to the initial position.    Args:     initial_population: None or a list of `Tensor`s. The initial population.     initial_position: None or a list of `Tensor`s. The initial position.       If initial_population is None, this argument must not be None.     population_size: Scalar integer `Tensor`. The number of members in the       population. If the initial population is not None, this parameter is       ignored.     population_stddev: A positive scalar real `Tensor` of the same dtype       as `initial_position` or `initial_population` (whichever is not None).       This parameter is ignored if `initial_population`       is specified. Used to generate the population from the       `initial_position` by adding random normal noise with zero mean and       the specified standard deviation.     seed: Seed for random number generation.    Returns:     A list of `Tensor`s. The initial population.
Performs recombination by binary crossover for the current population.    Let v_i denote the i'th component of the member v and m_i the corresponding   component of the mutant vector corresponding to v. Then the crossed over   vector w_i is determined by setting w_i =   (m_i with probability=crossover_prob else v_i). In addition, DE requires that   at least one of the components is crossed over (otherwise we end   up with no change). This is done by choosing on index say k randomly where   a force crossover is performed (i.e. w_k = m_k). This is the scheme   implemented in this function.    Args:     population: A Python list of `Tensor`s where each `Tensor` in the list       must be of rank at least 1 and all the elements must have a common       first dimension. The base population to cross over.     population_size: A scalar integer `Tensor`. The number of elements in the       population (i.e. size of the first dimension of any member of       `population`).     mutants: A Python list of `Tensor`s with the same structure as `population`.       The mutated population.     crossover_prob: A postive real scalar `Tensor` bounded above by 1.0. The       probability of a crossover being performed for each axis.     seed: `int` or None. The random seed for this `Op`. If `None`, no seed is       applied.    Returns:     A list of `Tensor`s of the same structure, dtype and shape as `population`.     The recombined population.
Computes the mutatated vectors for each population member.    Args:     population:  Python `list` of `Tensor`s representing the       current population vectors. Each `Tensor` must be of the same real dtype.       The first dimension of each `Tensor` indexes individual       population members. For example, if the population is a list with a       single `Tensor` of shape [n, m1, m2], then `n` is the population size and       the shape of an individual solution is [m1, m2].       If there is more than one element in the population, then each `Tensor`       in the list should have the first axis of the same size.     population_size: Scalar integer `Tensor`. The size of the population.     mixing_indices: `Tensor` of integral dtype and shape [n, 3] where `n` is the       number of members in the population. Each element of the `Tensor` must be       a valid index into the first dimension of the population (i.e range       between `0` and `n-1` inclusive).     differential_weight: Real scalar `Tensor`. Must be positive and less than       2.0. The parameter controlling the strength of mutation.    Returns:     mutants: `Tensor` or Python `list` of `Tensor`s of the same shape and dtype       as the input population. The mutated vectors.
Generates an array of indices suitable for mutation operation.    The mutation operation in differential evolution requires that for every   element of the population, three distinct other elements be chosen to produce   a trial candidate. This function generates an array of shape [size, 3]   satisfying the properties that:     (a). array[i, :] does not contain the index 'i'.     (b). array[i, :] does not contain any overlapping indices.     (c). All elements in the array are between 0 and size - 1 inclusive.    Args:     size: Scalar integer `Tensor`. The number of samples as well as a the range       of the indices to sample from.     seed: `int` or None. The random seed for this `Op`. If `None`, no seed is       applied.       Default value: `None`.     name:  Python `str` name prefixed to Ops created by this function.       Default value: 'get_mixing_indices'.    Returns:     sample: A `Tensor` of shape [size, 3] and same dtype as `size` containing     samples without replacement between 0 and size - 1 (inclusive) with the     `i`th row not including the number `i`.
Converts the input arg to a list if it is not a list already.    Args:     tensor_or_list: A `Tensor` or a Python list of `Tensor`s. The argument to       convert to a list of `Tensor`s.    Returns:     A tuple of two elements. The first is a Python list of `Tensor`s containing     the original arguments. The second is a boolean indicating whether     the original argument was a list or tuple already.
Gets a Tensor of type `dtype`, 0 if `tol` is None, validation optional.
Soft Thresholding operator.    This operator is defined by the equations    ```none                                 { x[i] - gamma,  x[i] >   gamma   SoftThreshold(x, gamma)[i] =  { 0,             x[i] ==  gamma                                 { x[i] + gamma,  x[i] <  -gamma   ```    In the context of proximal gradient methods, we have    ```none   SoftThreshold(x, gamma) = prox_{gamma L1}(x)   ```    where `prox` is the proximity operator.  Thus the soft thresholding operator   is used in proximal gradient descent for optimizing a smooth function with   (non-smooth) L1 regularization, as outlined below.    The proximity operator is defined as:    ```none   prox_r(x) = argmin{ r(z) + 0.5 ||x - z||_2**2 : z },   ```    where `r` is a (weakly) convex function, not necessarily differentiable.   Because the L2 norm is strictly convex, the above argmin is unique.    One important application of the proximity operator is as follows.  Let `L` be   a convex and differentiable function with Lipschitz-continuous gradient.  Let   `R` be a convex lower semicontinuous function which is possibly   nondifferentiable.  Let `gamma` be an arbitrary positive real.  Then    ```none   x_star = argmin{ L(x) + R(x) : x }   ```    if and only if the fixed-point equation is satisfied:    ```none   x_star = prox_{gamma R}(x_star - gamma grad L(x_star))   ```    Proximal gradient descent thus typically consists of choosing an initial value   `x^{(0)}` and repeatedly applying the update    ```none   x^{(k+1)} = prox_{gamma^{(k)} R}(x^{(k)} - gamma^{(k)} grad L(x^{(k)}))   ```    where `gamma` is allowed to vary from iteration to iteration.  Specializing to   the case where `R(x) = ||x||_1`, we minimize `L(x) + ||x||_1` by repeatedly   applying the update    ```   x^{(k+1)} = SoftThreshold(x - gamma grad L(x^{(k)}), gamma)   ```    (This idea can also be extended to second-order approximations, although the   multivariate case does not have a known closed form like above.)    Args:     x: `float` `Tensor` representing the input to the SoftThreshold function.     threshold: nonnegative scalar, `float` `Tensor` representing the radius of       the interval on which each coordinate of SoftThreshold takes the value       zero.  Denoted `gamma` above.     name: Python string indicating the name of the TensorFlow operation.       Default value: `'soft_threshold'`.    Returns:     softthreshold: `float` `Tensor` with the same shape and dtype as `x`,       representing the value of the SoftThreshold function.    #### References    [1]: Yu, Yao-Liang. The Proximity Operator.        https://www.cs.cmu.edu/~suvrit/teach/yaoliang_proximity.pdf    [2]: Wikipedia Contributors. Proximal gradient methods for learning.        _Wikipedia, The Free Encyclopedia_, 2018.        https://en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning
Clips values to a specified min and max while leaving gradient unaltered.    Like `tf.clip_by_value`, this function returns a tensor of the same type and   shape as input `t` but with values clamped to be no smaller than to   `clip_value_min` and no larger than `clip_value_max`. Unlike   `tf.clip_by_value`, the gradient is unaffected by this op, i.e.,    ```python   tf.gradients(tfp.math.clip_by_value_preserve_gradient(x), x)[0]   # ==> ones_like(x)   ```    Note: `clip_value_min` needs to be smaller or equal to `clip_value_max` for   correct results.    Args:     t: A `Tensor`.     clip_value_min: A scalar `Tensor`, or a `Tensor` with the same shape       as `t`. The minimum value to clip by.     clip_value_max: A scalar `Tensor`, or a `Tensor` with the same shape       as `t`. The maximum value to clip by.     name: A name for the operation (optional).       Default value: `'clip_by_value_preserve_gradient'`.    Returns:     clipped_t: A clipped `Tensor`.
Build an iterator over training batches.
Save a synthetic image as a PNG file.    Args:     images: samples of synthetic images generated by the generative network.     fname: Python `str`, filename to save the plot to.
Converts a sequence of productions into a string of terminal symbols.      Args:       productions: Tensor of shape [1, num_productions, num_production_rules].         Slices along the `num_productions` dimension represent one-hot vectors.      Returns:       str that concatenates all terminal symbols from `productions`.      Raises:       ValueError: If the first production rule does not begin with         `self.start_symbol`.
Runs the model forward to generate a sequence of productions.      Args:       inputs: Unused.      Returns:       productions: Tensor of shape [1, num_productions, num_production_rules].         Slices along the `num_productions` dimension represent one-hot vectors.
Runs the model forward to return a stochastic encoding.      Args:       inputs: Tensor of shape [1, num_productions, num_production_rules]. It is         a sequence of productions of length `num_productions`. Each production         is a one-hot vector of length `num_production_rules`: it determines         which production rule the production corresponds to.      Returns:       latent_code_posterior: A random variable capturing a sample from the         variational distribution, of shape [1, self.latent_size].
Integral of the `hat` function, used for sampling.      We choose a `hat` function, h(x) = x^(-power), which is a continuous     (unnormalized) density touching each positive integer at the (unnormalized)     pmf. This function implements `hat` integral: H(x) = int_x^inf h(t) dt;     which is needed for sampling purposes.      Arguments:       x: A Tensor of points x at which to evaluate H(x).      Returns:       A Tensor containing evaluation H(x) at x.
Inverse function of _hat_integral.
Compute the matrix rank; the number of non-zero SVD singular values.    Arguments:     a: (Batch of) `float`-like matrix-shaped `Tensor`(s) which are to be       pseudo-inverted.     tol: Threshold below which the singular value is counted as "zero".       Default value: `None` (i.e., `eps * max(rows, cols) * max(singular_val)`).     validate_args: When `True`, additional assertions might be embedded in the       graph.       Default value: `False` (i.e., no graph assertions are added).     name: Python `str` prefixed to ops created by this function.       Default value: "matrix_rank".    Returns:     matrix_rank: (Batch of) `int32` scalars representing the number of non-zero       singular values.
Compute the Moore-Penrose pseudo-inverse of a matrix.    Calculate the [generalized inverse of a matrix](   https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) using its   singular-value decomposition (SVD) and including all large singular values.    The pseudo-inverse of a matrix `A`, is defined as: "the matrix that 'solves'   [the least-squares problem] `A @ x = b`," i.e., if `x_hat` is a solution, then   `A_pinv` is the matrix such that `x_hat = A_pinv @ b`. It can be shown that if   `U @ Sigma @ V.T = A` is the singular value decomposition of `A`, then   `A_pinv = V @ inv(Sigma) U^T`. [(Strang, 1980)][1]    This function is analogous to [`numpy.linalg.pinv`](   https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html).   It differs only in default value of `rcond`. In `numpy.linalg.pinv`, the   default `rcond` is `1e-15`. Here the default is   `10. * max(num_rows, num_cols) * np.finfo(dtype).eps`.    Args:     a: (Batch of) `float`-like matrix-shaped `Tensor`(s) which are to be       pseudo-inverted.     rcond: `Tensor` of small singular value cutoffs.  Singular values smaller       (in modulus) than `rcond` * largest_singular_value (again, in modulus) are       set to zero. Must broadcast against `tf.shape(a)[:-2]`.       Default value: `10. * max(num_rows, num_cols) * np.finfo(a.dtype).eps`.     validate_args: When `True`, additional assertions might be embedded in the       graph.       Default value: `False` (i.e., no graph assertions are added).     name: Python `str` prefixed to ops created by this function.       Default value: "pinv".    Returns:     a_pinv: The pseudo-inverse of input `a`. Has same shape as `a` except       rightmost two dimensions are transposed.    Raises:     TypeError: if input `a` does not have `float`-like `dtype`.     ValueError: if input `a` has fewer than 2 dimensions.    #### Examples    ```python   import tensorflow as tf   import tensorflow_probability as tfp    a = tf.constant([[1.,  0.4,  0.5],                    [0.4, 0.2,  0.25],                    [0.5, 0.25, 0.35]])   tf.matmul(tfp.math.pinv(a), a)   # ==> array([[1., 0., 0.],                [0., 1., 0.],                [0., 0., 1.]], dtype=float32)    a = tf.constant([[1.,  0.4,  0.5,  1.],                    [0.4, 0.2,  0.25, 2.],                    [0.5, 0.25, 0.35, 3.]])   tf.matmul(tfp.math.pinv(a), a)   # ==> array([[ 0.76,  0.37,  0.21, -0.02],                [ 0.37,  0.43, -0.33,  0.02],                [ 0.21, -0.33,  0.81,  0.01],                [-0.02,  0.02,  0.01,  1.  ]], dtype=float32)   ```    #### References    [1]: G. Strang. "Linear Algebra and Its Applications, 2nd Ed." Academic Press,        Inc., 1980, pp. 139-142.
Solves systems of linear eqns `A X = RHS`, given LU factorizations.    Note: this function does not verify the implied matrix is actually invertible   nor is this condition checked even when `validate_args=True`.    Args:     lower_upper: `lu` as returned by `tf.linalg.lu`, i.e., if       `matmul(P, matmul(L, U)) = X` then `lower_upper = L + U - eye`.     perm: `p` as returned by `tf.linag.lu`, i.e., if       `matmul(P, matmul(L, U)) = X` then `perm = argmax(P)`.     rhs: Matrix-shaped float `Tensor` representing targets for which to solve;       `A X = RHS`. To handle vector cases, use:       `lu_solve(..., rhs[..., tf.newaxis])[..., 0]`.     validate_args: Python `bool` indicating whether arguments should be checked       for correctness. Note: this function does not verify the implied matrix is       actually invertible, even when `validate_args=True`.       Default value: `False` (i.e., don't validate arguments).     name: Python `str` name given to ops managed by this object.       Default value: `None` (i.e., "lu_solve").    Returns:     x: The `X` in `A @ X = RHS`.    #### Examples    ```python   import numpy as np   import tensorflow as tf   import tensorflow_probability as tfp    x = [[[1., 2],         [3, 4]],        [[7, 8],         [3, 4]]]   inv_x = tfp.math.lu_solve(*tf.linalg.lu(x), rhs=tf.eye(2))   tf.assert_near(tf.matrix_inverse(x), inv_x)   # ==> True   ```
Computes a matrix inverse given the matrix's LU decomposition.    This op is conceptually identical to,    ````python   inv_X = tf.lu_matrix_inverse(*tf.linalg.lu(X))   tf.assert_near(tf.matrix_inverse(X), inv_X)   # ==> True   ```    Note: this function does not verify the implied matrix is actually invertible   nor is this condition checked even when `validate_args=True`.    Args:     lower_upper: `lu` as returned by `tf.linalg.lu`, i.e., if       `matmul(P, matmul(L, U)) = X` then `lower_upper = L + U - eye`.     perm: `p` as returned by `tf.linag.lu`, i.e., if       `matmul(P, matmul(L, U)) = X` then `perm = argmax(P)`.     validate_args: Python `bool` indicating whether arguments should be checked       for correctness. Note: this function does not verify the implied matrix is       actually invertible, even when `validate_args=True`.       Default value: `False` (i.e., don't validate arguments).     name: Python `str` name given to ops managed by this object.       Default value: `None` (i.e., "lu_matrix_inverse").    Returns:     inv_x: The matrix_inv, i.e.,       `tf.matrix_inverse(tfp.math.lu_reconstruct(lu, perm))`.    #### Examples    ```python   import numpy as np   import tensorflow as tf   import tensorflow_probability as tfp    x = [[[3., 4], [1, 2]],        [[7., 8], [3, 4]]]   inv_x = tfp.math.lu_matrix_inverse(*tf.linalg.lu(x))   tf.assert_near(tf.matrix_inverse(x), inv_x)   # ==> True   ```
Returns list of assertions related to `lu_reconstruct` assumptions.
Returns list of assertions related to `lu_solve` assumptions.
Returns a block diagonal rank 2 SparseTensor from a batch of SparseTensors.    Args:     sp_a: A rank 3 `SparseTensor` representing a batch of matrices.    Returns:     sp_block_diag_a: matrix-shaped, `float` `SparseTensor` with the same dtype     as `sparse_or_matrix`, of shape [B * M, B * N] where `sp_a` has shape     [B, M, N]. Each [M, N] batch of `sp_a` is lined up along the diagonal.
Checks that input is a `float` matrix.
Computes the neg-log-likelihood gradient and Fisher information for a GLM.    Note that Fisher information is related to the Hessian of the log-likelihood   by the equation    ```none   FisherInfo = E[Hessian with respect to model_coefficients of -LogLikelihood(       Y | model_matrix, model_coefficients)]   ```    where `LogLikelihood` is the log-likelihood of a generalized linear model   parameterized by `model_matrix` and `model_coefficients`, and the expectation   is taken over Y, distributed according to the same GLM with the same parameter   values.    Args:     model_matrix: (Batch of) matrix-shaped, `float` `Tensor` or `SparseTensor`       where each row represents a sample's features.  Has shape `[N, n]` where       `N` is the number of data samples and `n` is the number of features per       sample.     linear_response: (Batch of) vector-shaped `Tensor` with the same dtype as       `model_matrix`, equal to `model_matix @ model_coefficients` where       `model_coefficients` are the coefficients of the linear component of the       GLM.     response: (Batch of) vector-shaped `Tensor` with the same dtype as       `model_matrix` where each element represents a sample's observed response       (to the corresponding row of features).     model: `tfp.glm.ExponentialFamily`-like instance, which specifies the link       function and distribution of the GLM, and thus characterizes the negative       log-likelihood. Must have sufficient statistic equal to the response, that       is, `T(y) = y`.    Returns:     grad_neg_log_likelihood: (Batch of) vector-shaped `Tensor` with the same       shape and dtype as a single row of `model_matrix`, representing the       gradient of the negative log likelihood of `response` given linear       response `linear_response`.     fim_middle: (Batch of) vector-shaped `Tensor` with the same shape and dtype       as a single column of `model_matrix`, satisfying the equation       `Fisher information =       Transpose(model_matrix)       @ diag(fim_middle)       @ model_matrix`.
r"""Fits a GLM using coordinate-wise FIM-informed proximal gradient descent.    This function uses a L1- and L2-regularized, second-order quasi-Newton method   to find maximum-likelihood parameters for the given model and observed data.   The second-order approximations use negative Fisher information in place of   the Hessian, that is,    ```none   FisherInfo = E_Y[Hessian with respect to model_coefficients of -LogLikelihood(       Y | model_matrix, current value of model_coefficients)]   ```    For large, sparse data sets, `model_matrix` should be supplied as a   `SparseTensor`.    Args:     model_matrix: (Batch of) matrix-shaped, `float` `Tensor` or `SparseTensor`       where each row represents a sample's features.  Has shape `[N, n]` where       `N` is the number of data samples and `n` is the number of features per       sample.     response: (Batch of) vector-shaped `Tensor` with the same dtype as       `model_matrix` where each element represents a sample's observed response       (to the corresponding row of features).     model: `tfp.glm.ExponentialFamily`-like instance, which specifies the link       function and distribution of the GLM, and thus characterizes the negative       log-likelihood which will be minimized. Must have sufficient statistic       equal to the response, that is, `T(y) = y`.     model_coefficients_start: (Batch of) vector-shaped, `float` `Tensor` with       the same dtype as `model_matrix`, representing the initial values of the       coefficients for the GLM regression.  Has shape `[n]` where `model_matrix`       has shape `[N, n]`.     tolerance: scalar, `float` `Tensor` representing the tolerance for each       optiization step; see the `tolerance` argument of `fit_sparse_one_step`.     l1_regularizer: scalar, `float` `Tensor` representing the weight of the L1       regularization term.     l2_regularizer: scalar, `float` `Tensor` representing the weight of the L2       regularization term.       Default value: `None` (i.e., no L2 regularization).     maximum_iterations: Python integer specifying maximum number of iterations       of the outer loop of the optimizer (i.e., maximum number of calls to       `fit_sparse_one_step`).  After this many iterations of the outer loop, the       algorithm will terminate even if the return value `model_coefficients` has       not converged.       Default value: `1`.     maximum_full_sweeps_per_iteration: Python integer specifying the maximum       number of coordinate descent sweeps allowed in each iteration.       Default value: `1`.     learning_rate: scalar, `float` `Tensor` representing a multiplicative factor       used to dampen the proximal gradient descent steps.       Default value: `None` (i.e., factor is conceptually `1`).     name: Python string representing the name of the TensorFlow operation.       The default name is `"fit_sparse"`.    Returns:     model_coefficients: (Batch of) `Tensor` of the same shape and dtype as       `model_coefficients_start`, representing the computed model coefficients       which minimize the regularized negative log-likelihood.     is_converged: scalar, `bool` `Tensor` indicating whether the minimization       procedure converged across all batches within the specified number of       iterations.  Here convergence means that an iteration of the inner loop       (`fit_sparse_one_step`) returns `True` for its `is_converged` output       value.     iter: scalar, `int` `Tensor` indicating the actual number of iterations of       the outer loop of the optimizer completed (i.e., number of calls to       `fit_sparse_one_step` before achieving convergence).    #### Example    ```python   from __future__ import print_function   import numpy as np   import tensorflow as tf   import tensorflow_probability as tfp   tfd = tfp.distributions    def make_dataset(n, d, link, scale=1., dtype=np.float32):     model_coefficients = tfd.Uniform(         low=np.array(-1, dtype), high=np.array(1, dtype)).sample(             d, seed=42)     radius = np.sqrt(2.)     model_coefficients *= radius / tf.linalg.norm(model_coefficients)     mask = tf.random_shuffle(tf.range(d)) < tf.to_int32(0.5 * tf.to_float(d))     model_coefficients = tf.where(mask, model_coefficients,                                   tf.zeros_like(model_coefficients))     model_matrix = tfd.Normal(         loc=np.array(0, dtype), scale=np.array(1, dtype)).sample(             [n, d], seed=43)     scale = tf.convert_to_tensor(scale, dtype)     linear_response = tf.matmul(model_matrix,                                 model_coefficients[..., tf.newaxis])[..., 0]     if link == 'linear':       response = tfd.Normal(loc=linear_response, scale=scale).sample(seed=44)     elif link == 'probit':       response = tf.cast(           tfd.Normal(loc=linear_response, scale=scale).sample(seed=44) > 0,                      dtype)     elif link == 'logit':       response = tfd.Bernoulli(logits=linear_response).sample(seed=44)     else:       raise ValueError('unrecognized true link: {}'.format(link))     return model_matrix, response, model_coefficients, mask    with tf.Session() as sess:     x_, y_, model_coefficients_true_, _ = sess.run(make_dataset(         n=int(1e5), d=100, link='probit'))      model = tfp.glm.Bernoulli()     model_coefficients_start = tf.zeros(x_.shape[-1], np.float32)      model_coefficients, is_converged, num_iter = tfp.glm.fit_sparse(         model_matrix=tf.convert_to_tensor(x_),         response=tf.convert_to_tensor(y_),         model=model,         model_coefficients_start=model_coefficients_start,         l1_regularizer=800.,         l2_regularizer=None,         maximum_iterations=10,         maximum_full_sweeps_per_iteration=10,         tolerance=1e-6,         learning_rate=None)      model_coefficients_, is_converged_, num_iter_ = sess.run([         model_coefficients, is_converged, num_iter])      print("is_converged:", is_converged_)     print("    num_iter:", num_iter_)     print("\nLearned / True")     print(np.concatenate(         [[model_coefficients_], [model_coefficients_true_]], axis=0).T)    # ==>   # is_converged: True   #     num_iter: 1   #   # Learned / True   # [[ 0.          0.        ]   #  [ 0.          0.        ]   #  [ 0.          0.        ]   #  [ 0.11195257  0.12484948]   #  [ 0.          0.        ]   #  [ 0.05191106  0.06394956]   #  [-0.15090358 -0.15325639]   #  [-0.18187316 -0.18825999]   #  [-0.06140942 -0.07994166]   #  [ 0.          0.        ]   #  [ 0.          0.        ]   #  [ 0.          0.        ]   #  [ 0.14474444  0.15810856]   #  [ 0.          0.        ]   #  [-0.25249591 -0.24260855]   #  [ 0.          0.        ]   #  [ 0.          0.        ]   #  [-0.03888761 -0.06755984]   #  [ 0.          0.        ]   #  [ 0.          0.        ]   #  [ 0.          0.        ]   #  [-0.0192222  -0.04169233]   #  [ 0.          0.        ]   #  [ 0.          0.        ]   #  [ 0.01434913  0.03568212]   #  [-0.11336883 -0.12873614]   #  [ 0.          0.        ]   #  [-0.24496339 -0.24048163]   #  [ 0.          0.        ]   #  [ 0.          0.        ]   #  [ 0.04088281  0.06565224]   #  [-0.12784363 -0.13359821]   #  [ 0.05618424  0.07396613]   #  [ 0.          0.        ]   #  [ 0.          0.        ]   #  [ 0.          0.        ]   #  [ 0.         -0.01719233]   #  [ 0.          0.        ]   #  [ 0.          0.        ]   #  [-0.00076072 -0.03607186]   #  [ 0.21801499  0.21146794]   #  [-0.02161094 -0.04031265]   #  [ 0.0918689   0.10487888]   #  [ 0.0106154   0.03233612]   #  [-0.07817317 -0.09725142]   #  [ 0.          0.        ]   #  [ 0.          0.        ]   #  [-0.23725343 -0.24194022]   #  [ 0.          0.        ]   #  [-0.08725718 -0.1048776 ]   #  [ 0.          0.        ]   #  [ 0.          0.        ]   #  [-0.02114314 -0.04145789]   #  [ 0.          0.        ]   #  [ 0.          0.        ]   #  [-0.02710908 -0.04590397]   #  [ 0.15293184  0.15415154]   #  [ 0.2114463   0.2088728 ]   #  [-0.10969634 -0.12368613]   #  [ 0.         -0.01505797]   #  [-0.01140458 -0.03234904]   #  [ 0.16051085  0.1680062 ]   #  [ 0.09816848  0.11094204]   ```    #### References    [1]: Jerome Friedman, Trevor Hastie and Rob Tibshirani. Regularization Paths        for Generalized Linear Models via Coordinate Descent. _Journal of        Statistical Software_, 33(1), 2010.        https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf    [2]: Guo-Xun Yuan, Chia-Hua Ho and Chih-Jen Lin. An Improved GLMNET for        L1-regularized Logistic Regression. _Journal of Machine Learning        Research_, 13, 2012.        http://www.jmlr.org/papers/volume13/yuan12a/yuan12a.pdf
Generate the slices for building an autoregressive mask.
Generate the mask for building an autoregressive dense layer.
A autoregressively masked dense layer. Analogous to `tf.layers.dense`.    See [Germain et al. (2015)][1] for detailed explanation.    Arguments:     inputs: Tensor input.     units: Python `int` scalar representing the dimensionality of the output       space.     num_blocks: Python `int` scalar representing the number of blocks for the       MADE masks.     exclusive: Python `bool` scalar representing whether to zero the diagonal of       the mask, used for the first layer of a MADE.     kernel_initializer: Initializer function for the weight matrix.       If `None` (default), weights are initialized using the       `tf.glorot_random_initializer`.     reuse: Python `bool` scalar representing whether to reuse the weights of a       previous layer by the same name.     name: Python `str` used to describe ops managed by this function.     *args: `tf.layers.dense` arguments.     **kwargs: `tf.layers.dense` keyword arguments.    Returns:     Output tensor.    Raises:     NotImplementedError: if rightmost dimension of `inputs` is unknown prior to       graph execution.    #### References    [1]: Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE:        Masked Autoencoder for Distribution Estimation. In _International        Conference on Machine Learning_, 2015. https://arxiv.org/abs/1502.03509
Returns a degree vectors for the input.
Returns a list of degree vectors, one for each input and hidden layer.    A unit with degree d can only receive input from units with degree < d. Output   units always have the same degree as their associated input unit.    Args:     input_size: Number of inputs.     hidden_units: list with the number of hidden units per layer. It does not       include the output layer. Each hidden unit size must be at least the size       of length (otherwise autoregressivity is not possible).     input_order: Order of degrees to the input units: 'random', 'left-to-right',       'right-to-left', or an array of an explicit order. For example,       'left-to-right' builds an autoregressive model       p(x) = p(x1) p(x2 | x1) ... p(xD | x<D).     hidden_degrees: Method for assigning degrees to the hidden units:       'equal', 'random'.  If 'equal', hidden units in each layer are allocated       equally (up to a remainder term) to each degree.  Default: 'equal'.    Raises:     ValueError: invalid input order.     ValueError: invalid hidden degrees.
Returns a list of binary mask matrices enforcing autoregressivity.
Returns a masked version of the given initializer.
See tfkl.Layer.build.
See tfkl.Layer.call.
Sample a multinomial.    The batch shape is given by broadcasting num_trials with   remove_last_dimension(logits).    Args:     num_samples: Python int or singleton integer Tensor: number of multinomial       samples to draw.     num_classes: Python int or singleton integer Tensor: number of classes.     logits: Floating Tensor with last dimension k, of (unnormalized) logit       probabilities per class.     num_trials: Tensor of number of categorical trials each multinomial consists       of.  num_trials[..., tf.newaxis] must broadcast with logits.     dtype: dtype at which to emit samples.     seed: Random seed.    Returns:     samples: Tensor of given dtype and shape [n] + batch_shape + [k].
Build a zero-dimensional MVNDiag object.
Build an observation_noise_fn that observes a Tensor timeseries.
Build regression weights from model parameters.
Computes the number of edges on longest path from node to root.
Creates tuple of str tuple-str pairs representing resolved & sorted DAG.
Creates lists of callables suitable for JDSeq.
Creates `dist_fn`, `dist_fn_wrapped`, `dist_fn_args`, `dist_fn_name`.
Variational loss for the VGP.      Given `observations` and `observation_index_points`, compute the     negative variational lower bound as specified in [Hensman, 2013][1].      Args:       observations: `float` `Tensor` representing collection, or batch of         collections, of observations corresponding to         `observation_index_points`. Shape has the form `[b1, ..., bB, e]`, which         must be brodcastable with the batch and example shapes of         `observation_index_points`. The batch shape `[b1, ..., bB]` must be         broadcastable with the shapes of all other batched parameters         (`kernel.batch_shape`, `observation_index_points`, etc.).       observation_index_points: `float` `Tensor` representing finite (batch of)         vector(s) of points where observations are defined. Shape has the         form `[b1, ..., bB, e1, f1, ..., fF]` where `F` is the number of feature         dimensions and must equal `kernel.feature_ndims` and `e1` is the number         (size) of index points in each batch (we denote it `e1` to distinguish         it from the numer of inducing index points, denoted `e2` below). If         set to `None` uses `index_points` as the origin for observations.         Default value: None.       kl_weight: Amount by which to scale the KL divergence loss between prior         and posterior.         Default value: 1.       name: Python `str` name prefixed to Ops created by this class.         Default value: "GaussianProcess".     Returns:       loss: Scalar tensor representing the negative variational lower bound.         Can be directly used in a `tf.Optimizer`.     Raises:       ValueError: if `mean_fn` is not `None` and is not callable.      #### References      [1]: Hensman, J., Lawrence, N. "Gaussian Processes for Big Data", 2013          https://arxiv.org/abs/1309.6835
Model selection for optimal variational hyperparameters.      Given the full training set (parameterized by `observations` and     `observation_index_points`), compute the optimal variational     location and scale for the VGP. This is based of the method suggested     in [Titsias, 2009][1].      Args:       kernel: `PositiveSemidefiniteKernel`-like instance representing the         GP's covariance function.       inducing_index_points: `float` `Tensor` of locations of inducing points in         the index set. Shape has the form `[b1, ..., bB, e2, f1, ..., fF]`, just         like `observation_index_points`. The batch shape components needn't be         identical to those of `observation_index_points`, but must be broadcast         compatible with them.       observation_index_points: `float` `Tensor` representing finite (batch of)         vector(s) of points where observations are defined. Shape has the         form `[b1, ..., bB, e1, f1, ..., fF]` where `F` is the number of feature         dimensions and must equal `kernel.feature_ndims` and `e1` is the number         (size) of index points in each batch (we denote it `e1` to distinguish         it from the numer of inducing index points, denoted `e2` below).       observations: `float` `Tensor` representing collection, or batch of         collections, of observations corresponding to         `observation_index_points`. Shape has the form `[b1, ..., bB, e]`, which         must be brodcastable with the batch and example shapes of         `observation_index_points`. The batch shape `[b1, ..., bB]` must be         broadcastable with the shapes of all other batched parameters         (`kernel.batch_shape`, `observation_index_points`, etc.).       observation_noise_variance: `float` `Tensor` representing the variance         of the noise in the Normal likelihood distribution of the model. May be         batched, in which case the batch shape must be broadcastable with the         shapes of all other batched parameters (`kernel.batch_shape`,         `index_points`, etc.).         Default value: `0.`       mean_fn: Python `callable` that acts on index points to produce a (batch         of) vector(s) of mean values at those index points. Takes a `Tensor` of         shape `[b1, ..., bB, f1, ..., fF]` and returns a `Tensor` whose shape is         (broadcastable with) `[b1, ..., bB]`. Default value: `None` implies         constant zero function.       jitter: `float` scalar `Tensor` added to the diagonal of the covariance         matrix to ensure positive definiteness of the covariance matrix.         Default value: `1e-6`.       name: Python `str` name prefixed to Ops created by this class.         Default value: "optimal_variational_posterior".     Returns:       loc, scale: Tuple representing the variational location and scale.     Raises:       ValueError: if `mean_fn` is not `None` and is not callable.      #### References      [1]: Titsias, M. "Variational Model Selection for Sparse Gaussian Process          Regression", 2009.          http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf
Build utility method to compute whether the season is changing.
Build change-of-basis matrices for constrained seasonal effects.    This method builds the matrix that transforms seasonal effects into   effect residuals (differences from the mean effect), and additionally   projects these residuals onto the subspace where the mean effect is zero.    See `ConstrainedSeasonalStateSpaceModel` for mathematical details.    Args:     num_seasons: scalar `int` number of seasons.     dtype: TensorFlow `dtype` for the returned values.   Returns:     effects_to_residuals: `Tensor` of shape       `[num_seasons-1, num_seasons]`, such that `differences_from_mean_effect =       matmul(effects_to_residuals, seasonal_effects)`.  In the       notation of `ConstrainedSeasonalStateSpaceModel`, this is       `effects_to_residuals = P * R`.     residuals_to_effects: the (pseudo)-inverse of the above; a       `Tensor` of shape `[num_seasons, num_seasons-1]`. In the       notation of `ConstrainedSeasonalStateSpaceModel`, this is       `residuals_to_effects = R^{-1} * P'`.
Build a function computing transitions for a seasonal effect model.
Build the transition noise model for a SeasonalStateSpaceModel.
Build transition noise distribution for a ConstrainedSeasonalSSM.
Returns `True` if given observation data is empty.    Emptiness means either     1. Both `observation_index_points` and `observations` are `None`, or     2. the "number of observations" shape is 0. The shape of     `observation_index_points` is `[..., N, f1, ..., fF]`, where `N` is the     number of observations and the `f`s are feature dims. Thus, we look at the     shape element just to the left of the leftmost feature dim. If that shape is     zero, we consider the data empty.    We don't check the shape of observations; validations are checked elsewhere in   the calling code, to ensure these shapes are consistent.    Args:     feature_ndims: the number of feature dims, as reported by the GP kernel.     observation_index_points: the observation data locations in the index set.     observations: the observation data.    Returns:     is_empty: True if the data were deemed to be empty.
Ensure that observation data and locations have consistent shapes.    This basically means that the batch shapes are broadcastable. We can only   ensure this when those shapes are fully statically defined.     Args:     kernel: The GP kernel.     observation_index_points: the observation data locations in the index set.     observations: the observation data.    Raises:     ValueError: if the observations' batch shapes are not broadcastable.
Add a learning rate scheduler to the contained `schedules`          :param scheduler: learning rate scheduler to be add         :param max_iteration: iteration numbers this scheduler will run
Configure checkpoint settings.           :param checkpoint_trigger: the interval to write snapshots         :param checkpoint_path: the path to write snapshots into         :param isOverWrite: whether to overwrite existing snapshots in path.default is True
Configure constant clipping settings.           :param min_value: the minimum value to clip by         :param max_value: the maxmimum value to clip by
Do an optimization.
Set train summary. A TrainSummary object contains information         necessary for the optimizer to know how often the logs are recorded,         where to store the logs and how to retrieve them, etc. For details,         refer to the docs of TrainSummary.           :param summary: a TrainSummary object
Set validation summary. A ValidationSummary object contains information         necessary for the optimizer to know how often the logs are recorded,         where to store the logs and how to retrieve them, etc. For details,         refer to the docs of ValidationSummary.           :param summary: a ValidationSummary object
Create an optimizer.         Depend on the input type, the returning optimizer can be a local optimizer \         or a distributed optimizer.          :param model: the neural net model         :param training_set: (features, label) for local mode. RDD[Sample] for distributed mode.         :param criterion: the loss function         :param optim_method: the algorithm to use for optimization,            e.g. SGD, Adagrad, etc. If optim_method is None, the default algorithm is SGD.         :param end_trigger: when to end the optimization. default value is MapEpoch(1)         :param batch_size: training batch size         :param cores: This is for local optimizer only and use total physical cores as the default value
Set new training dataset, for optimizer reuse          :param training_rdd: the training dataset         :param batch_size: training batch size         :return:
Set the interval of recording for each indicator.           :param tag: tag name. Supported tag names are "LearningRate", "Loss","Throughput", "Parameters". "Parameters" is an umbrella tag thatincludes weight, bias, gradWeight, gradBias, and some running status(eg. runningMean and runningVar in BatchNormalization). If youdidn't set any triggers, we will by default record Loss and Throughputin each iteration, while *NOT* recording LearningRate and Parameters,as recording parameters may introduce substantial overhead when themodel is very big, LearningRate is not a public attribute for allOptimMethod.         :param trigger: trigger
Parse or download mnist data if train_dir is empty.      :param: train_dir: The directory storing the mnist data      :param: data_type: Reading training set or testing set.It can be either "train" or "test"      :return:      ```     (ndarray, ndarray) representing (features, labels)     features is a 4D unit8 numpy array [index, y, x, depth] representing each pixel valued from 0 to 255.     labels is 1D unit8 nunpy array representing the label valued from 0 to 9.     ```
Parse or download news20 if source_dir is empty.      :param source_dir: The directory storing news data.     :return: A list of (tokens, label)
Parse or download the pre-trained glove word2vec if source_dir is empty.      :param source_dir: The directory storing the pre-trained word2vec     :param dim: The dimension of a vector     :return: A dict mapping from word to vector
Configures the learning process. Must be called before fit or evaluate.          # Arguments         optimizer: Optimization method to be used. One can alternatively pass in the corresponding                    string representation, such as 'sgd'.         loss: Criterion to be used. One can alternatively pass in the corresponding string               representation, such as 'mse'.         metrics: List of validation methods to be used. Default is None. One can alternatively use ['accuracy'].
Train a model for a fixed number of epochs on a dataset.          # Arguments         x: Input data. A Numpy array or RDD of Sample or Image DataSet.         y: Labels. A Numpy array. Default is None if x is already RDD of Sample or Image DataSet.         batch_size: Number of samples per gradient update.         nb_epoch: Number of iterations to train.         validation_data: Tuple (x_val, y_val) where x_val and y_val are both Numpy arrays.                          Or RDD of Sample. Default is None if no validation is involved.         distributed: Boolean. Whether to train the model in distributed mode or local mode.                      Default is True. In local mode, x and y must both be Numpy arrays.
Evaluate a model on a given dataset in distributed mode.          # Arguments         x: Input data. A Numpy array or RDD of Sample.         y: Labels. A Numpy array. Default is None if x is already RDD of Sample.         batch_size: Number of samples per gradient update.
Use a model to do prediction.          # Arguments         x: Input data. A Numpy array or RDD of Sample.         distributed: Boolean. Whether to do prediction in distributed mode or local mode.                      Default is True. In local mode, x must be a Numpy array.
Get mnist dataset and parallelize into RDDs.     Data would be downloaded automatically if it doesn't present at the specific location.      :param sc: SparkContext.     :param data_type: "train" for training data and "test" for testing data.     :param location: Location to store mnist dataset.     :return: RDD of (features: ndarray, label: ndarray).
Preprocess mnist dataset.     Normalize and transform into Sample of RDDs.
When to end the optimization based on input option.
Set validation and checkpoint for distributed optimizer.
Return the broadcasted value
Call API in PythonBigDL
Call Java Function
Return a JavaRDD of Object by unpickling       It will convert each Python object into Java object by Pyrolite, whenever     the RDD is serialized in batch or not.
Convert Python object into Java
Convert to a bigdl activation layer         given the name of the activation as a string
Convert a ndarray to a DenseTensor which would be used in Java side.          >>> import numpy as np         >>> from bigdl.util.common import JTensor         >>> from bigdl.util.common import callBigDlFunc         >>> np.random.seed(123)         >>> data = np.random.uniform(0, 1, (2, 3)).astype("float32")         >>> result = JTensor.from_ndarray(data)         >>> expected_storage = np.array([[0.69646919, 0.28613934, 0.22685145], [0.55131477, 0.71946895, 0.42310646]])         >>> expected_shape = np.array([2, 3])         >>> np.testing.assert_allclose(result.storage, expected_storage, rtol=1e-6, atol=1e-6)         >>> np.testing.assert_allclose(result.shape, expected_shape)         >>> data_back = result.to_ndarray()         >>> (data == data_back).all()         True         >>> tensor1 = callBigDlFunc("float", "testTensor", JTensor.from_ndarray(data))  # noqa         >>> array_from_tensor = tensor1.to_ndarray()         >>> (array_from_tensor == data).all()         True
get label as ndarray from ImageFeature
Read parquet file as DistributedImageFrame
write ImageFrame as parquet file
get image from ImageFrame
get image list from ImageFrame
get label rdd from ImageFrame
get prediction rdd from ImageFrame
Generates output predictions for the input samples,         processing the samples in a batched way.          # Arguments             x: the input data, as a Numpy array or list of Numpy array for local mode.                as RDD[Sample] for distributed mode             is_distributed: used to control run in local or cluster. the default value is False         # Returns             A Numpy array or RDD[Sample] of predictions.
Optimize the model by the given options          :param x: ndarray or list of ndarray for local mode.                   RDD[Sample] for distributed mode         :param y: ndarray or list of ndarray for local mode and would be None for cluster mode.             is_distributed: used to control run in local or cluster. the default value is False.             NB: if is_distributed=true, x should be RDD[Sample] and y should be None         :param is_distributed: Whether to train in local mode or distributed mode         :return:             A Numpy array or RDD[Sample] of predictions.
Apply the transformer to the images in "inputCol" and store the transformed result         into "outputCols"
Save a Keras model definition to JSON with given path
Define a convnet model in Keras 1.2.2
module predict, return the predict label          :param data_rdd: the data to be predict.         :return: An RDD represent the predict label.
Set weights for this layer          :param weights: a list of numpy arrays which represent weight and bias         :return:          >>> linear = Linear(3,2)         creating: createLinear         >>> linear.set_weights([np.array([[1,2,3],[4,5,6]]), np.array([7,8])])         >>> weights = linear.get_weights()         >>> weights[0].shape == (2,3)         True         >>> np.testing.assert_allclose(weights[0][0], np.array([1., 2., 3.]))         >>> np.testing.assert_allclose(weights[1], np.array([7., 8.]))         >>> relu = ReLU()         creating: createReLU         >>> from py4j.protocol import Py4JJavaError         >>> try:         ...     relu.set_weights([np.array([[1,2,3],[4,5,6]]), np.array([7,8])])         ... except Py4JJavaError as err:         ...     print(err.java_exception)         ...         java.lang.IllegalArgumentException: requirement failed: this layer does not have weight/bias         >>> relu.get_weights()         The layer does not have weight/bias         >>> add = Add(2)         creating: createAdd         >>> try:         ...     add.set_weights([np.array([7,8]), np.array([1,2])])         ... except Py4JJavaError as err:         ...     print(err.java_exception)         ...         java.lang.IllegalArgumentException: requirement failed: the number of input weight/bias is not consistant with number of weight/bias of this layer, number of input 1, number of output 2         >>> cAdd = CAdd([4, 1])         creating: createCAdd         >>> cAdd.set_weights(np.ones([4, 1]))         >>> (cAdd.get_weights()[0] == np.ones([4, 1])).all()         True
Get weights for this layer          :return: list of numpy arrays which represent weight and bias
Save a model to protobuf files so that it can be used in tensorflow inference.          When saving the model, placeholders will be added to the tf model as input nodes. So         you need to pass in the names and shapes of the placeholders. BigDL model doesn't have         such information. The order of the placeholder information should be same as the inputs         of the graph model.         :param inputs: placeholder information, should be an array of tuples (input_name, shape)                        where 'input_name' is a string and shape is an array of integer         :param path: the path to be saved to         :param byte_order: model byte order         :param data_format: model data format, should be "nhwc" or "nchw"
Set this layer in the training mode or in predition mode if is_training=False
Load a pre-trained Torch model.          :param path: The path containing the pre-trained model.         :return: A pre-trained model.
Load a pre-trained Keras model.          :param json_path: The json path containing the keras model definition.         :param hdf5_path: The HDF5 path containing the pre-trained keras model weights with or without the model architecture.         :return: A bigdl model.
Create a python Criterion by a java criterion object          :param jcriterion: A java criterion object which created by Py4j         :return: a criterion.
The file path can be stored in a local file system, HDFS, S3,         or any Hadoop-supported file system.
Load IMDB dataset     Transform input data into an RDD of Sample
Define a recurrent convolutional model in Keras 1.2.2
Return a list of shape tuples if there are multiple inputs.         Return one shape tuple otherwise.
Return a list of shape tuples if there are multiple outputs.         Return one shape tuple otherwise.
Get mnist dataset with features and label as ndarray.     Data would be downloaded automatically if it doesn't present at the specific location.      :param data_type: "train" for training data and "test" for testing data.     :param location: Location to store mnist dataset.     :return: (features: ndarray, label: ndarray)
Parse or download movielens 1m  data if train_dir is empty.      :param data_dir: The directory storing the movielens data     :return: a 2D numpy array with user index and item index in each row
Get and return the jar path for bigdl if exists.
Check if spark version is below 2.2
Export variable tensors from the checkpoint files.      :param checkpoint_path: tensorflow checkpoint path     :return: dictionary of tensor. The key is the variable name and the value is the numpy
Save a variable dictionary to a Java object file, so it can be read by BigDL      :param tensors: tensor dictionary     :param target_path: where is the Java object file store     :param bigdl_type: model variable numeric type     :return: nothing
Expand and tile tensor along given axis      Args:         units: tf tensor with dimensions [batch_size, time_steps, n_input_features]         axis: axis along which expand and tile. Must be 1 or 2
Collecting possible continuations of length <= n for every node
Simple attention without any conditions.         Computes weighted sum of memory elements.
Computes weighted sum of inputs conditioned on state
Computes BLEU score of translated segments against one or more references.    Args:     reference_corpus: list of lists of references for each translation. Each         reference should be tokenized into a list of tokens.     translation_corpus: list of translations to score. Each translation         should be tokenized into a list of tokens.     max_order: Maximum n-gram order to use when computing BLEU score.     smooth: Whether or not to apply Lin et al. 2004 smoothing.    Returns:     3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram     precisions and brevity penalty.
Returns opened file object for writing dialog logs.          Returns:             log_file: opened Python file object.
Logs single dialog utterance to current dialog log file.          Args:             utterance: Dialog utterance.             direction: 'in' or 'out' utterance direction.             dialog_id: Dialog ID.
get summary ops for the magnitude of gradient updates
Dump the trained weights from a model to a HDF5 file.
Read data by dataset_reader from specified config.
Make training and evaluation of the model described in corresponding configuration file.
Exchange messages between basic pipelines and the Yandex.Dialogs service.     If the pipeline returns multiple values, only the first one is forwarded to Yandex.
Convert labels to one-hot vectors for multi-class multi-label classification      Args:         labels: list of samples where each sample is a class or a list of classes which sample belongs with         classes: array of classes' names      Returns:         2d array with one-hot representation of given samples
Convert vectors of probabilities to one-hot representations using confident threshold      Args:         proba: samples where each sample is a vector of probabilities to belong with given classes         confident_threshold: boundary of probability to belong with a class         classes: array of classes' names      Returns:         2d array with one-hot representation of given samples
Configure session for particular device          Returns:             tensorflow.Session
Checks existence of the model file, loads the model if the file exists
Extract values of momentum variables from optimizer          Returns:             optimizer's `rho` or `beta_1`
Update graph variables setting giving `learning_rate` and `momentum`          Args:             learning_rate: learning rate value to be set in graph (set if not None)             momentum: momentum value to be set in graph (set if not None)          Returns:             None
Calculates F1 macro measure.      Args:         y_true: list of true values         y_predicted: list of predicted values      Returns:         F1 score
Converts word to a tuple of symbols, optionally converts it to lowercase     and adds capitalization label.      Args:         word: input word         to_lower: whether to lowercase         append_case: whether to add case mark             ('<FIRST_UPPER>' for first capital and '<ALL_UPPER>' for all caps)      Returns:         a preprocessed word
Number of convolutional layers stacked on top of each other      Args:         units: a tensorflow tensor with dimensionality [None, n_tokens, n_features]         n_hidden_list: list with number of hidden units at the ouput of each layer         filter_width: width of the kernel in tokens         use_batch_norm: whether to use batch normalization between layers         use_dilation: use power of 2 dilation scheme [1, 2, 4, 8 .. ] for layers 1, 2, 3, 4 ...         training_ph: boolean placeholder determining whether is training phase now or not.             It is used only for batch normalization to determine whether to use             current batch average (std) or memory stored average (std)         add_l2_losses: whether to add l2 losses on network kernels to                 tf.GraphKeys.REGULARIZATION_LOSSES or not      Returns:         units: tensor at the output of the last convolutional layer
Bi directional recurrent neural network. GRU or LSTM          Args:             units: a tensorflow tensor with dimensionality [None, n_tokens, n_features]             n_hidden: list with number of hidden units at the ouput of each layer             seq_lengths: length of sequences for different length sequences in batch                 can be None for maximum length as a length for every sample in the batch             cell_type: 'lstm' or 'gru'             trainable_initial_states: whether to create a special trainable variable                 to initialize the hidden states of the network or use just zeros             use_peepholes: whether to use peephole connections (only 'lstm' case affected)             name: what variable_scope to use for the network parameters          Returns:             units: tensor at the output of the last recurrent layer                 with dimensionality [None, n_tokens, n_hidden_list[-1]]             last_units: tensor of last hidden states for GRU and tuple                 of last hidden stated and last cell states for LSTM                 dimensionality of cell states and hidden states are                 similar and equal to [B x 2 * H], where B - batch                 size and H is number of hidden units
Stackted recurrent neural networks GRU or LSTM          Args:             units: a tensorflow tensor with dimensionality [None, n_tokens, n_features]             n_hidden_list: list with number of hidden units at the ouput of each layer             seq_lengths: length of sequences for different length sequences in batch                 can be None for maximum length as a length for every sample in the batch             cell_type: 'lstm' or 'gru'             use_peepholes: whether to use peephole connections (only 'lstm' case affected)             name: what variable_scope to use for the network parameters         Returns:             units: tensor at the output of the last recurrent layer                 with dimensionality [None, n_tokens, n_hidden_list[-1]]             last_units: tensor of last hidden states for GRU and tuple                 of last hidden stated and last cell states for LSTM                 dimensionality of cell states and hidden states are                 similar and equal to [B x 2 * H], where B - batch                 size and H is number of hidden units
Highway convolutional network. Skip connection with gating         mechanism.      Args:         units: a tensorflow tensor with dimensionality [None, n_tokens, n_features]         n_hidden_list: list with number of hidden units at the output of each layer         filter_width: width of the kernel in tokens         use_batch_norm: whether to use batch normalization between layers         use_dilation: use power of 2 dilation scheme [1, 2, 4, 8 .. ] for layers 1, 2, 3, 4 ...         training_ph: boolean placeholder determining whether is training phase now or not.             It is used only for batch normalization to determine whether to use             current batch average (std) or memory stored average (std)     Returns:         units: tensor at the output of the last convolutional layer                 with dimensionality [None, n_tokens, n_hidden_list[-1]]
Token embedding layer. Create matrix of for token embeddings.         Can be initialized with given matrix (for example pre-trained         with word2ve algorithm      Args:         token_indices: token indices tensor of type tf.int32         token_embedding_matrix: matrix of embeddings with dimensionality             [n_tokens, embeddings_dimension]         n_tokens: total number of unique tokens         token_embedding_dim: dimensionality of embeddings, typical 100..300         name: embedding matrix name (variable name)         trainable: whether to set the matrix trainable or not      Returns:         embedded_tokens: tf tensor of size [B, T, E], where B - batch size             T - number of tokens, E - token_embedding_dim
Fast CuDNN GRU implementation      Args:         units: tf.Tensor with dimensions [B x T x F], where             B - batch size             T - number of tokens             F - features          n_hidden: dimensionality of hidden state         trainable_initial_states: whether to create a special trainable variable             to initialize the hidden states of the network or use just zeros         seq_lengths: tensor of sequence lengths with dimension [B]         n_layers: number of layers         input_initial_h: initial hidden state, tensor         name: name of the variable scope to use         reuse:whether to reuse already initialized variable      Returns:         h - all hidden states along T dimension,             tf.Tensor with dimensionality [B x T x F]         h_last - last hidden state, tf.Tensor with dimensionality [B x H]
CuDNN Compatible GRU implementation.         It should be used to load models saved with CudnnGRUCell to run on CPU.          Args:             units: tf.Tensor with dimensions [B x T x F], where                 B - batch size                 T - number of tokens                 F - features              n_hidden: dimensionality of hidden state             trainable_initial_states: whether to create a special trainable variable                 to initialize the hidden states of the network or use just zeros             seq_lengths: tensor of sequence lengths with dimension [B]             n_layers: number of layers             input_initial_h: initial hidden state, tensor             name: name of the variable scope to use             reuse:whether to reuse already initialized variable          Returns:             h - all hidden states along T dimension,                 tf.Tensor with dimensionality [B x T x F]             h_last - last hidden state, tf.Tensor with dimensionality [B x H]
Fast CuDNN LSTM implementation          Args:             units: tf.Tensor with dimensions [B x T x F], where                 B - batch size                 T - number of tokens                 F - features             n_hidden: dimensionality of hidden state             n_layers: number of layers             trainable_initial_states: whether to create a special trainable variable                 to initialize the hidden states of the network or use just zeros             seq_lengths: tensor of sequence lengths with dimension [B]             initial_h: optional initial hidden state, masks trainable_initial_states                 if provided             initial_c: optional initial cell state, masks trainable_initial_states                 if provided             name: name of the variable scope to use             reuse:whether to reuse already initialized variable           Returns:             h - all hidden states along T dimension,                 tf.Tensor with dimensionality [B x T x F]             h_last - last hidden state, tf.Tensor with dimensionality [B x H]                 where H - number of hidden units             c_last - last cell state, tf.Tensor with dimensionality [B x H]                 where H - number of hidden units
CuDNN Compatible LSTM implementation.         It should be used to load models saved with CudnnLSTMCell to run on CPU.          Args:             units: tf.Tensor with dimensions [B x T x F], where                 B - batch size                 T - number of tokens                 F - features             n_hidden: dimensionality of hidden state             n_layers: number of layers             trainable_initial_states: whether to create a special trainable variable                 to initialize the hidden states of the network or use just zeros             seq_lengths: tensor of sequence lengths with dimension [B]             initial_h: optional initial hidden state, masks trainable_initial_states                 if provided             initial_c: optional initial cell state, masks trainable_initial_states                 if provided             name: name of the variable scope to use             reuse:whether to reuse already initialized variable           Returns:             h - all hidden states along T dimension,                 tf.Tensor with dimensionality [B x T x F]             h_last - last hidden state, tf.Tensor with dimensionality [B x H]                 where H - number of hidden units             c_last - last cell state, tf.Tensor with dimensionality [B x H]                 where H - number of hidden units
Fast CuDNN Bi-GRU implementation      Args:         units: tf.Tensor with dimensions [B x T x F], where             B - batch size             T - number of tokens             F - features         n_hidden: dimensionality of hidden state         seq_lengths: number of tokens in each sample in the batch         n_layers: number of layers         trainable_initial_states: whether to create a special trainable variable                 to initialize the hidden states of the network or use just zeros         name: name of the variable scope to use         reuse:whether to reuse already initialized variable       Returns:         h - all hidden states along T dimension,             tf.Tensor with dimensionality [B x T x F]         h_last - last hidden state, tf.Tensor with dimensionality [B x H * 2]             where H - number of hidden units
Fast CuDNN Bi-LSTM implementation      Args:         units: tf.Tensor with dimensions [B x T x F], where             B - batch size             T - number of tokens             F - features         n_hidden: dimensionality of hidden state         seq_lengths: number of tokens in each sample in the batch         n_layers: number of layers         trainable_initial_states: whether to create a special trainable variable             to initialize the hidden states of the network or use just zeros         name: name of the variable scope to use         reuse:whether to reuse already initialized variable      Returns:         h - all hidden states along T dimension,             tf.Tensor with dimensionality [B x T x F]         h_last - last hidden state, tf.Tensor with dimensionality [B x H * 2]             where H - number of hidden units         c_last - last cell state, tf.Tensor with dimensionality [B x H * 2]             where H - number of hidden units
Fast CuDNN Stacked Bi-GRU implementation      Args:         units: tf.Tensor with dimensions [B x T x F], where             B - batch size             T - number of tokens             F - features         n_hidden: dimensionality of hidden state         seq_lengths: number of tokens in each sample in the batch         n_stacks: number of stacked Bi-GRU         keep_prob: dropout keep_prob between Bi-GRUs (intra-layer dropout)         concat_stacked_outputs: return last Bi-GRU output or concat outputs from every Bi-GRU,         trainable_initial_states: whether to create a special trainable variable                 to initialize the hidden states of the network or use just zeros         name: name of the variable scope to use         reuse: whether to reuse already initialized variable       Returns:         h - all hidden states along T dimension,             tf.Tensor with dimensionality [B x T x ((n_hidden * 2) * n_stacks)]
Dropout with the same drop mask for all fixed_mask_dims      Args:         units: a tensor, usually with shapes [B x T x F], where             B - batch size             T - tokens dimension             F - feature dimension         keep_prob: keep probability         fixed_mask_dims: in these dimensions the mask will be the same      Returns:         dropped units tensor
Builds the network using Keras.
Builds word-level network
Creates the basic network architecture,         transforming word embeddings to intermediate outputs
Trains model on a single batch          Args:             data: a batch of word sequences             labels: a batch of correct tag sequences         Returns:             the trained model
Makes predictions on a single batch          Args:             data: a batch of word sequences together with additional inputs             return_indexes: whether to return tag indexes in vocabulary or tags themselves          Returns:             a batch of label sequences
Transforms a sentence to Numpy array, which will be the network input.          Args:             sent: input sentence             bucket_length: the width of the bucket          Returns:             A 3d array, answer[i][j][k] contains the index of k-th letter             in j-th word of i-th input sentence.
Transforms a sentence of tags to Numpy array, which will be the network target.          Args:             tags: input sentence of tags             bucket_length: the width of the bucket          Returns:             A 2d array, answer[i][j] contains the index of j-th tag in i-th input sentence.
Calculate BLEU score      Parameters:         y_true: list of reference tokens         y_predicted: list of query tokens         weights: n-gram weights         smoothing_function: SmoothingFunction         auto_reweigh: Option to re-normalize the weights uniformly         penalty: either enable brevity penalty or not      Return:         BLEU score
Verify signature certificate URL against Amazon Alexa requirements.      Each call of Agent passes incoming utterances batch through skills filter,     agent skills, skills processor. Batch of dialog IDs can be provided, in     other case utterances indexes in incoming batch are used as dialog IDs.      Args:         url: Signature certificate URL from SignatureCertChainUrl HTTP header.      Returns:         result: True if verification was successful, False if not.
Extracts pycrypto X509 objects from SSL certificates chain string.      Args:         certs_txt: SSL certificates chain string.      Returns:         result: List of pycrypto X509 objects.
Verifies if Amazon and additional certificates creates chain of trust to a root CA.      Args:         certs_chain: List of pycrypto X509 intermediate certificates from signature chain URL.         amazon_cert: Pycrypto X509 Amazon certificate.      Returns:         result: True if verification was successful, False if not.
Verifies Alexa request signature.      Args:         amazon_cert: Pycrypto X509 Amazon certificate.         signature: Base64 decoded Alexa request signature from Signature HTTP header.         request_body: full HTTPS request body     Returns:         result: True if verification was successful, False if not.
Conducts series of Alexa SSL certificate verifications against Amazon Alexa requirements.      Args:         signature_chain_url: Signature certificate URL from SignatureCertChainUrl HTTP header.     Returns:         result: Amazon certificate if verification was successful, None if not.
Returns list of json compatible states of the RichMessage instance         nested controls.          Returns:             json_controls: Json representation of RichMessage instance                 nested controls.
Returns list of MS Bot Framework compatible states of the         RichMessage instance nested controls.          Returns:             ms_bf_controls: MS Bot Framework representation of RichMessage instance                 nested controls.
Returns list of Telegram compatible states of the RichMessage         instance nested controls.          Returns:             telegram_controls: Telegram representation of RichMessage instance nested                 controls.
Returns list of Amazon Alexa compatible states of the RichMessage         instance nested controls.          Returns:             alexa_controls: Amazon Alexa representation of RichMessage instance nested                 controls.
DeepPavlov console configuration utility.
Constructs function encapsulated in the graph.
Constructs function encapsulated in the graph and the session.
Calculate accuracy in terms of absolute coincidence      Args:         y_true: array of true values         y_predicted: array of predicted values      Returns:         portion of absolutely coincidental samples
Rounds predictions and calculates accuracy in terms of absolute coincidence.      Args:         y_true: list of true values         y_predicted: list of predicted values      Returns:         portion of absolutely coincidental samples
We'll stub out all the initializers in the pretrained LM with     a function that loads the weights from the file
Reads a file from a path and returns data as a list of tuples of inputs and correct outputs          for every data type in ``train``, ``valid`` and ``test``.
Builds agent based on PatternMatchingSkill and HighestConfidenceSelector.      This is agent building tutorial. You can use this .py file to check how hello-bot agent works.      Returns:         agent: Agent capable of handling several simple greetings.
Takes an array of integers and transforms it     to an array of one-hot encoded vectors
Prettifies the dictionary of metrics.
Populate settings directory with default settings files      Args:         force: if ``True``, replace existing settings files with default ones      Returns:         ``True`` if any files were copied and ``False`` otherwise
Load model parameters from self.load_path
Save model parameters to self.save_path
Get train operation for given loss          Args:             loss: loss, tf tensor or scalar             learning_rate: scalar or placeholder.             clip_norm: clip gradients norm by clip_norm.             learnable_scopes: which scopes are trainable (None for all).             optimizer: instance of tf.train.Optimizer, default Adam.             **kwargs: parameters passed to tf.train.Optimizer object                (scalars or placeholders).          Returns:             train_op
Finds all dictionary words in d-window from word
sets 1.0 cost for every replacement, insertion, deletion and transposition
Initiates self-destruct timer.
Routes Alexa requests to appropriate handlers.          Args:             request: Alexa request.         Returns:             response: Response conforming Alexa response specification.
Infers DeepPavlov agent with raw user input extracted from Alexa request.          Args:             utterance: Raw user input extracted from Alexa request.         Returns:             response: DeepPavlov agent response.
Populates generated response with additional data conforming Alexa response specification.          Args:             response: Raw user input extracted from Alexa request.             request: Alexa request.         Returns:             response: Response conforming Alexa response specification.
Handles IntentRequest Alexa request.          Args:             request: Alexa request.         Returns:             response: "response" part of response dict conforming Alexa specification.
Handles LaunchRequest Alexa request.          Args:             request: Alexa request.         Returns:             response: "response" part of response dict conforming Alexa specification.
Handles all unsupported types of Alexa requests. Returns standard message.          Args:             request: Alexa request.         Returns:             response: "response" part of response dict conforming Alexa specification.
method that defines ``Struct``'s pretty printing rules for iPython          Args:             p (IPython.lib.pretty.RepresentationPrinter): pretty printer object             cycle (bool): is ``True`` if pretty detected a cycle
Calculates perplexity by loss      Args:         losses: list of numpy arrays of model losses      Returns:         perplexity : float
Build and return the model described in corresponding configuration file.
Start interaction with the model described in corresponding configuration file.
Make a prediction with the component described in corresponding configuration file.
Reads input file in CONLL-U format      Args:         infile: a path to a file         word_column: column containing words (default=1)         pos_column: column containing part-of-speech labels (default=3)         tag_column: column containing fine-grained tags (default=5)         max_sents: maximal number of sents to read         read_only_words: whether to read only words      Returns:         a list of sentences. Each item contains a word sequence and a tag sequence, which is ``None``         in case ``read_only_words = True``
Returns a function object with the name given in string.
Decorator for metric registration.
Returns a metric callable with a corresponding name.
Convert given string label of decay type to special index          Args:             label: name of decay type.                 Set of values: `"linear"`, `"cosine"`, `"exponential"`,                  `"onecycle"`, `"trapezoid"`, `["polynomial", K]`, where K is a polynomial power          Returns:             index of decay type
Find the best value according to given losses          Args:             values: list of considered values             losses: list of obtained loss values corresponding to `values`             max_loss_div: maximal divergence of loss to be considered significant             min_val_div: minimum divergence of loss to be considered significant          Returns:             best value divided by `min_val_div`
Embed one text sample          Args:             tokens: tokenized text sample             mean: whether to return mean embedding of tokens per sample          Returns:             list of embedded tokens or array of mean values
parses requirements from requirements.txt
Calculates log loss.      Args:         y_true: list or array of true values         y_predicted: list or array of predicted values      Returns:         Log loss
Exports a TF-Hub module
Format catalog item output      Parameters:         item_data: item's attributes values      Returns:         [rich_message]: list of formatted rich message
Make an agent      Returns:         agent: created Ecommerce agent
Parse parameters and run ms bot framework
Download a file from URL to one or several target locations      Args:         dest_file_path: path or list of paths to the file destination files (including file name)         source_url: the source URL         force_download: download file if it already exists, or not
Simple tar archive extractor      Args:         file_path: path to the tar file to be extracted         extract_folder: folder to which the files will be extracted
Download and extract .tar.gz or .gz file to one or several target locations.     The archive is deleted if extraction was successful.      Args:         url: URL for file downloading         download_path: path to the directory where downloaded file will be stored         until the end of extraction         extract_paths: path or list of paths where contents of archive will be extracted
Updates dict recursively      You need to use this function to update dictionary if depth of editing_dict is more then 1      Args:         editable_dict: dictionary, that will be edited         editing_dict: dictionary, that contains edits     Returns:         None
Given a file URL, return a md5 query of the file      Args:         url: a given URL     Returns:         URL of the md5 file
Given a URL, set or replace a query parameter and return the modified URL.      Args:         url: a given  URL         param_name: the parameter name to add         param_value: the parameter value     Returns:         URL with the added parameter
Returns Amazon Alexa compatible state of the PlainText instance.          Creating Amazon Alexa response blank with populated "outputSpeech" and         "card sections.          Returns:             response: Amazon Alexa representation of PlainText state.
Returns json compatible state of the Button instance.          Returns:             control_json: Json representation of Button state.
Returns MS Bot Framework compatible state of the Button instance.          Creates MS Bot Framework CardAction (button) with postBack value return.          Returns:             control_json: MS Bot Framework representation of Button state.
Returns json compatible state of the ButtonsFrame instance.          Returns json compatible state of the ButtonsFrame instance including         all nested buttons.          Returns:             control_json: Json representation of ButtonsFrame state.
Returns MS Bot Framework compatible state of the ButtonsFrame instance.          Creating MS Bot Framework activity blank with RichCard in "attachments". RichCard         is populated with CardActions corresponding buttons embedded in ButtonsFrame.          Returns:             control_json: MS Bot Framework representation of ButtonsFrame state.
Calculates F-1 score between y_true and y_predicted         F-1 score uses the best matching y_true answer      The same as in SQuAD-v2.0      Args:         y_true: list of correct answers (correct answers are represented by list of strings)         y_predicted: list of predicted answers      Returns:         F-1 score : float
Calculates recall at k ranking metric.      Args:         y_true: Labels. Not used in the calculation of the metric.         y_predicted: Predictions.             Each prediction contains ranking score of all ranking candidates for the particular data sample.             It is supposed that the ranking score for the true candidate goes first in the prediction.      Returns:         Recall at k
r"""Return True if at least one GPU is available
Recursively apply config's variables values to its property
Read config's variables and apply their values to all its properties
Convert relative paths to absolute with resolving user directory.
Builds and returns the Component from corresponding dictionary of parameters.
Thread run method implementation.
Deletes Conversation instance.          Args:             conversation_key: Conversation key.
Conducts cleanup of periodical certificates with expired validation.
Conducts series of Alexa request verifications against Amazon Alexa requirements.          Args:             signature_chain_url: Signature certificate URL from SignatureCertChainUrl HTTP header.             signature: Base64 decoded Alexa request signature from Signature HTTP header.             request_body: full HTTPS request body         Returns:             result: True if verification was successful, False if not.
Processes Alexa requests from skill server and returns responses to Alexa.          Args:             request: Dict with Alexa request payload and metadata.         Returns:             result: Alexa formatted or error response.
Returns a class object with the name given as a string.
Register classes that could be initialized from JSON configuration file.     If name is not passed, the class name is converted to snake-case.
Returns a registered class object with the name given in the string.
Extract full regularization path explored during lambda search from glm model.          :param model: source lambda search model
Create a custom GLM model using the given coefficients.          Needs to be passed source model trained on the dataset to extract the dataset information from.          :param model: source model, used for extracting dataset information         :param coefs: dictionary containing model coefficients         :param threshold: (optional, only for binomial) decision threshold used for classification
Create H2OCluster object from a list of key-value pairs.          TODO: This method should be moved into the base H2OResponse class.
Shut down the server.          This method checks if the H2O cluster is still running, and if it does shuts it down (via a REST API call).          :param prompt: A logical value indicating whether to prompt the user before shutting down the H2O server.
Determine if the H2O cluster is running or not.          :returns: True if the cluster is up; False otherwise
Print current cluster status information.          :param detailed: if True, then also print detailed information about each node.
List all jobs performed by the cluster.
Return the list of all known timezones.
Update information in this object from another H2OCluster instance.          :param H2OCluster other: source of the new information for this object.
Parameters for metalearner algorithm          Type: ``dict``  (default: ``None``).         Example: metalearner_gbm_params = {'max_depth': 2, 'col_sample_rate': 0.3}
Repeatedly test a function waiting for it to return True.          Arguments:         test_func      -- A function that will be run repeatedly         error          -- A function that will be run to produce an error message                           it will be called with (node, timeTakenSecs, numberOfRetries)                     OR                        -- A string that will be interpolated with a dictionary of                           { 'timeTakenSecs', 'numberOfRetries' }         timeoutSecs    -- How long in seconds to keep trying before declaring a failure         retryDelaySecs -- How long to wait between retry attempts
Return the summary for a single column for a single Frame in the h2o cluster.
Delete a frame on the h2o cluster, given its key.
Return a model builder or all of the model builders known to the     h2o cluster.  The model builders are contained in a dictionary     called "model_builders" at the top level of the result.  The     dictionary maps algorithm names to parameters lists.  Each of the     parameters contains all the metdata required by a client to     present a model building interface to the user.      if parameters = True, return the parameters?
Check a dictionary of model builder parameters on the h2o cluster      using the given algorithm and model parameters.
Score a model on the h2o cluster on the given Frame and return only the model metrics.
ModelMetrics list.
Delete a model on the h2o cluster, given its key.
Pretty tabulated string of all the cached data, and column names
Create a new reservation for count instances
terminate all the instances given by its ids
stop all the instances given by its ids
Start all the instances given by its ids
Reboot all the instances given by its ids
Wait for ssh service to appear on given hosts
Return fully qualified function name.      This method will attempt to find "full name" of the given function object. This full name is either of     the form "<class name>.<method name>" if the function is a class method, or "<module name>.<func name>"     if it's a regular function. Thus, this is an attempt to back-port func.__qualname__ to Python 2.      :param func: a function object.      :returns: string with the function's full name as explained above.
Given a frame and a compiled function code, find the corresponding function object within the frame.      This function addresses the following problem: when handling a stacktrace, we receive information about     which piece of code was being executed in the form of a CodeType object. That objects contains function name,     file name, line number, and the compiled bytecode. What it *doesn't* contain is the function object itself.      So this utility function aims at locating this function object, and it does so by searching through objects     in the preceding local frame (i.e. the frame where the function was called from). We expect that the function     should usually exist there -- either by itself, or as a method on one of the objects.      :param types.FrameType frame: local frame where the function ought to be found somewhere.     :param types.CodeType code: the compiled code of the function to look for.      :returns: the function object, or None if not found.
Return function's declared arguments as a string.      For example for this function it returns "func, highlight=None"; for the ``_wrap`` function it returns     "text, wrap_at=120, indent=4". This should usually coincide with the function's declaration (the part     which is inside the parentheses).
Return piece of text, wrapped around if needed.      :param text: text that may be too long and then needs to be wrapped.     :param wrap_at: the maximum line length.     :param indent: number of spaces to prepend to all subsequent lines after the first.
Wait until job's completion.
Train the H2O model.          :param x: A list of column names or indices indicating the predictor columns.         :param y: An index or a column name indicating the response column.         :param H2OFrame training_frame: The H2OFrame having the columns indicated by x and y (as well as any             additional columns specified by fold, offset, and weights).         :param offset_column: The name or index of the column in training_frame that holds the offsets.         :param fold_column: The name or index of the column in training_frame that holds the per-row fold             assignments.         :param weights_column: The name or index of the column in training_frame that holds the per-row weights.         :param validation_frame: H2OFrame with validation data to be scored on while training.         :param float max_runtime_secs: Maximum allowed runtime in seconds for model training. Use 0 to disable.         :param bool verbose: Print scoring history to stdout. Defaults to False.
Fit an H2O model as part of a scikit-learn pipeline or grid search.          A warning will be issued if a caller other than sklearn attempts to use this method.          :param H2OFrame X: An H2OFrame consisting of the predictor variables.         :param H2OFrame y: An H2OFrame consisting of the response variable.         :param params: Extra arguments.         :returns: The current instance of H2OEstimator for method chaining.
Obtain parameters for this estimator.          Used primarily for sklearn Pipelines and sklearn grid search.          :param deep: If True, return parameters of all sub-objects that are estimators.          :returns: A dict of parameters
Helper function to handle caught signals.
Clear the output directory.
This function is written to remove sandbox directories if they exist under the     parent_dir.      :param parent_dir: string denoting full parent directory path     :param dir_name: string denoting directory path which could be a sandbox     :return: None
Look at the stdout log and figure out which port the JVM chose.          If successful, port number is stored in self.port; otherwise the         program is terminated. This call is blocking, and will wait for         up to 30s for the server to start up.
Look at the stdout log and wait until the cluster of proper size is formed.         This call is blocking.         Exit if this fails.          :param nodes_per_cloud:         :return none
Normal node shutdown.         Ignore failures for now.          :return none
Normal cluster shutdown.          :return none
Return an ip to use to talk to this cluster.
Return a port to use to talk to this cluster.
Return the coordinates of the ROC curve for a given set of data.          The coordinates are two-tuples containing the false positive rates as a list and true positive rates as a list.         If all are False (default), then return is the training data. If more than one ROC         curve is requested, the data is returned as a dictionary of two-tuples.          :param bool train: If True, return the ROC value for the training data.         :param bool valid: If True, return the ROC value for the validation data.         :param bool xval: If True, return the ROC value for each of the cross-validated splits.          :returns: The ROC values for the specified key(s).
Determines vec_size for a pre-trained model after basic model verification.
Mean absolute error regression loss.      :param y_actual: H2OFrame of actual response.     :param y_predicted: H2OFrame of predicted response.     :param weights: (Optional) sample weights     :returns: mean absolute error loss (best is 0.0).
Mean squared error regression loss      :param y_actual: H2OFrame of actual response.     :param y_predicted: H2OFrame of predicted response.     :param weights: (Optional) sample weights     :returns: mean squared error loss (best is 0.0).
Median absolute error regression loss      :param y_actual: H2OFrame of actual response.     :param y_predicted: H2OFrame of predicted response.     :returns: median absolute error loss (best is 0.0)
Explained variance regression score function.      :param y_actual: H2OFrame of actual response.     :param y_predicted: H2OFrame of predicted response.     :param weights: (Optional) sample weights     :returns: the explained variance score.
Assert that the argument has the specified type.      This function is used to check that the type of the argument is correct, otherwises it raises an H2OTypeError.     See more details in the module's help.      :param var: variable to check     :param types: the expected types     :param kwargs:         message: override the error message         skip_frames: how many local frames to skip when printing out the error.      :raises H2OTypeError: if the argument is not of the desired type.
Assert that string variable matches the provided regular expression.      :param v: variable to check.     :param regex: regular expression to check against (can be either a string, or compiled regexp).
Assert that variable satisfies the provided condition.      :param v: variable to check. Its value is only used for error reporting.     :param bool cond: condition that must be satisfied. Should be somehow related to the variable ``v``.     :param message: message string to use instead of the default.
Magic variable name retrieval.      This function is designed as a helper for assert_is_type() function. Typically such assertion is used like this::          assert_is_type(num_threads, int)      If the variable `num_threads` turns out to be non-integer, we would like to raise an exception such as          H2OTypeError("`num_threads` is expected to be integer, but got <str>")      and in order to compose an error message like that, we need to know that the variables that was passed to     assert_is_type() carries a name "num_threads". Naturally, the variable itself knows nothing about that.      This is where this function comes in: we walk up the stack trace until the first frame outside of this     file, find the original line that called the assert_is_type() function, and extract the variable name from     that line. This is slightly fragile, in particular we assume that only one assert_is_type statement can be per line,     or that this statement does not spill over multiple lines, etc.
Return True if the variable is of the specified type, and False otherwise.      :param var: variable to check     :param vtype: expected variable's type
Return the name of the provided type.          _get_type_name(int) == "integer"         _get_type_name(str) == "string"         _get_type_name(tuple) == "tuple"         _get_type_name(Exception) == "Exception"         _get_type_name(U(int, float, bool)) == "integer|float|bool"         _get_type_name(U(H2OFrame, None)) == "?H2OFrame"
Attempt to find the source code of the ``lambda_fn`` within the string ``src``.
Return True if the variable does not match any of the types, and False otherwise.
Check whether the provided value is a valid enum constant.
Retrieve the config as a dictionary of key-value pairs.
Find and parse config file, storing all variables in ``self._config``.
Return possible locations for the .h2oconfig file, one at a time.
Start the progress bar, and return only when the progress reaches 100%.          :param progress_fn: the executor function (or a generator). This function should take no arguments             and return either a single number -- the current progress level, or a tuple (progress level, delay),             where delay is the time interval for when the progress should be checked again. This function may at             any point raise the ``StopIteration(message)`` exception, which will interrupt the progress bar,             display the ``message`` in red font, and then re-raise the exception.         :raises StopIteration: if the job is interrupted. The reason for interruption is provided in the exception's             message. The message will say "cancelled" if the job was interrupted by the user by pressing Ctrl+C.
Save the current model progress into ``self._progress_data``, and update ``self._next_poll_time``.          :param res: tuple (progress level, poll delay).         :param now: current timestamp.
Compute t0, x0, v0, ve.
Estimate the moment when the underlying process is expected to reach completion.          This function should only return future times. Also this function is not allowed to return time moments less         than self._next_poll_time if the actual progress is below 100% (this is because we won't know that the         process have finished until we poll the external progress function).
Determine when to query the progress status next.          This function is used if the external progress function did not return time interval for when it should be         queried next.
Calculate the modelled progress state for the given time moment.          :returns: tuple (x, v) of the progress level and progress speed.
Return the projected time when progress level `x_target` will be reached.          Since the underlying progress model is nonlinear, we need to do use Newton method to find a numerical solution         to the equation x(t) = x_target.
Print the rendered string to the stdout.
Initial rendering stage, done in order to compute widths of all widgets.
Find current STDOUT's width, in characters.
Inform the widget about the encoding of the underlying character stream.
Returns encoding map as an object that maps 'column_name' -> 'frame_with_encoding_map_for_this_column_name'          :param frame frame: An H2OFrame object with which to create the target encoding map
Retrieve an existing H2OFrame from the H2O cluster using the frame's id.          :param str frame_id: id of the frame to retrieve         :param int rows: number of rows to fetch for preview (10 by default)         :param int rows_offset: offset to fetch rows from (0 by default)         :param int cols: number of columns to fetch (all by default)         :param full_cols: number of columns to fetch together with backed data         :param int cols_offset: offset to fetch rows from (0 by default)         :param bool light: wether to use light frame endpoint or not         :returns: an existing H2OFrame with the id provided; or None if such frame doesn't exist.
Reload frame information from the backend H2O server.
The type for the given column.          :param col: either a name, or an index of the column to look up         :returns: type of the column, one of: ``str``, ``int``, ``real``, ``enum``, ``time``, ``bool``.         :raises H2OValueError: if such column does not exist in the frame.
Extract columns of the specified type from the frame.          :param str coltype: A character string indicating which column type to filter by. This must be             one of the following:              - ``"numeric"``      - Numeric, but not categorical or time             - ``"categorical"``  - Integer, with a categorical/factor String mapping             - ``"string"``       - String column             - ``"time"``         - Long msec since the Unix Epoch - with a variety of display/parse options             - ``"uuid"``         - UUID             - ``"bad"``          - No none-NA rows (triple negative! all NAs or zero rows)          :returns: list of indices of columns that have the requested type
Display summary information about the frame.          Summary includes min/mean/max/sigma and other rollup data.          :param bool return_data: Return a dictionary of the summary output
Generate an in-depth description of this H2OFrame.          This will print to the console the dimensions of the frame; names/types/summary statistics for each column;         and finally first ten rows of the frame.          :param bool chunk_summary: Retrieve the chunk summary along with the distribution summary
Return the first ``rows`` and ``cols`` of the frame as a new H2OFrame.          :param int rows: maximum number of rows to return         :param int cols: maximum number of columns to return         :returns: a new H2OFrame cut from the top left corner of the current frame, and having dimensions at             most ``rows`` x ``cols``.
Multiply this frame, viewed as a matrix, by another matrix.          :param matrix: another frame that you want to multiply the current frame by; must be compatible with the             current frame (i.e. its number of rows must be the same as number of columns in the current frame).         :returns: new H2OFrame, which is the result of multiplying the current frame by ``matrix``.
Get the factor levels.          :returns: A list of lists, one list per column, of levels.
Get the number of factor levels for each categorical column.          :returns: A list of the number of levels per column.
A method to set all column values to one of the levels.          :param str level: The level at which the column will be set (a string)          :returns: H2OFrame with entries set to the desired level.
Replace the levels of a categorical column.          New levels must be aligned with the old domain. This call has copy-on-write semantics.          :param List[str] levels: A list of strings specifying the new levels. The number of new             levels must match the number of old levels.         :returns: A single-column H2OFrame with the desired levels.
Change names of columns in the frame.          Dict key is an index or name of the column whose name is to be set.         Dict value is the new name of the column.          :param columns: dict-like transformations to apply to the column names
Change names of all columns in the frame.          :param List[str] names: The list of new names for every column in the frame.
Set a new name for a column.          :param col: index or name of the column whose name is to be set; may be skipped for 1-column frames         :param name: the new name of the column
Test whether elements of an H2OFrame are contained in the ``item``.          :param items: An item or a list of items to compare the H2OFrame against.          :returns: An H2OFrame of 0s and 1s showing whether each element in the original H2OFrame is contained in item.
Build a fold assignments column for cross-validation.          Rows are assigned a fold according to the current row number modulo ``n_folds``.          :param int n_folds: An integer specifying the number of validation sets to split the training data into.         :returns: A single-column H2OFrame with the fold assignments.
Build a fold assignment column with the constraint that each fold has the same class         distribution as the fold column.          :param int n_folds: The number of folds to build.         :param int seed: A seed for the random number generator.          :returns: A single column H2OFrame with the fold assignments.
Compactly display the internal structure of an H2OFrame.
Obtain the dataset as a python-local object.          :param bool use_pandas: If True (default) then return the H2OFrame as a pandas DataFrame (requires that the             ``pandas`` library was installed). If False, then return the contents of the H2OFrame as plain nested             list, in a row-wise order.         :param bool header: If True (default), then column names will be appended as the first row in list          :returns: A python object (a list of lists of strings, each list is a row, if use_pandas=False, otherwise             a pandas DataFrame) containing this H2OFrame instance's data.
Pop a column from the H2OFrame at index i.          :param i: The index (int) or name (str) of the column to pop.         :returns: an H2OFrame containing the column dropped from the current frame; the current frame is modified             in-place and loses the column.
Compute quantiles.          :param List[float] prob: list of probabilities for which quantiles should be computed.         :param str combine_method: for even samples this setting determines how to combine quantiles. This can be             one of ``"interpolate"``, ``"average"``, ``"low"``, ``"high"``.         :param weights_column: optional weights for each row. If not given, all rows are assumed to have equal             importance. This parameter can be either the name of column containing the observation weights in             this frame, or a single-column separate H2OFrame of observation weights.          :returns: a new H2OFrame containing the quantiles and probabilities.
Append multiple H2OFrames to this frame, column-wise or row-wise.          :param List[H2OFrame] frames: list of frames that should be appended to the current frame.         :param int axis: if 1 then append column-wise (default), if 0 then append row-wise.          :returns: an H2OFrame of the combined datasets.
Append data to this frame column-wise.          :param H2OFrame data: append columns of frame ``data`` to the current frame. You can also cbind a number,             in which case it will get converted into a constant column.          :returns: new H2OFrame with all frames in ``data`` appended column-wise.
Append data to this frame row-wise.          :param data: an H2OFrame or a list of H2OFrame's to be combined with current frame row-wise.         :returns: this H2OFrame with all frames in data appended row-wise.
Split a frame into distinct subsets of size determined by the given ratios.          The number of subsets is always 1 more than the number of ratios given. Note that         this does not give an exact split. H2O is designed to be efficient on big data         using a probabilistic splitting method rather than an exact split. For example         when specifying a split of 0.75/0.25, H2O will produce a test/train split with         an expected value of 0.75/0.25 rather than exactly 0.75/0.25. On small datasets,         the sizes of the resulting splits will deviate from the expected value more than         on big data, where they will be very close to exact.          :param List[float] ratios: The fractions of rows for each split.         :param List[str] destination_frames: The names of the split frames.         :param int seed: seed for the random number generator          :returns: A list of H2OFrames
Return a new ``GroupBy`` object using this frame and the desired grouping columns.          The returned groups are sorted by the natural group-by column sort.          :param by: The columns to group on (either a single column name, or a list of column names, or             a list of column indices).
Return a new Frame that fills NA along a given axis and along a given direction with a maximum fill length          :param method: ``"forward"`` or ``"backward"``         :param axis:  0 for columnar-wise or 1 for row-wise fill         :param maxlen: Max number of consecutive NA's to fill                  :return:
Impute missing values into the frame, modifying it in-place.          :param int column: Index of the column to impute, or -1 to impute the entire frame.         :param str method: The method of imputation: ``"mean"``, ``"median"``, or ``"mode"``.         :param str combine_method: When the method is ``"median"``, this setting dictates how to combine quantiles             for even samples. One of ``"interpolate"``, ``"average"``, ``"low"``, ``"high"``.         :param by: The list of columns to group on.         :param H2OFrame group_by_frame: Impute the values with this pre-computed grouped frame.         :param List values: The list of impute values, one per column. None indicates to skip the column.          :returns: A list of values used in the imputation or the group-by result used in imputation.
Merge two datasets based on common column names.  We do not support all_x=True and all_y=True.         Only one can be True or none is True.  The default merge method is auto and it will default to the         radix method.  The radix method will return the correct merge result regardless of duplicated rows          in the right frame.  In addition, the radix method can perform merge even if you have string columns          in your frames.  If there are duplicated rows in your rite frame, they will not be included if you use         the hash method.  The hash method cannot perform merge if you have string columns in your left frame.         Hence, we consider the radix method superior to the hash method and is the default method to use.          :param H2OFrame other: The frame to merge to the current one. By default, must have at least one column in common with             this frame, and all columns in common are used as the merge key.  If you want to use only a subset of the             columns in common, rename the other columns so the columns are unique in the merged result.         :param bool all_x: If True, include all rows from the left/self frame         :param bool all_y: If True, include all rows from the right/other frame         :param by_x: list of columns in the current frame to use as a merge key.         :param by_y: list of columns in the ``other`` frame to use as a merge key. Should have the same number of             columns as in the ``by_x`` list.         :param method: string representing the merge method, one of auto(default), radix or hash.          :returns: New H2OFrame with the result of merging the current frame with the ``other`` frame.
Reorder levels of an H2O factor for one single column of a H2O frame          The levels of a factor are reordered such that the reference level is at level 0, all remaining levels are         moved down as needed.          :param str y: The reference level         :returns: New reordered factor column
Insert missing values into the current frame, modifying it in-place.          Randomly replaces a user-specified fraction of entries in a H2O dataset with missing         values.          :param float fraction: A number between 0 and 1 indicating the fraction of entries to replace with missing.         :param int seed: The seed for the random number generator used to determine which values to make missing.          :returns: the original H2OFrame with missing values inserted.
Compute the variance-covariance matrix of one or two H2OFrames.          :param H2OFrame y: If this parameter is given, then a covariance  matrix between the columns of the target             frame and the columns of ``y`` is computed. If this parameter is not provided then the covariance matrix             of the target frame is returned. If target frame has just a single column, then return the scalar variance             instead of the matrix. Single rows are treated as single columns.         :param str use: A string indicating how to handle missing values. This could be one of the following:              - ``"everything"``: outputs NaNs whenever one of its contributing observations is missing             - ``"all.obs"``: presence of missing observations will throw an error             - ``"complete.obs"``: discards missing values along with all observations in their rows so that only               complete observations are used         :param bool na_rm: an alternative to ``use``: when this is True then default value for ``use`` is             ``"everything"``; and if False then default ``use`` is ``"complete.obs"``. This parameter has no effect             if ``use`` is given explicitly.          :returns: An H2OFrame of the covariance matrix of the columns of this frame (if ``y`` is not given),             or with the columns of ``y`` (if ``y`` is given). However when this frame and ``y`` are both single rows             or single columns, then the variance is returned as a scalar.
Compute the correlation matrix of one or two H2OFrames.          :param H2OFrame y: If this parameter is provided, then compute correlation between the columns of ``y``             and the columns of the current frame. If this parameter is not given, then just compute the correlation             matrix for the columns of the current frame.         :param str use: A string indicating how to handle missing values. This could be one of the following:              - ``"everything"``: outputs NaNs whenever one of its contributing observations is missing             - ``"all.obs"``: presence of missing observations will throw an error             - ``"complete.obs"``: discards missing values along with all observations in their rows so that only               complete observations are used         :param bool na_rm: an alternative to ``use``: when this is True then default value for ``use`` is             ``"everything"``; and if False then default ``use`` is ``"complete.obs"``. This parameter has no effect             if ``use`` is given explicitly.          :returns: An H2OFrame of the correlation matrix of the columns of this frame (if ``y`` is not given),             or with the columns of ``y`` (if ``y`` is given). However when this frame and ``y`` are both single rows             or single columns, then the correlation is returned as a scalar.
Compute a pairwise distance measure between all rows of two numeric H2OFrames.          :param H2OFrame y: Frame containing queries (small)         :param str use: A string indicating what distance measure to use. Must be one of:              - ``"l1"``:        Absolute distance (L1-norm, >=0)             - ``"l2"``:        Euclidean distance (L2-norm, >=0)             - ``"cosine"``:    Cosine similarity (-1...1)             - ``"cosine_sq"``: Squared Cosine similarity (0...1)          :examples:           >>>           >>> iris_h2o = h2o.import_file(path=pyunit_utils.locate("smalldata/iris/iris.csv"))           >>> references = iris_h2o[10:150,0:4           >>> queries    = iris_h2o[0:10,0:4]           >>> A = references.distance(queries, "l1")           >>> B = references.distance(queries, "l2")           >>> C = references.distance(queries, "cosine")           >>> D = references.distance(queries, "cosine_sq")           >>> E = queries.distance(references, "l1")           >>> (E.transpose() == A).all()          :returns: An H2OFrame of the matrix containing pairwise distance / similarity between the              rows of this frame (N x p) and ``y`` (M x p), with dimensions (N x M).
Convert columns in the current frame to categoricals.          :returns: new H2OFrame with columns of the "enum" type.
Split the strings in the target column on the given regular expression pattern.          :param str pattern: The split pattern.         :returns: H2OFrame containing columns of the split strings.
For each string in the frame, count the occurrences of the provided pattern.  If countmathces is applied to         a frame, all columns of the frame must be type string, otherwise, the returned frame will contain errors.          The pattern here is a plain string, not a regular expression. We will search for the occurrences of the         pattern as a substring in element of the frame. This function is applicable to frames containing only         string or categorical columns.          :param str pattern: The pattern to count matches on in each string. This can also be a list of strings,             in which case all of them will be searched for.         :returns: numeric H2OFrame with the same shape as the original, containing counts of matches of the             pattern for each cell in the original frame.
For each string, return a new string that is a substring of the original string.          If end_index is not specified, then the substring extends to the end of the original string. If the start_index         is longer than the length of the string, or is greater than or equal to the end_index, an empty string is         returned. Negative start_index is coerced to 0.          :param int start_index: The index of the original string at which to start the substring, inclusive.         :param int end_index: The index of the original string at which to end the substring, exclusive.         :returns: An H2OFrame containing the specified substrings.
Return a copy of the column with leading characters removed.          The set argument is a string specifying the set of characters to be removed.         If omitted, the set argument defaults to removing whitespace.          :param character set: The set of characters to lstrip from strings in column.         :returns: a new H2OFrame with the same shape as the original frame and having all its values             trimmed from the left (equivalent of Python's ``str.lstrip()``).
For each string compute its Shannon entropy, if the string is empty the entropy is 0.          :returns: an H2OFrame of Shannon entropies.
For each string, find the count of all possible substrings with 2 characters or more that are contained in         the line-separated text file whose path is given.          :param str path_to_words: Path to file that contains a line-separated list of strings considered valid.         :returns: An H2OFrame with the number of substrings that are contained in the given word list.
Compute the counts of values appearing in a column, or co-occurence counts between two columns.          :param H2OFrame data2: An optional single column to aggregate counts by.         :param bool dense: If True (default) then use dense representation, which lists only non-zero counts,             1 combination per row. Set to False to expand counts across all combinations.          :returns: H2OFrame of the counts at each combination of factor levels
Compute a histogram over a numeric column.          :param breaks: Can be one of ``"sturges"``, ``"rice"``, ``"sqrt"``, ``"doane"``, ``"fd"``, ``"scott"``;             or a single number for the number of breaks; or a list containing the split points, e.g:             ``[-50, 213.2123, 9324834]``. If breaks is "fd", the MAD is used over the IQR in computing bin width.         :param bool plot: If True (default), then a plot will be generated using ``matplotlib``.          :returns: If ``plot`` is False, return H2OFrame with these columns: breaks, counts, mids_true,             mids, and density; otherwise this method draws a plot and returns nothing.
Compute the iSAX index for DataFrame which is assumed to be numeric time series data.          References:              - http://www.cs.ucr.edu/~eamonn/SAX.pdf             - http://www.cs.ucr.edu/~eamonn/iSAX_2.0.pdf          :param int num_words: Number of iSAX words for the timeseries, i.e. granularity along the time series         :param int max_cardinality: Maximum cardinality of the iSAX word. Each word can have less than the max         :param bool optimized_card: An optimization flag that will find the max cardinality regardless of what is             passed in for ``max_cardinality``.          :returns: An H2OFrame with the name of time series, string representation of iSAX word, followed by             binary representation.
Substitute the first occurrence of pattern in a string with replacement.          :param str pattern: A regular expression.         :param str replacement: A replacement string.         :param bool ignore_case: If True then pattern will match case-insensitively.         :returns: an H2OFrame with all values matching ``pattern`` replaced with ``replacement``.
Translate characters from lower to upper case for a particular column.          :returns: new H2OFrame with all strings in the current frame converted to the uppercase.
Searches for matches to argument `pattern` within each element         of a string column.          Default behavior is to return indices of the elements matching the pattern. Parameter         `output_logical` can be used to return a logical vector indicating if the element matches         the pattern (1) or not (0).          :param str pattern: A character string containing a regular expression.         :param bool ignore_case: If True, then case is ignored during matching.         :param bool invert:  If True, then identify elements that do not match the pattern.         :param bool output_logical: If True, then return logical vector of indicators instead of list of matching positions         :return: H2OFrame holding the matching positions or a logical list if `output_logical` is enabled.
Remove rows with NAs from the H2OFrame.          :returns: new H2OFrame with all rows from the original frame containing any NAs removed.
Conduct a diff-1 transform on a numeric frame column.          :returns: an H2OFrame where each element is equal to the corresponding element in the source             frame minus the previous-row element in the same frame.
For each element in an H2OFrame, determine if it is NA or not.          :returns: an H2OFrame of 1s and 0s, where 1s mean the values were NAs.
Extract the "minute" part from a date column.          :returns: a single-column H2OFrame containing the "minute" part from the source frame.
Generate a column of random numbers drawn from a uniform distribution [0,1) and         having the same data layout as the source frame.          :param int seed: seed for the random number generator.          :returns: Single-column H2OFrame filled with doubles sampled uniformly from [0,1).
Construct a column that can be used to perform a random stratified split.          :param float test_frac: The fraction of rows that will belong to the "test".         :param int seed: The seed for the random number generator.          :returns: an H2OFrame having single categorical column with two levels: ``"train"`` and ``"test"``.          :examples:           >>> stratsplit = df["y"].stratified_split(test_frac=0.3, seed=12349453)           >>> train = df[stratsplit=="train"]           >>> test = df[stratsplit=="test"]           >>>           >>> # check that the distributions among the initial frame, and the           >>> # train/test frames match           >>> df["y"].table()["Count"] / df["y"].table()["Count"].sum()           >>> train["y"].table()["Count"] / train["y"].table()["Count"].sum()           >>> test["y"].table()["Count"] / test["y"].table()["Count"].sum()
Cut a numeric vector into categorical "buckets".          This method is only applicable to a single-column numeric frame.          :param List[float] breaks: The cut points in the numeric vector.         :param List[str] labels: Labels for categorical levels produced. Defaults to set notation of             intervals defined by the breaks.         :param bool include_lowest: By default, cuts are defined as intervals ``(lo, hi]``. If this parameter             is True, then the interval becomes ``[lo, hi]``.         :param bool right: Include the high value: ``(lo, hi]``. If False, get ``(lo, hi)``.         :param int dig_lab: Number of digits following the decimal point to consider.          :returns: Single-column H2OFrame of categorical data.
Get the index of the max value in a column or row          :param bool skipna: If True (default), then NAs are ignored during the search. Otherwise presence             of NAs renders the entire result NA.         :param int axis: Direction of finding the max index. If 0 (default), then the max index is searched columnwise, and the             result is a frame with 1 row and number of columns as in the original frame. If 1, then the max index is searched             rowwise and the result is a frame with 1 column, and number of rows equal to the number of rows in the original frame.         :returns: either a list of max index values per-column or an H2OFrame containing max index values                   per-row from the original frame.
Apply a lambda expression to an H2OFrame.          :param fun: a lambda expression to be applied per row or per column.         :param axis: 0 = apply to each column; 1 = apply to each row         :returns: a new H2OFrame with the results of applying ``fun`` to the current frame.
Parse code from a string of text.
Parse the provided file, and return Code object.
Move the token by `drow` rows and `dcol` columns.
Convert the parsed representation back into the source code.
Get the sizes of each cluster.          If all are False (default), then return the training metric value.         If more than one options is set to True, then return a dictionary of metrics where         the keys are "train", "valid", and "xval".          :param bool train: If True, return the cluster sizes for the training data.         :param bool valid: If True, return the cluster sizes for the validation data.         :param bool xval: If True, return the cluster sizes for each of the cross-validated splits.          :returns: The cluster sizes for the specified key(s).
The centers for the KMeans model.
The standardized centers for the kmeans model.
Connect to an existing H2O server, remote or local.      There are two ways to connect to a server: either pass a `server` parameter containing an instance of     an H2OLocalServer, or specify `ip` and `port` of the server that you want to connect to.      :param server: An H2OLocalServer instance to connect to (optional).     :param url: Full URL of the server to connect to (can be used instead of `ip` + `port` + `https`).     :param ip: The ip address (or host name) of the server where H2O is running.     :param port: Port number that H2O service is listening to.     :param https: Set to True to connect via https:// instead of http://.     :param verify_ssl_certificates: When using https, setting this to False will disable SSL certificates verification.     :param auth: Either a (username, password) pair for basic authentication, an instance of h2o.auth.SpnegoAuth                  or one of the requests.auth authenticator objects.     :param proxy: Proxy server address.     :param cookies: Cookie (or list of) to add to request     :param verbose: Set to False to disable printing connection status messages.     :param connection_conf: Connection configuration object encapsulating connection parameters.     :returns: the new :class:`H2OConnection` object.
Perform a REST API request to a previously connected server.      This function is mostly for internal purposes, but may occasionally be useful for direct access to     the backend H2O server. It has same parameters as :meth:`H2OConnection.request <h2o.backend.H2OConnection.request>`.
Used to verify that h2o-python module and the H2O server are compatible with each other.
Import a single file or collection of files.      :param path: A path to a data file (remote or local).     :param pattern: Character string containing a regular expression to match file(s) in the folder.     :returns: either a :class:`H2OFrame` with the content of the provided file, or a list of such frames if         importing multiple files.
Upload a dataset from the provided local path to the H2O cluster.      Does a single-threaded push to H2O. Also see :meth:`import_file`.      :param path: A path specifying the location of the data to upload.     :param destination_frame:  The unique hex key assigned to the imported file. If none is given, a key will         be automatically generated.     :param header: -1 means the first line is data, 0 means guess, 1 means first line is header.     :param sep: The field separator character. Values on each line of the file are separated by         this character. If not provided, the parser will automatically detect the separator.     :param col_names: A list of column names for the file.     :param col_types: A list of types or a dictionary of column names to types to specify whether columns         should be forced to a certain type upon import parsing. If a list, the types for elements that are         one will be guessed. The possible types a column may have are:          - "unknown" - this will force the column to be parsed as all NA         - "uuid"    - the values in the column must be true UUID or will be parsed as NA         - "string"  - force the column to be parsed as a string         - "numeric" - force the column to be parsed as numeric. H2O will handle the compression of the numeric           data in the optimal manner.         - "enum"    - force the column to be parsed as a categorical column.         - "time"    - force the column to be parsed as a time column. H2O will attempt to parse the following           list of date time formats: (date) "yyyy-MM-dd", "yyyy MM dd", "dd-MMM-yy", "dd MMM yy", (time)           "HH:mm:ss", "HH:mm:ss:SSS", "HH:mm:ss:SSSnnnnnn", "HH.mm.ss" "HH.mm.ss.SSS", "HH.mm.ss.SSSnnnnnn".           Times can also contain "AM" or "PM".     :param na_strings: A list of strings, or a list of lists of strings (one list per column), or a dictionary         of column names to strings which are to be interpreted as missing values.     :param skipped_columns: an integer lists of column indices to skip and not parsed into the final frame from the import file.      :returns: a new :class:`H2OFrame` instance.      :examples:         >>> frame = h2o.upload_file("/path/to/local/data")
Import a dataset that is already on the cluster.      The path to the data must be a valid path for each node in the H2O cluster. If some node in the H2O cluster     cannot see the file, then an exception will be thrown by the H2O cluster. Does a parallel/distributed     multi-threaded pull of the data. The main difference between this method and :func:`upload_file` is that     the latter works with local files, whereas this method imports remote files (i.e. files local to the server).     If you running H2O server on your own maching, then both methods behave the same.      :param path: path(s) specifying the location of the data to import or a path to a directory of files to import     :param destination_frame: The unique hex key assigned to the imported file. If none is given, a key will be         automatically generated.     :param parse: If True, the file should be parsed after import. If False, then a list is returned containing the file path.     :param header: -1 means the first line is data, 0 means guess, 1 means first line is header.     :param sep: The field separator character. Values on each line of the file are separated by         this character. If not provided, the parser will automatically detect the separator.     :param col_names: A list of column names for the file.     :param col_types: A list of types or a dictionary of column names to types to specify whether columns         should be forced to a certain type upon import parsing. If a list, the types for elements that are         one will be guessed. The possible types a column may have are:          - "unknown" - this will force the column to be parsed as all NA         - "uuid"    - the values in the column must be true UUID or will be parsed as NA         - "string"  - force the column to be parsed as a string         - "numeric" - force the column to be parsed as numeric. H2O will handle the compression of the numeric           data in the optimal manner.         - "enum"    - force the column to be parsed as a categorical column.         - "time"    - force the column to be parsed as a time column. H2O will attempt to parse the following           list of date time formats: (date) "yyyy-MM-dd", "yyyy MM dd", "dd-MMM-yy", "dd MMM yy", (time)           "HH:mm:ss", "HH:mm:ss:SSS", "HH:mm:ss:SSSnnnnnn", "HH.mm.ss" "HH.mm.ss.SSS", "HH.mm.ss.SSSnnnnnn".           Times can also contain "AM" or "PM".     :param na_strings: A list of strings, or a list of lists of strings (one list per column), or a dictionary         of column names to strings which are to be interpreted as missing values.     :param pattern: Character string containing a regular expression to match file(s) in the folder if `path` is a         directory.     :param skipped_columns: an integer list of column indices to skip and not parsed into the final frame from the import file.     :param custom_non_data_line_markers: If a line in imported file starts with any character in given string it will NOT be imported. Empty string means all lines are imported, None means that default behaviour for given format will be used      :returns: a new :class:`H2OFrame` instance.      :examples:         >>> # Single file import         >>> iris = import_file("h2o-3/smalldata/iris.csv")         >>> # Return all files in the folder iris/ matching the regex r"iris_.*\.csv"         >>> iris_pattern = h2o.import_file(path = "h2o-3/smalldata/iris",         ...                                pattern = "iris_.*\.csv")
Import Hive table to H2OFrame in memory.      Make sure to start H2O with Hive on classpath. Uses hive-site.xml on classpath to connect to Hive.      :param database: Name of Hive database (default database will be used by default)     :param table: name of Hive table to import     :param partitions: a list of lists of strings - partition key column values of partitions you want to import.     :param allow_multi_format: enable import of partitioned tables with different storage formats used. WARNING:         this may fail on out-of-memory for tables with a large number of small partitions.      :returns: an :class:`H2OFrame` containing data of the specified Hive table.      :examples:         >>> my_citibike_data = h2o.import_hive_table("default", "table", [["2017", "01"], ["2017", "02"]])
Import SQL table to H2OFrame in memory.      Assumes that the SQL table is not being updated and is stable.     Runs multiple SELECT SQL queries concurrently for parallel ingestion.     Be sure to start the h2o.jar in the terminal with your downloaded JDBC driver in the classpath::          java -cp <path_to_h2o_jar>:<path_to_jdbc_driver_jar> water.H2OApp      Also see :func:`import_sql_select`.     Currently supported SQL databases are MySQL, PostgreSQL, MariaDB, Hive, Oracle and Microsoft SQL.      :param connection_url: URL of the SQL database connection as specified by the Java Database Connectivity (JDBC)         Driver. For example, "jdbc:mysql://localhost:3306/menagerie?&useSSL=false"     :param table: name of SQL table     :param columns: a list of column names to import from SQL table. Default is to import all columns.     :param username: username for SQL server     :param password: password for SQL server     :param optimize: DEPRECATED. Ignored - use fetch_mode instead. Optimize import of SQL table for faster imports.     :param fetch_mode: Set to DISTRIBUTED to enable distributed import. Set to SINGLE to force a sequential read by a single node         from the database.      :returns: an :class:`H2OFrame` containing data of the specified SQL table.      :examples:         >>> conn_url = "jdbc:mysql://172.16.2.178:3306/ingestSQL?&useSSL=false"         >>> table = "citibike20k"         >>> username = "root"         >>> password = "abc123"         >>> my_citibike_data = h2o.import_sql_table(conn_url, table, username, password)
Import the SQL table that is the result of the specified SQL query to H2OFrame in memory.      Creates a temporary SQL table from the specified sql_query.     Runs multiple SELECT SQL queries on the temporary table concurrently for parallel ingestion, then drops the table.     Be sure to start the h2o.jar in the terminal with your downloaded JDBC driver in the classpath::        java -cp <path_to_h2o_jar>:<path_to_jdbc_driver_jar> water.H2OApp      Also see h2o.import_sql_table. Currently supported SQL databases are MySQL, PostgreSQL, MariaDB, Hive, Oracle      and Microsoft SQL Server.      :param connection_url: URL of the SQL database connection as specified by the Java Database Connectivity (JDBC)         Driver. For example, "jdbc:mysql://localhost:3306/menagerie?&useSSL=false"     :param select_query: SQL query starting with `SELECT` that returns rows from one or more database tables.     :param username: username for SQL server     :param password: password for SQL server     :param optimize: DEPRECATED. Ignored - use fetch_mode instead. Optimize import of SQL table for faster imports.     :param use_temp_table: whether a temporary table should be created from select_query     :param temp_table_name: name of temporary table to be created from select_query     :param fetch_mode: Set to DISTRIBUTED to enable distributed import. Set to SINGLE to force a sequential read by a single node         from the database.      :returns: an :class:`H2OFrame` containing data of the specified SQL query.      :examples:         >>> conn_url = "jdbc:mysql://172.16.2.178:3306/ingestSQL?&useSSL=false"         >>> select_query = "SELECT bikeid from citibike20k"         >>> username = "root"         >>> password = "abc123"         >>> my_citibike_data = h2o.import_sql_select(conn_url, select_query,         ...                                          username, password, fetch_mode)
Parse dataset using the parse setup structure.      :param setup: Result of ``h2o.parse_setup()``     :param id: an id for the frame.     :param first_line_is_header: -1, 0, 1 if the first line is to be used as the header      :returns: an :class:`H2OFrame` object.
Create a deep clone of the frame ``data``.      :param data: an H2OFrame to be cloned     :param xid: (internal) id to be assigned to the new frame.     :returns: new :class:`H2OFrame` which is the clone of the passed frame.
Load a model from the server.      :param model_id: The model identification in H2O      :returns: Model object, a subclass of H2OEstimator
Return the specified grid.      :param grid_id: The grid identification in h2o      :returns: an :class:`H2OGridSearch` instance.
Obtain a handle to the frame in H2O with the frame_id key.      :param str frame_id: id of the frame to retrieve.     :returns: an :class:`H2OFrame` object
Download the POJO for this model to the directory specified by path; if path is "", then dump to screen.      :param model: the model whose scoring POJO should be retrieved.     :param path: an absolute path to the directory where POJO should be saved.     :param get_jar: retrieve the h2o-genmodel.jar also (will be saved to the same folder ``path``).     :param jar_name: Custom name of genmodel jar.     :returns: location of the downloaded POJO file.
Download an H2O data set to a CSV file on the local disk.      Warning: Files located on the H2O server may be very large! Make sure you have enough     hard drive space to accommodate the entire file.      :param data: an H2OFrame object to be downloaded.     :param filename: name for the CSV file where the data should be saved to.
Download H2O log files to disk.      :param dirname: a character string indicating the directory that the log file should be saved in.     :param filename: a string indicating the name that the CSV file should be. Note that the saved format is .zip, so the file name must include the .zip extension.      :returns: path of logs written in a zip file.      :examples: The following code will save the zip file `'autoh2o_log.zip'` in a directory that is one down from where you are currently working into a directory called `your_directory_name`. (Please note that `your_directory_name` should be replaced with the name of the directory that you've created and that already exists.)          >>> h2o.download_all_logs(dirname='./your_directory_name/', filename = 'autoh2o_log.zip')
Export a given H2OFrame to a path on the machine this python session is currently connected to.      :param frame: the Frame to save to disk.     :param path: the path to the save point on disk.     :param force: if True, overwrite any preexisting file with the same path     :param parts: enables export to multiple 'part' files instead of just a single file.         Convenient for large datasets that take too long to store in a single file.         Use parts=-1 to instruct H2O to determine the optimal number of part files or         specify your desired maximum number of part files. Path needs to be a directory         when exporting to multiple files, also that directory must be empty.         Default is ``parts = 1``, which is to export to a single file.
Convert an H2O data object into a python-specific object.      WARNING! This will pull all data local!      If Pandas is available (and use_pandas is True), then pandas will be used to parse the     data frame. Otherwise, a list-of-lists populated by character data will be returned (so     the types of data will all be str).      :param data: an H2O data object.     :param use_pandas: If True, try to use pandas for reading in the data.     :param header: If True, return column names as first element in list      :returns: List of lists (Rows x Columns).
H2O built-in demo facility.      :param funcname: A string that identifies the h2o python function to demonstrate.     :param interactive: If True, the user will be prompted to continue the demonstration after every segment.     :param echo: If True, the python commands that are executed will be displayed.     :param test: If True, `h2o.init()` will not be called (used for pyunit testing).      :example:         >>> import h2o         >>> h2o.demo("gbm")
Imports a data file within the 'h2o_data' folder.
Create Model Metrics from predicted and actual values in H2O.      :param H2OFrame predicted: an H2OFrame containing predictions.     :param H2OFrame actuals: an H2OFrame containing actual values.     :param domain: list of response factors for classification.     :param distribution: distribution for regression.
Upload given file into DKV and save it under give key as raw object.      :param dest_key:  name of destination key in DKV     :param file_path:  path to file to upload     :return: key name if object was uploaded successfully
Upload given metrics function into H2O cluster.      The metrics can have different representation:       - class: needs to implement map(pred, act, weight, offset, model), reduce(l, r) and metric(l) methods       - string: the same as in class case, but the class is given as a string      :param func:  metric representation: string, class     :param func_file:  internal name of file to save given metrics representation     :param func_name:  name for h2o key under which the given metric is saved     :param class_name: name of class wrapping the metrics function (when supplied as string)     :param source_provider: a function which provides a source code for given function     :return: reference to uploaded metrics function      :examples:         >>> class CustomMaeFunc:         >>>     def map(self, pred, act, w, o, model):         >>>         return [abs(act[0] - pred[0]), 1]         >>>         >>>     def reduce(self, l, r):         >>>         return [l[0] + r[0], l[1] + r[1]]         >>>         >>>     def metric(self, l):         >>>         return l[0] / l[1]         >>>         >>>         >>> h2o.upload_custom_metric(CustomMaeFunc, func_name="mae")         >>>         >>> custom_func_str = '''class CustomMaeFunc:         >>>     def map(self, pred, act, w, o, model):         >>>         return [abs(act[0] - pred[0]), 1]         >>>         >>>     def reduce(self, l, r):         >>>         return [l[0] + r[0], l[1] + r[1]]         >>>         >>>     def metric(self, l):         >>>         return l[0] / l[1]'''         >>>         >>>         >>> h2o.upload_custom_metric(custom_func_str, class_name="CustomMaeFunc", func_name="mae")
Check that the provided frame id is valid in Rapids language.
Convert given number of bytes into a human readable representation, i.e. add prefix such as kb, Mb, Gb,     etc. The `size` argument must be a non-negative integer.      :param size: integer representing byte size of something     :return: string representation of the size, in human-readable form
Return a "canonical" version of slice ``s``.      :param slice s: the original slice expression     :param total int: total number of elements in the collection sliced by ``s``     :return slice: a slice equivalent to ``s`` but not containing any negative indices or Nones.
Return True if slice ``s`` in "normalized" form.
MOJO scoring function to take a Pandas frame and use MOJO model as zip file to score.      :param dataframe: Pandas frame to score.     :param mojo_zip_path: Path to MOJO zip downloaded from H2O.     :param genmodel_jar_path: Optional, path to genmodel jar file. If None (default) then the h2o-genmodel.jar in the same         folder as the MOJO zip will be used.     :param classpath: Optional, specifies custom user defined classpath which will be used when scoring. If None         (default) then the default classpath for this MOJO model will be used.     :param java_options: Optional, custom user defined options for Java. By default ``-Xmx4g`` is used.     :param verbose: Optional, if True, then additional debug information will be printed. False by default.     :return: Pandas frame with predictions
MOJO scoring function to take a CSV file and use MOJO model as zip file to score.      :param input_csv_path: Path to input CSV file.     :param mojo_zip_path: Path to MOJO zip downloaded from H2O.     :param output_csv_path: Optional, name of the output CSV file with computed predictions. If None (default), then         predictions will be saved as prediction.csv in the same folder as the MOJO zip.     :param genmodel_jar_path: Optional, path to genmodel jar file. If None (default) then the h2o-genmodel.jar in the same         folder as the MOJO zip will be used.     :param classpath: Optional, specifies custom user defined classpath which will be used when scoring. If None         (default) then the default classpath for this MOJO model will be used.     :param java_options: Optional, custom user defined options for Java. By default ``-Xmx4g -XX:ReservedCodeCacheSize=256m`` is used.     :param verbose: Optional, if True, then additional debug information will be printed. False by default.     :return: List of computed predictions
The decorator to mark deprecated functions.
Wait until grid finishes computing.
Obtain a hidden layer's details on a dataset.          :param test_data: Data to create a feature space on.         :param int layer: Index of the hidden layer.         :returns: A dictionary of hidden layer details for each model.
Print a detailed summary of the explored models.
Print models sorted by metric.
Get the hyperparameters of a model explored by grid search.          :param str id: The model id of the model with hyperparameters of interest.         :param bool display: Flag to indicate whether to display the hyperparameter names.          :returns: A list of the hyperparameters for the specified model.
Derived and returned the model parameters used to train the particular grid search model.          :param str id: The model id of the model with hyperparameters of interest.         :param bool display: Flag to indicate whether to display the hyperparameter names.          :returns: A dict of model pararmeters derived from the hyper-parameters used to train this particular model.
Retrieve an H2OGridSearch instance.          Optionally specify a metric by which to sort models and a sort order.         Note that if neither cross-validation nor a validation frame is used in the grid search, then the         training metrics will display in the "get grid" output. If a validation frame is passed to the grid, and         ``nfolds = 0``, then the validation metrics will display. However, if ``nfolds`` > 1, then cross-validation         metrics will display even if a validation frame is provided.          :param str sort_by: A metric by which to sort the models in the grid space. Choices are: ``"logloss"``,             ``"residual_deviance"``, ``"mse"``, ``"auc"``, ``"r2"``, ``"accuracy"``, ``"precision"``, ``"recall"``,             ``"f1"``, etc.         :param bool decreasing: Sort the models in decreasing order of metric if true, otherwise sort in increasing             order (default).          :returns: A new H2OGridSearch instance optionally sorted on the specified metric.
Get the F1 values for a set of thresholds for the models explored.          If all are False (default), then return the training metric value.         If more than one options is set to True, then return a dictionary of metrics where         the keys are "train", "valid", and "xval".          :param List thresholds: If None, then the thresholds in this set of metrics will be used.         :param bool train: If True, return the F1 value for the training data.         :param bool valid: If True, return the F1 value for the validation data.         :param bool xval: If True, return the F1 value for each of the cross-validated splits.         :returns: Dictionary of model keys to F1 values
Return the Importance of components associcated with a pca model.          use_pandas: ``bool``  (default: ``False``).
Convert archetypes of the model into original feature space.          :param H2OFrame test_data: The dataset upon which the model was trained.         :param bool reverse_transform: Whether the transformation of the training data during model-building             should be reversed on the projected archetypes.          :returns: model archetypes projected back into the original training data's feature space.
Produce the scree plot.          Library ``matplotlib`` is required for this function.          :param str type: either ``"barplot"`` or ``"lines"``.
Convert names with underscores into camelcase.      For example:         "num_rows" => "numRows"         "very_long_json_name" => "veryLongJsonName"         "build_GBM_model" => "buildGbmModel"         "KEY" => "key"         "middle___underscores" => "middleUnderscores"         "_exclude_fields" => "_excludeFields" (retain initial/trailing underscores)         "__http_status__" => "__httpStatus__"      :param name: name to be converted
Dedent text to the specific indentation level.      :param ind: common indentation level for the resulting text (number of spaces to append to every line)     :param text: text that should be transformed.     :return: ``text`` with all common indentation removed, and then the specified amount of indentation added.
This function will extract the various operation time for GLRM model building iterations.      :param javaLogText:     :return:
Main program.  Take user input, parse it and call other functions to execute the commands     and extract run summary and store run result in json file      @return: none
Close an existing connection; once closed it cannot be used again.          Strictly speaking it is not necessary to close all connection that you opened -- we have several mechanisms         in place that will do so automatically (__del__(), __exit__() and atexit() handlers), however there is also         no good reason to make this method private.
Return the session id of the current connection.          The session id is issued (through an API request) the first time it is requested, but no sooner. This is         because generating a session id puts it into the DKV on the server, which effectively locks the cluster. Once         issued, the session id will stay the same until the connection is closed.
Start logging all API requests to the provided destination.          :param dest: Where to write the log: either a filename (str), or an open file handle (file). If not given,             then a new temporary file will be created.
Make a copy of the `data` object, preparing it to be sent to the server.          The data will be sent via x-www-form-urlencoded or multipart/form-data mechanisms. Both of them work with         plain lists of key/value pairs, so this method converts the data into such format.
Prepare `filename` to be sent to the server.          The "preparation" consists of creating a data structure suitable         for passing to requests.request().
Log the beginning of an API request.
Log response from an API request.
Log the message `msg` to the destination `self._logging_dest`.          If this destination is a file name, then we append the message to the file and then close the file         immediately. If the destination is an open file handle, then we simply write the message there and do not         attempt to close it.
Given a response object, prepare it to be handed over to the external caller.          Preparation steps include:            * detect if the response has error status, and convert it to an appropriate exception;            * detect Content-Type, and based on that either parse the response as JSON or return as plain text.
Helper function to print connection status messages when in verbose mode.
Retrieve information about an AutoML instance.      :param str project_name:  A string indicating the project_name of the automl instance to retrieve.     :returns: A dictionary containing the project_name, leader model, and leaderboard.
Download the POJO for the leader model in AutoML to the directory specified by path.          If path is an empty string, then dump the output to screen.          :param path:  An absolute path to the directory where POJO should be saved.         :param get_genmodel_jar: if True, then also download h2o-genmodel.jar and store it in folder ``path``.         :param genmodel_name Custom name of genmodel jar         :returns: name of the POJO file written.
Download the leader model in AutoML in MOJO format.          :param path: the path where MOJO file should be saved.         :param get_genmodel_jar: if True, then also download h2o-genmodel.jar and store it in folder ``path``.         :param genmodel_name Custom name of genmodel jar         :returns: name of the MOJO file written.
Fit this object by computing the means and standard deviations used by the transform method.          :param X: An H2OFrame; may contain NAs and/or categoricals.         :param y: None (Ignored)         :param params: Ignored         :returns: This H2OScaler instance
Scale an H2OFrame with the fitted means and standard deviations.          :param X: An H2OFrame; may contain NAs and/or categoricals.         :param y: None (Ignored)         :param params: (Ignored)         :returns: A scaled H2OFrame.
Undo the scale transformation.          :param X: An H2OFrame; may contain NAs and/or categoricals.         :param y: None (Ignored)         :param params: (Ignored)         :returns: An H2OFrame
remove extra characters before the actual string we are     looking for.  The Jenkins console output is encoded using utf-8.  However, the stupid     redirect function can only encode using ASCII.  I have googled for half a day with no     results to how to resolve the issue.  Hence, we are going to the heat and just manually     get rid of the junk.      Parameters     ----------      string_content :  str         contains a line read in from jenkins console      :return: str: contains the content of the line after the string '[0m'
Find the slave machine where a Jenkins job was executed on.  It will save this     information in g_failed_test_info_dict.  In addition, it will     delete this particular function handle off the temp_func_list as we do not need     to perform this action again.      Parameters     ----------      each_line :  str         contains a line read in from jenkins console     temp_func_list :  list of Python function handles         contains a list of functions that we want to invoke to extract information from         the Jenkins console text.      :return: bool to determine if text mining should continue on the jenkins console text
Find the git hash and branch info that  a Jenkins job was taken from.  It will save this     information in g_failed_test_info_dict.  In addition, it will delete this particular     function handle off the temp_func_list as we do not need to perform this action again.      Parameters     ----------      each_line :  str         contains a line read in from jenkins console     temp_func_list :  list of Python function handles         contains a list of functions that we want to invoke to extract information from         the Jenkins console text.      :return: bool to determine if text mining should continue on the jenkins console text
Find if a Jenkins job has taken too long to finish and was killed.  It will save this     information in g_failed_test_info_dict.      Parameters     ----------      each_line :  str         contains a line read in from jenkins console     temp_func_list :  list of Python function handles         contains a list of functions that we want to invoke to extract information from         the Jenkins console text.      :return: bool to determine if text mining should continue on the jenkins console text
Find if a Jenkins job has failed to build.  It will save this     information in g_failed_test_info_dict.  In addition, it will delete this particular     function handle off the temp_func_list as we do not need to perform this action again.      Parameters     ----------      each_line :  str         contains a line read in from jenkins console     temp_func_list :  list of Python function handles         contains a list of functions that we want to invoke to extract information from         the Jenkins console text.      :return: bool to determine if text mining should continue on the jenkins console text
Find the build id of a jenkins job.  It will save this     information in g_failed_test_info_dict.  In addition, it will delete this particular     function handle off the temp_func_list as we do not need to perform this action again.      Parameters     ----------      each_line :  str         contains a line read in from jenkins console     temp_func_list :  list of Python function handles         contains a list of functions that we want to invoke to extract information from         the Jenkins console text.      :return: bool to determine if text mining should continue on the jenkins console text
From user input, grab the jenkins job name and saved it in g_failed_test_info_dict.     In addition, it will grab the jenkins url and the view name into g_jenkins_url, and     g_view_name.      Parameters     ----------     url_string :  str         contains information on the jenkins job whose console output we are interested in.      :return: none
scan through the java output text and extract the bad java messages that may or may not happened when     unit tests are run.  It will not record any bad java messages that are stored in g_ok_java_messages.      :return: none
Save the log scraping results into logs denoted by g_output_filename_failed_tests and     g_output_filename_passed_tests.      :return: none
Concatecate all log file into a summary text file to be sent to users     at the end of a daily log scraping.      :return: none
Write one log file into the summary text file.      Parameters     ----------     fhandle :  Python file handle         file handle to the summary text file     file2read : Python file handle         file handle to log file where we want to add its content to the summary text file.      :return: none
Loop through all java messages that are not associated with a unit test and     write them into a log file.      Parameters     ----------     key :  str         9.general_bad_java_messages     val : list of list of str         contains the bad java messages and the message types.       :return: none
Load in pickle file that contains dict structure with bad java messages to ignore per unit test     or for all cases.  The ignored bad java info is stored in g_ok_java_messages dict.      :return:
Return enum constant `s` converted to a canonical snake-case.
Find synonyms using a word2vec model.          :param str word: A single word to find synonyms for.         :param int count: The first "count" synonyms will be returned.          :returns: the approximate reconstruction of the training data.
Wait until the job finishes.          This method will continuously query the server about the status of the job, until the job reaches a         completion. During this time we will display (in stdout) a progress bar with % completion status.
Convert the munging operations performed on H2OFrame into a POJO.          :param pojo_name:  (str) Name of POJO         :param path:  (str) path of POJO.         :param get_jar: (bool) Whether to also download the h2o-genmodel.jar file needed to compile the POJO         :return: None
To perform the munging operations on a frame specified in steps on the frame fr.          :param fr: H2OFrame where munging operations are to be performed on.         :return: H2OFrame after munging operations are completed.
Find the percentile of a list of values.      @parameter N - is a list of values. Note N MUST BE already sorted.     @parameter percent - a float value from 0.0 to 1.0.     @parameter key - optional key function to compute value from each element of N.      @return - the percentile of the values
Dictionary of the default parameters of the model.
Dictionary of actual parameters of the model.
Return hidden layer details.          :param test_data: Data to create a feature space on         :param layer: 0 index hidden layer
Retrieve Model Score History.          :returns: The score history as an H2OTwoDimTable or a Pandas DataFrame.
Print innards of model, without regards to type.
Pretty print the variable importances, or return them in a list.          :param use_pandas: If True, then the variable importances will be returned as a pandas data frame.          :returns: A list or Pandas DataFrame.
Retreive the residual degress of freedom if this model has the attribute, or None otherwise.          :param bool train: Get the residual dof for the training set. If both train and valid are False, then train             is selected by default.         :param bool valid: Get the residual dof for the validation set. If both train and valid are True, then train             is selected by default.          :returns: Return the residual dof, or None if it is not present.
Return the coefficients which can be applied to the non-standardized data.          Note: standardize = True by default, if set to False then coef() return the coefficients which are fit directly.
Download the POJO for this model to the directory specified by path.          If path is an empty string, then dump the output to screen.          :param path:  An absolute path to the directory where POJO should be saved.         :param get_genmodel_jar: if True, then also download h2o-genmodel.jar and store it in folder ``path``.         :param genmodel_name Custom name of genmodel jar         :returns: name of the POJO file written.
Download the model in MOJO format.          :param path: the path where MOJO file should be saved.         :param get_genmodel_jar: if True, then also download h2o-genmodel.jar and store it in folder ``path``.         :param genmodel_name Custom name of genmodel jar         :returns: name of the MOJO file written.
Save Model Details of an H2O Model in JSON Format to disk.          :param model: The model object to save.         :param path: a path to save the model details at (hdfs, s3, local)         :param force: if True overwrite destination directory in case it exists, or throw exception if set to False.          :returns str: the path of the saved model details
Check that y_actual and y_predicted have the same length.          :param H2OFrame y_actual:         :param H2OFrame y_predicted:          :returns: None
Obtain a list of cross-validation models.          :returns: list of H2OModel objects.
GBM model demo.
Deep Learning model demo.
GLM model demo.
Wait for a key press on the console and return it.      Borrowed from http://stackoverflow.com/questions/983354/how-do-i-make-python-to-wait-for-a-pressed-key
Convert to a python 'data frame'.
Print the contents of this table.
Start new H2O server on the local machine.          :param jar_path: Path to the h2o.jar executable. If not given, then we will search for h2o.jar in the             locations returned by `._jar_paths()`.         :param nthreads: Number of threads in the thread pool. This should be related to the number of CPUs used.             -1 means use all CPUs on the host. A positive integer specifies the number of CPUs directly.         :param enable_assertions: If True, pass `-ea` option to the JVM.         :param max_mem_size: Maximum heap size (jvm option Xmx), in bytes.         :param min_mem_size: Minimum heap size (jvm option Xms), in bytes.         :param log_dir: Directory for H2O logs to be stored if a new instance is started. Default directory is determined         by H2O internally.         :param log_level: The logger level for H2O if a new instance is started.         :param ice_root: A directory where H2O stores its temporary files. Default location is determined by             tempfile.mkdtemp().         :param port: Port where to start the new server. This could be either an integer, or a string of the form             "DDDDD+", indicating that the server should start looking for an open port starting from DDDDD and up.         :param name: name of the h2o cluster to be started         :param extra_classpath List of paths to libraries that should be included on the Java classpath.         :param verbose: If True, then connection info will be printed to the stdout.         :param jvm_custom_args Custom, user-defined arguments for the JVM H2O is instantiated in         :param bind_to_localhost A flag indicating whether access to the H2O instance should be restricted to the local             machine (default) or if it can be reached from other computers on the network.             Only applicable when H2O is started from the Python client.          :returns: a new H2OLocalServer instance
Return the location of an h2o.jar executable.          :param path0: Explicitly given h2o.jar path. If provided, then we will simply check whether the file is there,             otherwise we will search for an executable in locations returned by ._jar_paths().          :raises H2OStartupError: if no h2o.jar executable can be found.
Produce potential paths for an h2o.jar executable.
Retrieve the Hit Ratios.          If all are False (default), then return the training metric value.         If more than one options is set to True, then return a dictionary of metrics where the keys are "train",         "valid", and "xval".          :param train: If train is True, then return the hit ratio value for the training data.         :param valid: If valid is True, then return the hit ratio value for the validation data.         :param xval:  If xval is True, then return the hit ratio value for the cross validation data.         :return: The hit ratio for this regression model.
Equivalent of csv.DictWriter, but allows `delimiter` to be a unicode string on Py2.
Convert uri to absolute filepath          Parameters         ----------         uri : string             URI of python module to return path for          Returns         -------         path : None or string             Returns None if there is no valid path for this URI             Otherwise returns absolute file system path for URI          Examples         --------         >>> docwriter = ApiDocWriter('sphinx')         >>> import sphinx         >>> modpath = sphinx.__path__[0]         >>> res = docwriter._uri2path('sphinx.builder')         >>> res == os.path.join(modpath, 'builder.py')         True         >>> res = docwriter._uri2path('sphinx')         >>> res == os.path.join(modpath, '__init__.py')         True         >>> docwriter._uri2path('sphinx.does_not_exist')
Convert directory path to uri
Parse lines of text for functions and classes
Make autodoc documentation template string for a module          Parameters         ----------         uri : string             python location of module - e.g 'sphinx.builder'          Returns         -------         S : string             Contents of API doc
Return module sequence discovered from ``self.package_name``            Parameters         ----------         None          Returns         -------         mods : sequence             Sequence of module names within ``self.package_name``          Examples         --------         >>> dw = ApiDocWriter('sphinx')         >>> mods = dw.discover_modules()         >>> 'sphinx.util' in mods         True         >>> dw.package_skip_patterns.append('\.util$')         >>> 'sphinx.util' in dw.discover_modules()         False         >>>
Generate API reST files.          Parameters         ----------         outdir : string             Directory name in which to store files             We create automatic filenames for each module                      Returns         -------         None          Notes         -----         Sets self.written_modules to list of written modules
Make a reST API index file from written files          Parameters         ----------         path : string             Filename to write index to         outdir : string             Directory to which to write generated index file         froot : string, optional             root (filename without extension) of filename to write to             Defaults to 'gen'.  We add ``self.rst_extension``.         relative_to : string             path to which written filenames are relative.  This             component of the written file path will be removed from             outdir, in the generated index.  Default is None, meaning,             leave path as it is.
Convert this confusion matrix into a 2x2 plain list of values.
Load java messages that can be ignored pickle file into a dict structure g_ok_java_messages.      :return: none
Add new java messages to ignore from user text file.  It first reads in the new java ignored messages     from the user text file and generate a dict structure to out of the new java ignored messages.  This     is achieved by function extract_message_to_dict.  Next, new java messages will be added to the original     ignored java messages dict g_ok_java_messages.  Again, this is achieved by function update_message_dict.      :return: none
Update the g_ok_java_messages dict structure by     1. add the new java ignored messages stored in message_dict if action == 1     2. remove the java ignored messages stired in message_dict if action == 2.      Parameters     ----------      message_dict :  Python dict       key: unit test name or "general"       value: list of java messages that are to be ignored if they are found when running the test stored as the key.  If         the key is "general", the list of java messages are to be ignored when running all tests.     action : int       if 1: add java ignored messages stored in message_dict to g_ok_java_messages dict;       if 2: remove java ignored messages stored in message_dict from g_ok_java_messages dict.      :return: none
Read in a text file that java messages to be ignored and generate a dictionary structure out of     it with key and value pairs.  The keys are test names and the values are lists of java message     strings associated with that test name where we are either going to add to the existing java messages     to ignore or remove them from g_ok_java_messages.      Parameters     ----------      filename :  Str        filename that contains ignored java messages.  The text file shall contain something like this:         keyName = general         Message = nfolds: nfolds cannot be larger than the number of rows (406).         KeyName = pyunit_cv_cars_gbm.py         Message = Caught exception: Illegal argument(s) for GBM model: GBM_model_python_1452503348770_2586.  \             Details: ERRR on field: _nfolds: nfolds must be either 0 or >1.         ...      :return:     message_dict : dict         contains java message to be ignored with key as unit test name or "general" and values as list of ignored java         messages.
Save the ignored java message dict stored in g_ok_java_messages into a pickle file for future use.      :return: none
Write the java ignored messages in g_ok_java_messages into a text file for humans to read.      :return: none
Parse user inputs and set the corresponing global variables to perform the     necessary tasks.      Parameters     ----------      argv : string array         contains flags and input options from users      :return:
Illustrate what the various input flags are and the options should be.      :return: none
Find all python files in the given directory and all subfolders.
Search the file for any magic incantations.      :param filename: file to search     :returns: a tuple containing the spell and then maybe some extra words (or None if no magic present)
Executed when script is run as-is.
Transform H2OFrame using a MOJO Pipeline.          :param data: Frame to be transformed.         :param allow_timestamps: Allows datetime columns to be used directly with MOJO pipelines. It is recommended         to parse your datetime columns as Strings when using pipelines because pipelines can interpret certain datetime         formats in a different way. If your H2OFrame is parsed from a binary file format (eg. Parquet) instead of CSV         it is safe to turn this option on and use datetime columns directly.          :returns: A new H2OFrame.
This function will look at the local directory and pick out files that have the correct start name and     summarize the results into one giant dict.      :return: None
This function will print out the intermittents onto the screen for casual viewing.  It will also print out     where the giant summary dictionary is going to be stored.      :return: None
Produce the desired metric plot.          :param type: the type of metric plot (currently, only ROC supported).         :param server: if True, generate plot inline using matplotlib's "Agg" backend.         :returns: None
Get the confusion matrix for the specified metric          :param metrics: A string (or list of strings) among metrics listed in :const:`max_metrics`. Defaults to 'f1'.         :param thresholds: A value (or list of values) between 0 and 1.         :returns: a list of ConfusionMatrix objects (if there are more than one to return), or a single ConfusionMatrix             (if there is only one).
Returns True if a deep water model can be built, or False otherwise.
This method will remove data from the summary text file and the dictionary file for tests that occurs before     the number of months specified by monthToKeep.      :param monthToKeep:     :return:
Return endpoints, grouped by the class which handles them.
Set site domain and name.
Adds the default_data to data and dumps it to a json.
Comments last user_id's medias
Returns login and password stored in `secret.txt`.
Likes last user_id's medias
Likes last medias from hashtag
Filter bot from real users.
Reads list from file. One line - one item.         Returns the list if file items.
Add a specific enqueue time to the message.          :param schedule_time: The scheduled time to enqueue the message.         :type schedule_time: ~datetime.datetime
Defer the message.          This message will remain in the queue but must be received         specifically by its sequence number in order to be processed.          :raises: ~azure.servicebus.common.errors.MessageAlreadySettled if the message has been settled.         :raises: ~azure.servicebus.common.errors.MessageLockExpired if message lock has already expired.         :raises: ~azure.servicebus.common.errors.SessionLockExpired if session lock has already expired.         :raises: ~azure.servicebus.common.errors.MessageSettleFailed if message settle operation fails.
Gives the sas-url to download the configurations for vpn-sites in a         resource group.          :param resource_group_name: The resource group name.         :type resource_group_name: str         :param virtual_wan_name: The name of the VirtualWAN for which          configuration of all vpn-sites is needed.         :type virtual_wan_name: str         :param vpn_sites: List of resource-ids of the vpn-sites for which          config is to be downloaded.         :type vpn_sites:          list[~azure.mgmt.network.v2018_04_01.models.SubResource]         :param output_blob_sas_url: The sas-url to download the configurations          for vpn-sites         :type output_blob_sas_url: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns None or          ClientRawResponse<None> if raw==True         :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]         :raises:          :class:`ErrorException<azure.mgmt.network.v2018_04_01.models.ErrorException>`
Guess Python Autorest options based on the spec path.      Expected path:     specification/compute/resource-manager/readme.md
Updates a running PowerShell command with more data.          :param resource_group_name: The resource group name uniquely          identifies the resource group within the user subscriptionId.         :type resource_group_name: str         :param node_name: The node name (256 characters maximum).         :type node_name: str         :param session: The sessionId from the user.         :type session: str         :param pssession: The PowerShell sessionId from the user.         :type pssession: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns          PowerShellCommandResults or          ClientRawResponse<PowerShellCommandResults> if raw==True         :rtype:          ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.servermanager.models.PowerShellCommandResults]          or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.servermanager.models.PowerShellCommandResults]]         :raises:          :class:`ErrorException<azure.mgmt.servermanager.models.ErrorException>`
Deletes the managed application definition.          :param application_definition_id: The fully qualified ID of the          managed application definition, including the managed application name          and the managed application definition resource type. Use the format,          /subscriptions/{guid}/resourceGroups/{resource-group-name}/Microsoft.Solutions/applicationDefinitions/{applicationDefinition-name}         :type application_definition_id: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns None or          ClientRawResponse<None> if raw==True         :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]         :raises:          :class:`ErrorResponseException<azure.mgmt.resource.managedapplications.models.ErrorResponseException>`
Creates a new managed application definition.          :param application_definition_id: The fully qualified ID of the          managed application definition, including the managed application name          and the managed application definition resource type. Use the format,          /subscriptions/{guid}/resourceGroups/{resource-group-name}/Microsoft.Solutions/applicationDefinitions/{applicationDefinition-name}         :type application_definition_id: str         :param parameters: Parameters supplied to the create or update a          managed application definition.         :type parameters:          ~azure.mgmt.resource.managedapplications.models.ApplicationDefinition         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns ApplicationDefinition          or ClientRawResponse<ApplicationDefinition> if raw==True         :rtype:          ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.resource.managedapplications.models.ApplicationDefinition]          or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.resource.managedapplications.models.ApplicationDefinition]]         :raises:          :class:`ErrorResponseException<azure.mgmt.resource.managedapplications.models.ErrorResponseException>`
Return the target uri for the request.
Create connection for the request.
Sends request to cloud service server and return the response.
Executes script actions on the specified HDInsight cluster.          :param resource_group_name: The name of the resource group.         :type resource_group_name: str         :param cluster_name: The name of the cluster.         :type cluster_name: str         :param persist_on_success: Gets or sets if the scripts needs to be          persisted.         :type persist_on_success: bool         :param script_actions: The list of run time script actions.         :type script_actions:          list[~azure.mgmt.hdinsight.models.RuntimeScriptAction]         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns None or          ClientRawResponse<None> if raw==True         :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]         :raises:          :class:`ErrorResponseException<azure.mgmt.hdinsight.models.ErrorResponseException>`
Check the availability of a Front Door resource name.          :param name: The resource name to validate.         :type name: str         :param type: The type of the resource whose name is to be validated.          Possible values include: 'Microsoft.Network/frontDoors',          'Microsoft.Network/frontDoors/frontendEndpoints'         :type type: str or ~azure.mgmt.frontdoor.models.ResourceType         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: CheckNameAvailabilityOutput or ClientRawResponse if raw=true         :rtype: ~azure.mgmt.frontdoor.models.CheckNameAvailabilityOutput or          ~msrest.pipeline.ClientRawResponse         :raises:          :class:`ErrorResponseException<azure.mgmt.frontdoor.models.ErrorResponseException>`
Permanently deletes the specified vault. aka Purges the deleted Azure         key vault.          :param vault_name: The name of the soft-deleted vault.         :type vault_name: str         :param location: The location of the soft-deleted vault.         :type location: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns None or          ClientRawResponse<None> if raw==True         :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]         :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`
Returns the URI for the authorization server if present, otherwise empty string.
Extracts the host authority from the given URI.
Return a CLI profile class.      .. versionadded:: 1.1.6      :return: A CLI Profile     :rtype: azure.cli.core._profile.Profile     :raises: ImportError if azure-cli-core package is not available
Return Credentials and default SubscriptionID of current loaded profile of the CLI.      Credentials will be the "az login" command:     https://docs.microsoft.com/cli/azure/authenticate-azure-cli      Default subscription ID is either the only one you have, or you can define it:     https://docs.microsoft.com/cli/azure/manage-azure-subscriptions-azure-cli      .. versionadded:: 1.1.6      :param str resource: The alternative resource for credentials if not ARM (GraphRBac, etc.)     :param bool with_tenant: If True, return a three-tuple with last as tenant ID     :return: tuple of Credentials and SubscriptionID (and tenant ID if with_tenant)     :rtype: tuple
Gets predictions for a given utterance, in the form of intents and         entities. The current maximum query size is 500 characters.          :param app_id: The LUIS application ID (Guid).         :type app_id: str         :param query: The utterance to predict.         :type query: str         :param timezone_offset: The timezone offset for the location of the          request.         :type timezone_offset: float         :param verbose: If true, return all intents instead of just the top          scoring intent.         :type verbose: bool         :param staging: Use the staging endpoint slot.         :type staging: bool         :param spell_check: Enable spell checking.         :type spell_check: bool         :param bing_spell_check_subscription_key: The subscription key to use          when enabling Bing spell check         :type bing_spell_check_subscription_key: str         :param log: Log query (default is true)         :type log: bool         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: LuisResult or ClientRawResponse if raw=true         :rtype:          ~azure.cognitiveservices.language.luis.runtime.models.LuisResult or          ~msrest.pipeline.ClientRawResponse         :raises:          :class:`APIErrorException<azure.cognitiveservices.language.luis.runtime.models.APIErrorException>`
Check Name Availability for global uniqueness.          :param location: The location in which uniqueness will be verified.         :type location: str         :param name: Resource Name To Verify         :type name: str         :param type: Fully qualified resource type which includes provider          namespace         :type type: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: CheckNameAvailabilityResponse or ClientRawResponse if          raw=true         :rtype: ~azure.mgmt.mixedreality.models.CheckNameAvailabilityResponse          or ~msrest.pipeline.ClientRawResponse         :raises:          :class:`ErrorResponseException<azure.mgmt.mixedreality.models.ErrorResponseException>`
Opens the request.          method:             the request VERB 'GET', 'POST', etc.         url:             the url to connect
Sets up the timeout for the request.
Sets the request header.
Gets back all response headers.
Sends the request body.
Gets status of response.
Gets status text of response.
Gets response body as a SAFEARRAY and converts the SAFEARRAY to str.
Sets client certificate for the request.
Connects to host and sends the request.
Sends the headers of request.
Sends request body.
Gets the response and generates the _Response object
simplified an id to be more friendly for us people
converts a Python name into a serializable name
Verify whether two faces belong to a same person. Compares a face Id         with a Person Id.          :param face_id: FaceId of the face, comes from Face - Detect         :type face_id: str         :param person_id: Specify a certain person in a person group or a          large person group. personId is created in PersonGroup Person - Create          or LargePersonGroup Person - Create.         :type person_id: str         :param person_group_id: Using existing personGroupId and personId for          fast loading a specified person. personGroupId is created in          PersonGroup - Create. Parameter personGroupId and largePersonGroupId          should not be provided at the same time.         :type person_group_id: str         :param large_person_group_id: Using existing largePersonGroupId and          personId for fast loading a specified person. largePersonGroupId is          created in LargePersonGroup - Create. Parameter personGroupId and          largePersonGroupId should not be provided at the same time.         :type large_person_group_id: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: VerifyResult or ClientRawResponse if raw=true         :rtype: ~azure.cognitiveservices.vision.face.models.VerifyResult or          ~msrest.pipeline.ClientRawResponse         :raises:          :class:`APIErrorException<azure.cognitiveservices.vision.face.models.APIErrorException>`
Adds a job to the specified account.          The Batch service supports two ways to control the work done as part of         a job. In the first approach, the user specifies a Job Manager task.         The Batch service launches this task when it is ready to start the job.         The Job Manager task controls all other tasks that run under this job,         by using the Task APIs. In the second approach, the user directly         controls the execution of tasks under an active job, by using the Task         APIs. Also note: when naming jobs, avoid including sensitive         information such as user names or secret project names. This         information may appear in telemetry logs accessible to Microsoft         Support engineers.          :param job: The job to be added.         :type job: ~azure.batch.models.JobAddParameter         :param job_add_options: Additional parameters for the operation         :type job_add_options: ~azure.batch.models.JobAddOptions         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: None or ClientRawResponse if raw=true         :rtype: None or ~msrest.pipeline.ClientRawResponse         :raises:          :class:`BatchErrorException<azure.batch.models.BatchErrorException>`
get properties from entry xml
descends through a hierarchy of nodes returning the list of children         at the inner most level.  Only returns children who share a common parent,         not cousins.
Recursively searches from the parent to the child,         gathering all the applicable namespaces along the way
Converts xml response to service bus namespace          The xml format for namespace: <entry> <id>uuid:00000000-0000-0000-0000-000000000000;id=0000000</id> <title type="text">myunittests</title> <updated>2012-08-22T16:48:10Z</updated> <content type="application/xml">     <NamespaceDescription         xmlns="http://schemas.microsoft.com/netservices/2010/10/servicebus/connect"         xmlns:i="http://www.w3.org/2001/XMLSchema-instance">     <Name>myunittests</Name>     <Region>West US</Region>     <DefaultKey>0000000000000000000000000000000000000000000=</DefaultKey>     <Status>Active</Status>     <CreatedAt>2012-08-22T16:48:10.217Z</CreatedAt>     <AcsManagementEndpoint>https://myunittests-sb.accesscontrol.windows.net/</AcsManagementEndpoint>     <ServiceBusEndpoint>https://myunittests.servicebus.windows.net/</ServiceBusEndpoint>     <ConnectionString>Endpoint=sb://myunittests.servicebus.windows.net/;SharedSecretIssuer=owner;SharedSecretValue=0000000000000000000000000000000000000000000=</ConnectionString>     <SubscriptionId>00000000000000000000000000000000</SubscriptionId>     <Enabled>true</Enabled>     </NamespaceDescription> </content> </entry>
Converts xml response to service bus region          The xml format for region: <entry> <id>uuid:157c311f-081f-4b4a-a0ba-a8f990ffd2a3;id=1756759</id> <title type="text"></title> <updated>2013-04-10T18:25:29Z</updated> <content type="application/xml">     <RegionCodeDescription         xmlns="http://schemas.microsoft.com/netservices/2010/10/servicebus/connect"         xmlns:i="http://www.w3.org/2001/XMLSchema-instance">     <Code>East Asia</Code>     <FullName>East Asia</FullName>     </RegionCodeDescription> </content> </entry>
Converts xml response to service bus namespace availability          The xml format: <?xml version="1.0" encoding="utf-8"?> <entry xmlns="http://www.w3.org/2005/Atom">     <id>uuid:9fc7c652-1856-47ab-8d74-cd31502ea8e6;id=3683292</id>     <title type="text"></title>     <updated>2013-04-16T03:03:37Z</updated>     <content type="application/xml">         <NamespaceAvailability             xmlns="http://schemas.microsoft.com/netservices/2010/10/servicebus/connect"             xmlns:i="http://www.w3.org/2001/XMLSchema-instance">             <Result>false</Result>         </NamespaceAvailability>     </content> </entry>
Converts xml response to service bus metrics objects          The xml format for MetricProperties <entry>     <id>https://sbgm.windows.net/Metrics(\'listeners.active\')</id>     <title/>     <updated>2014-10-09T11:56:50Z</updated>     <author>         <name/>     </author>     <content type="application/xml">         <m:properties>             <d:Name>listeners.active</d:Name>             <d:PrimaryAggregation>Average</d:PrimaryAggregation>             <d:Unit>Count</d:Unit>             <d:DisplayName>Active listeners</d:DisplayName>         </m:properties>     </content> </entry>          The xml format for MetricValues     <entry>         <id>https://sbgm.windows.net/MetricValues(datetime\'2014-10-02T00:00:00Z\')</id>         <title/>         <updated>2014-10-09T18:38:28Z</updated>         <author>             <name/>         </author>         <content type="application/xml">             <m:properties>                 <d:Timestamp m:type="Edm.DateTime">2014-10-02T00:00:00Z</d:Timestamp>                 <d:Min m:type="Edm.Int64">-118</d:Min>                 <d:Max m:type="Edm.Int64">15</d:Max>                 <d:Average m:type="Edm.Single">-78.44444</d:Average>                 <d:Total m:type="Edm.Int64">0</d:Total>             </m:properties>         </content>     </entry>
Replaces the runbook draft content.          :param resource_group_name: Name of an Azure Resource group.         :type resource_group_name: str         :param automation_account_name: The name of the automation account.         :type automation_account_name: str         :param runbook_name: The runbook name.         :type runbook_name: str         :param runbook_content: The runbook draft content.         :type runbook_content: Generator         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns object or          ClientRawResponse<object> if raw==True         :rtype: ~msrestazure.azure_operation.AzureOperationPoller[Generator]          or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[Generator]]         :raises:          :class:`ErrorResponseException<azure.mgmt.automation.models.ErrorResponseException>`
Get domain name recommendations based on keywords.          Get domain name recommendations based on keywords.          :param keywords: Keywords to be used for generating domain          recommendations.         :type keywords: str         :param max_domain_recommendations: Maximum number of recommendations.         :type max_domain_recommendations: int         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: An iterator like instance of NameIdentifier         :rtype:          ~azure.mgmt.web.models.NameIdentifierPaged[~azure.mgmt.web.models.NameIdentifier]         :raises:          :class:`DefaultErrorResponseException<azure.mgmt.web.models.DefaultErrorResponseException>`
Asynchronous operation to modify a knowledgebase.          :param kb_id: Knowledgebase id.         :type kb_id: str         :param update_kb: Post body of the request.         :type update_kb:          ~azure.cognitiveservices.knowledge.qnamaker.models.UpdateKbOperationDTO         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: Operation or ClientRawResponse if raw=true         :rtype: ~azure.cognitiveservices.knowledge.qnamaker.models.Operation          or ~msrest.pipeline.ClientRawResponse         :raises:          :class:`ErrorResponseException<azure.cognitiveservices.knowledge.qnamaker.models.ErrorResponseException>`
Gets a collection that contains the object IDs of the groups of which         the user is a member.          :param object_id: The object ID of the user for which to get group          membership.         :type object_id: str         :param security_enabled_only: If true, only membership in          security-enabled groups should be checked. Otherwise, membership in          all groups should be checked.         :type security_enabled_only: bool         :param additional_properties: Unmatched properties from the message          are deserialized this collection         :type additional_properties: dict[str, object]         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: An iterator like instance of str         :rtype: ~azure.graphrbac.models.StrPaged[str]         :raises:          :class:`GraphErrorException<azure.graphrbac.models.GraphErrorException>`
Will clone the given PR branch and vuild the package with the given name.
Import data into Redis cache.          :param resource_group_name: The name of the resource group.         :type resource_group_name: str         :param name: The name of the Redis cache.         :type name: str         :param files: files to import.         :type files: list[str]         :param format: File format.         :type format: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns None or          ClientRawResponse<None> if raw==True         :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]         :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`
Publish runbook draft.          :param resource_group_name: Name of an Azure Resource group.         :type resource_group_name: str         :param automation_account_name: The name of the automation account.         :type automation_account_name: str         :param runbook_name: The parameters supplied to the publish runbook          operation.         :type runbook_name: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns None or          ClientRawResponse<None> if raw==True         :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]         :raises:          :class:`ErrorResponseException<azure.mgmt.automation.models.ErrorResponseException>`
Renew the message lock.          This will maintain the lock on the message to ensure         it is not returned to the queue to be reprocessed. In order to complete (or otherwise settle)         the message, the lock must be maintained. Messages received via ReceiveAndDelete mode are not         locked, and therefore cannot be renewed. This operation can also be performed as an asynchronous         background task by registering the message with an `azure.servicebus.aio.AutoLockRenew` instance.         This operation is only available for non-sessionful messages.          :raises: TypeError if the message is sessionful.         :raises: ~azure.servicebus.common.errors.MessageLockExpired is message lock has already expired.         :raises: ~azure.servicebus.common.errors.SessionLockExpired if session lock has already expired.         :raises: ~azure.servicebus.common.errors.MessageAlreadySettled is message has already been settled.
Replace alterations data.          :param word_alterations: Collection of word alterations.         :type word_alterations:          list[~azure.cognitiveservices.knowledge.qnamaker.models.AlterationsDTO]         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: None or ClientRawResponse if raw=true         :rtype: None or ~msrest.pipeline.ClientRawResponse         :raises:          :class:`ErrorResponseException<azure.cognitiveservices.knowledge.qnamaker.models.ErrorResponseException>`
Adds the specified value as a new version of the specified secret         resource.          Creates a new value of the specified secret resource. The name of the         value is typically the version identifier. Once created the value         cannot be changed.          :param secret_resource_name: The name of the secret resource.         :type secret_resource_name: str         :param secret_value_resource_name: The name of the secret resource          value which is typically the version identifier for the value.         :type secret_value_resource_name: str         :param name: Version identifier of the secret value.         :type name: str         :param value: The actual value of the secret.         :type value: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: SecretValueResourceDescription or ClientRawResponse if          raw=true         :rtype: ~azure.servicefabric.models.SecretValueResourceDescription or          ~msrest.pipeline.ClientRawResponse         :raises:          :class:`FabricErrorException<azure.servicefabric.models.FabricErrorException>`
Returns system properties for the specified storage account.          service_name:             Name of the storage service account.
Returns the primary and secondary access keys for the specified         storage account.          service_name:             Name of the storage service account.
Regenerates the primary or secondary access key for the specified         storage account.          service_name:             Name of the storage service account.         key_type:             Specifies which key to regenerate. Valid values are:             Primary, Secondary
Creates a new storage account in Windows Azure.          service_name:             A name for the storage account that is unique within Windows Azure.             Storage account names must be between 3 and 24 characters in length             and use numbers and lower-case letters only.         description:             A description for the storage account. The description may be up             to 1024 characters in length.         label:             A name for the storage account. The name may be up to 100             characters in length. The name can be used to identify the storage             account for your tracking purposes.         affinity_group:             The name of an existing affinity group in the specified             subscription. You can specify either a location or affinity_group,             but not both.         location:             The location where the storage account is created. You can specify             either a location or affinity_group, but not both.         geo_replication_enabled:             Deprecated. Replaced by the account_type parameter.         extended_properties:             Dictionary containing name/value pairs of storage account             properties. You can have a maximum of 50 extended property             name/value pairs. The maximum length of the Name element is 64             characters, only alphanumeric characters and underscores are valid             in the Name, and the name must start with a letter. The value has             a maximum length of 255 characters.         account_type:             Specifies whether the account supports locally-redundant storage,             geo-redundant storage, zone-redundant storage, or read access             geo-redundant storage.             Possible values are:                 Standard_LRS, Standard_ZRS, Standard_GRS, Standard_RAGRS
Updates the label, the description, and enables or disables the         geo-replication status for a storage account in Windows Azure.          service_name:             Name of the storage service account.         description:             A description for the storage account. The description may be up             to 1024 characters in length.         label:             A name for the storage account. The name may be up to 100             characters in length. The name can be used to identify the storage             account for your tracking purposes.         geo_replication_enabled:             Deprecated. Replaced by the account_type parameter.         extended_properties:             Dictionary containing name/value pairs of storage account             properties. You can have a maximum of 50 extended property             name/value pairs. The maximum length of the Name element is 64             characters, only alphanumeric characters and underscores are valid             in the Name, and the name must start with a letter. The value has             a maximum length of 255 characters.         account_type:             Specifies whether the account supports locally-redundant storage,             geo-redundant storage, zone-redundant storage, or read access             geo-redundant storage.             Possible values are:                 Standard_LRS, Standard_ZRS, Standard_GRS, Standard_RAGRS
Deletes the specified storage account from Windows Azure.          service_name:             Name of the storage service account.
Checks to see if the specified storage account name is available, or         if it has already been taken.          service_name:             Name of the storage service account.
Retrieves system properties for the specified hosted service. These         properties include the service name and service type; the name of the         affinity group to which the service belongs, or its location if it is         not part of an affinity group; and optionally, information on the         service's deployments.          service_name:             Name of the hosted service.         embed_detail:             When True, the management service returns properties for all             deployments of the service, as well as for the service itself.
Creates a new hosted service in Windows Azure.          service_name:             A name for the hosted service that is unique within Windows Azure.             This name is the DNS prefix name and can be used to access the             hosted service.         label:             A name for the hosted service. The name can be up to 100 characters             in length. The name can be used to identify the storage account for             your tracking purposes.         description:             A description for the hosted service. The description can be up to             1024 characters in length.         location:             The location where the hosted service will be created. You can             specify either a location or affinity_group, but not both.         affinity_group:             The name of an existing affinity group associated with this             subscription. This name is a GUID and can be retrieved by examining             the name element of the response body returned by             list_affinity_groups. You can specify either a location or             affinity_group, but not both.         extended_properties:             Dictionary containing name/value pairs of storage account             properties. You can have a maximum of 50 extended property             name/value pairs. The maximum length of the Name element is 64             characters, only alphanumeric characters and underscores are valid             in the Name, and the name must start with a letter. The value has             a maximum length of 255 characters.
Deletes the specified hosted service from Windows Azure.          service_name:             Name of the hosted service.         complete:             True if all OS/data disks and the source blobs for the disks should             also be deleted from storage.
Uploads a new service package and creates a new deployment on staging         or production.          service_name:             Name of the hosted service.         deployment_slot:             The environment to which the hosted service is deployed. Valid             values are: staging, production         name:             The name for the deployment. The deployment name must be unique             among other deployments for the hosted service.         package_url:             A URL that refers to the location of the service package in the             Blob service. The service package can be located either in a             storage account beneath the same subscription or a Shared Access             Signature (SAS) URI from any storage account.         label:             A name for the hosted service. The name can be up to 100 characters             in length. It is recommended that the label be unique within the             subscription. The name can be used to identify the hosted service             for your tracking purposes.         configuration:             The base-64 encoded service configuration file for the deployment.         start_deployment:             Indicates whether to start the deployment immediately after it is             created. If false, the service model is still deployed to the             virtual machines but the code is not run immediately. Instead, the             service is Suspended until you call Update Deployment Status and             set the status to Running, at which time the service will be             started. A deployed service still incurs charges, even if it is             suspended.         treat_warnings_as_error:             Indicates whether to treat package validation warnings as errors.             If set to true, the Created Deployment operation fails if there             are validation warnings on the service package.         extended_properties:             Dictionary containing name/value pairs of storage account             properties. You can have a maximum of 50 extended property             name/value pairs. The maximum length of the Name element is 64             characters, only alphanumeric characters and underscores are valid             in the Name, and the name must start with a letter. The value has             a maximum length of 255 characters.
Deletes the specified deployment.          service_name:             Name of the hosted service.         deployment_name:             The name of the deployment.
Initiates a virtual IP swap between the staging and production         deployment environments for a service. If the service is currently         running in the staging environment, it will be swapped to the         production environment. If it is running in the production         environment, it will be swapped to staging.          service_name:             Name of the hosted service.         production:             The name of the production deployment.         source_deployment:             The name of the source deployment.
Initiates a change to the deployment configuration.          service_name:             Name of the hosted service.         deployment_name:             The name of the deployment.         configuration:             The base-64 encoded service configuration file for the deployment.         treat_warnings_as_error:             Indicates whether to treat package validation warnings as errors.             If set to true, the Created Deployment operation fails if there             are validation warnings on the service package.         mode:             If set to Manual, WalkUpgradeDomain must be called to apply the             update. If set to Auto, the Windows Azure platform will             automatically apply the update To each upgrade domain for the             service. Possible values are: Auto, Manual         extended_properties:             Dictionary containing name/value pairs of storage account             properties. You can have a maximum of 50 extended property             name/value pairs. The maximum length of the Name element is 64             characters, only alphanumeric characters and underscores are valid             in the Name, and the name must start with a letter. The value has             a maximum length of 255 characters.
Initiates a change in deployment status.          service_name:             Name of the hosted service.         deployment_name:             The name of the deployment.         status:             The change to initiate to the deployment status. Possible values             include:                 Running, Suspended
Initiates an upgrade.          service_name:             Name of the hosted service.         deployment_name:             The name of the deployment.         mode:             If set to Manual, WalkUpgradeDomain must be called to apply the             update. If set to Auto, the Windows Azure platform will             automatically apply the update To each upgrade domain for the             service. Possible values are: Auto, Manual         package_url:             A URL that refers to the location of the service package in the             Blob service. The service package can be located either in a             storage account beneath the same subscription or a Shared Access             Signature (SAS) URI from any storage account.         configuration:             The base-64 encoded service configuration file for the deployment.         label:             A name for the hosted service. The name can be up to 100 characters             in length. It is recommended that the label be unique within the             subscription. The name can be used to identify the hosted service             for your tracking purposes.         force:             Specifies whether the rollback should proceed even when it will             cause local data to be lost from some role instances. True if the             rollback should proceed; otherwise false if the rollback should             fail.         role_to_upgrade:             The name of the specific role to upgrade.         extended_properties:             Dictionary containing name/value pairs of storage account             properties. You can have a maximum of 50 extended property             name/value pairs. The maximum length of the Name element is 64             characters, only alphanumeric characters and underscores are valid             in the Name, and the name must start with a letter. The value has             a maximum length of 255 characters.
Specifies the next upgrade domain to be walked during manual in-place         upgrade or configuration change.          service_name:             Name of the hosted service.         deployment_name:             The name of the deployment.         upgrade_domain:             An integer value that identifies the upgrade domain to walk.             Upgrade domains are identified with a zero-based index: the first             upgrade domain has an ID of 0, the second has an ID of 1, and so on.
Requests a reboot of a role instance that is running in a deployment.          service_name:             Name of the hosted service.         deployment_name:             The name of the deployment.         role_instance_name:             The name of the role instance.
Reinstalls the operating system on instances of web roles or worker         roles and initializes the storage resources that are used by them. If         you do not want to initialize storage resources, you can use         reimage_role_instance.          service_name:             Name of the hosted service.         deployment_name:             The name of the deployment.         role_instance_names:             List of role instance names.
Checks to see if the specified hosted service name is available, or if         it has already been taken.          service_name:             Name of the hosted service.
Lists all of the service certificates associated with the specified         hosted service.          service_name:             Name of the hosted service.
Returns the public data for the specified X.509 certificate associated         with a hosted service.          service_name:             Name of the hosted service.         thumbalgorithm:             The algorithm for the certificate's thumbprint.         thumbprint:             The hexadecimal representation of the thumbprint.
Adds a certificate to a hosted service.          service_name:             Name of the hosted service.         data:             The base-64 encoded form of the pfx/cer file.         certificate_format:             The service certificate format.         password:             The certificate password. Default to None when using cer format.
Deletes a service certificate from the certificate store of a hosted         service.          service_name:             Name of the hosted service.         thumbalgorithm:             The algorithm for the certificate's thumbprint.         thumbprint:             The hexadecimal representation of the thumbprint.
The Get Management Certificate operation retrieves information about         the management certificate with the specified thumbprint. Management         certificates, which are also known as subscription certificates,         authenticate clients attempting to connect to resources associated         with your Windows Azure subscription.          thumbprint:             The thumbprint value of the certificate.
The Add Management Certificate operation adds a certificate to the         list of management certificates. Management certificates, which are         also known as subscription certificates, authenticate clients         attempting to connect to resources associated with your Windows Azure         subscription.          public_key:             A base64 representation of the management certificate public key.         thumbprint:             The thumb print that uniquely identifies the management             certificate.         data:             The certificate's raw data in base-64 encoded .cer format.
The Delete Management Certificate operation deletes a certificate from         the list of management certificates. Management certificates, which         are also known as subscription certificates, authenticate clients         attempting to connect to resources associated with your Windows Azure         subscription.          thumbprint:             The thumb print that uniquely identifies the management             certificate.
Returns the system properties associated with the specified affinity         group.          affinity_group_name:             The name of the affinity group.
Creates a new affinity group for the specified subscription.          name:             A name for the affinity group that is unique to the subscription.         label:             A name for the affinity group. The name can be up to 100 characters             in length.         location:             The data center location where the affinity group will be created.             To list available locations, use the list_location function.         description:             A description for the affinity group. The description can be up to             1024 characters in length.
Deletes an affinity group in the specified subscription.          affinity_group_name:             The name of the affinity group.
List subscription operations.          start_time: Required. An ISO8601 date.         end_time: Required. An ISO8601 date.         object_id_filter: Optional. Returns subscription operations only for the specified object type and object ID         operation_result_filter: Optional. Returns subscription operations only for the specified result status, either Succeeded, Failed, or InProgress.         continuation_token: Optional.         More information at:         https://msdn.microsoft.com/en-us/library/azure/gg715318.aspx
Reserves an IPv4 address for the specified subscription.          name:             Required. Specifies the name for the reserved IP address.         label:             Optional. Specifies a label for the reserved IP address. The label             can be up to 100 characters long and can be used for your tracking             purposes.         location:             Required. Specifies the location of the reserved IP address. This             should be the same location that is assigned to the cloud service             containing the deployment that will use the reserved IP address.             To see the available locations, you can use list_locations.
Deletes a reserved IP address from the specified subscription.          name:             Required. Name of the reserved IP address.
Associate an existing reservedIP to a deployment.          name:             Required. Name of the reserved IP address.          service_name:             Required. Name of the hosted service.          deployment_name:             Required. Name of the deployment.          virtual_ip_name:             Optional. Name of the VirtualIP in case of multi Vip tenant.             If this value is not specified default virtualIP is used             for this operation.
Disassociate an existing reservedIP from the given deployment.          name:             Required. Name of the reserved IP address.          service_name:             Required. Name of the hosted service.          deployment_name:             Required. Name of the deployment.          virtual_ip_name:             Optional. Name of the VirtualIP in case of multi Vip tenant.             If this value is not specified default virtualIP is used             for this operation.
Retrieves information about the specified reserved IP address.          name:             Required. Name of the reserved IP address.
Retrieves the specified virtual machine.          service_name:             The name of the service.         deployment_name:             The name of the deployment.         role_name:             The name of the role.
Provisions a virtual machine based on the supplied configuration.          service_name:             Name of the hosted service.         deployment_name:             The name for the deployment. The deployment name must be unique             among other deployments for the hosted service.         deployment_slot:             The environment to which the hosted service is deployed. Valid             values are: staging, production         label:             Specifies an identifier for the deployment. The label can be up to             100 characters long. The label can be used for tracking purposes.         role_name:             The name of the role.         system_config:             Contains the metadata required to provision a virtual machine from             a Windows or Linux OS image.  Use an instance of             WindowsConfigurationSet or LinuxConfigurationSet.         os_virtual_hard_disk:             Contains the parameters Windows Azure uses to create the operating             system disk for the virtual machine. If you are creating a Virtual             Machine by using a VM Image, this parameter is not used.         network_config:             Encapsulates the metadata required to create the virtual network             configuration for a virtual machine. If you do not include a             network configuration set you will not be able to access the VM             through VIPs over the internet. If your virtual machine belongs to             a virtual network you can not specify which subnet address space             it resides under. Use an instance of ConfigurationSet.         availability_set_name:             Specifies the name of an availability set to which to add the             virtual machine. This value controls the virtual machine             allocation in the Windows Azure environment. Virtual machines             specified in the same availability set are allocated to different             nodes to maximize availability.         data_virtual_hard_disks:             Contains the parameters Windows Azure uses to create a data disk             for a virtual machine.         role_size:             The size of the virtual machine to allocate. The default value is             Small. Possible values are: ExtraSmall,Small,Medium,Large,             ExtraLarge,A5,A6,A7,A8,A9,Basic_A0,Basic_A1,Basic_A2,Basic_A3,             Basic_A4,Standard_D1,Standard_D2,Standard_D3,Standard_D4,             Standard_D11,Standard_D12,Standard_D13,Standard_D14,Standard_G1,             Standard_G2,Sandard_G3,Standard_G4,Standard_G5. The specified             value must be compatible with the disk selected in the              OSVirtualHardDisk values.         role_type:             The type of the role for the virtual machine. The only supported             value is PersistentVMRole.         virtual_network_name:             Specifies the name of an existing virtual network to which the             deployment will belong.         resource_extension_references:             Optional. Contains a collection of resource extensions that are to             be installed on the Virtual Machine. This element is used if             provision_guest_agent is set to True. Use an iterable of instances             of ResourceExtensionReference.         provision_guest_agent:             Optional. Indicates whether the VM Agent is installed on the             Virtual Machine. To run a resource extension in a Virtual Machine,             this service must be installed.         vm_image_name:             Optional. Specifies the name of the VM Image that is to be used to             create the Virtual Machine. If this is specified, the             system_config and network_config parameters are not used.         media_location:             Optional. Required if the Virtual Machine is being created from a             published VM Image. Specifies the location of the VHD file that is             created when VMImageName specifies a published VM Image.         dns_servers:             Optional. List of DNS servers (use DnsServer class) to associate             with the Virtual Machine.         reserved_ip_name:             Optional. Specifies the name of a reserved IP address that is to be             assigned to the deployment. You must run create_reserved_ip_address             before you can assign the address to the deployment using this             element.
Adds a virtual machine to an existing deployment.          service_name:             The name of the service.         deployment_name:             The name of the deployment.         role_name:             The name of the role.         system_config:             Contains the metadata required to provision a virtual machine from             a Windows or Linux OS image.  Use an instance of             WindowsConfigurationSet or LinuxConfigurationSet.         os_virtual_hard_disk:             Contains the parameters Windows Azure uses to create the operating             system disk for the virtual machine. If you are creating a Virtual             Machine by using a VM Image, this parameter is not used.         network_config:             Encapsulates the metadata required to create the virtual network             configuration for a virtual machine. If you do not include a             network configuration set you will not be able to access the VM             through VIPs over the internet. If your virtual machine belongs to             a virtual network you can not specify which subnet address space             it resides under.         availability_set_name:             Specifies the name of an availability set to which to add the             virtual machine. This value controls the virtual machine allocation             in the Windows Azure environment. Virtual machines specified in the             same availability set are allocated to different nodes to maximize             availability.         data_virtual_hard_disks:             Contains the parameters Windows Azure uses to create a data disk             for a virtual machine.         role_size:             The size of the virtual machine to allocate. The default value is             Small. Possible values are: ExtraSmall, Small, Medium, Large,             ExtraLarge. The specified value must be compatible with the disk             selected in the OSVirtualHardDisk values.         role_type:             The type of the role for the virtual machine. The only supported             value is PersistentVMRole.         resource_extension_references:             Optional. Contains a collection of resource extensions that are to             be installed on the Virtual Machine. This element is used if             provision_guest_agent is set to True.         provision_guest_agent:             Optional. Indicates whether the VM Agent is installed on the             Virtual Machine. To run a resource extension in a Virtual Machine,             this service must be installed.         vm_image_name:             Optional. Specifies the name of the VM Image that is to be used to             create the Virtual Machine. If this is specified, the             system_config and network_config parameters are not used.         media_location:             Optional. Required if the Virtual Machine is being created from a             published VM Image. Specifies the location of the VHD file that is             created when VMImageName specifies a published VM Image.
Updates the specified virtual machine.          service_name:             The name of the service.         deployment_name:             The name of the deployment.         role_name:             The name of the role.         os_virtual_hard_disk:             Contains the parameters Windows Azure uses to create the operating             system disk for the virtual machine.         network_config:             Encapsulates the metadata required to create the virtual network             configuration for a virtual machine. If you do not include a             network configuration set you will not be able to access the VM             through VIPs over the internet. If your virtual machine belongs to             a virtual network you can not specify which subnet address space             it resides under.         availability_set_name:             Specifies the name of an availability set to which to add the             virtual machine. This value controls the virtual machine allocation             in the Windows Azure environment. Virtual machines specified in the             same availability set are allocated to different nodes to maximize             availability.         data_virtual_hard_disks:             Contains the parameters Windows Azure uses to create a data disk             for a virtual machine.         role_size:             The size of the virtual machine to allocate. The default value is             Small. Possible values are: ExtraSmall, Small, Medium, Large,             ExtraLarge. The specified value must be compatible with the disk             selected in the OSVirtualHardDisk values.         role_type:             The type of the role for the virtual machine. The only supported             value is PersistentVMRole.         resource_extension_references:             Optional. Contains a collection of resource extensions that are to             be installed on the Virtual Machine. This element is used if             provision_guest_agent is set to True.         provision_guest_agent:             Optional. Indicates whether the VM Agent is installed on the             Virtual Machine. To run a resource extension in a Virtual Machine,             this service must be installed.
Deletes the specified virtual machine.          service_name:             The name of the service.         deployment_name:             The name of the deployment.         role_name:             The name of the role.         complete:             True if all OS/data disks and the source blobs for the disks should             also be deleted from storage.
The Capture Role operation captures a virtual machine image to your         image gallery. From the captured image, you can create additional         customized virtual machines.          service_name:             The name of the service.         deployment_name:             The name of the deployment.         role_name:             The name of the role.         post_capture_action:             Specifies the action after capture operation completes. Possible             values are: Delete, Reprovision.         target_image_name:             Specifies the image name of the captured virtual machine.         target_image_label:             Specifies the friendly name of the captured virtual machine.         provisioning_configuration:             Use an instance of WindowsConfigurationSet or LinuxConfigurationSet.
Starts the specified virtual machine.          service_name:             The name of the service.         deployment_name:             The name of the deployment.         role_name:             The name of the role.
Starts the specified virtual machines.          service_name:             The name of the service.         deployment_name:             The name of the deployment.         role_names:             The names of the roles, as an enumerable of strings.
Restarts the specified virtual machine.          service_name:             The name of the service.         deployment_name:             The name of the deployment.         role_name:             The name of the role.
Shuts down the specified virtual machine.          service_name:             The name of the service.         deployment_name:             The name of the deployment.         role_name:             The name of the role.         post_shutdown_action:             Specifies how the Virtual Machine should be shut down. Values are:                 Stopped                     Shuts down the Virtual Machine but retains the compute                     resources. You will continue to be billed for the resources                     that the stopped machine uses.                 StoppedDeallocated                     Shuts down the Virtual Machine and releases the compute                     resources. You are not billed for the compute resources that                     this Virtual Machine uses. If a static Virtual Network IP                     address is assigned to the Virtual Machine, it is reserved.
Shuts down the specified virtual machines.          service_name:             The name of the service.         deployment_name:             The name of the deployment.         role_names:             The names of the roles, as an enumerable of strings.         post_shutdown_action:             Specifies how the Virtual Machine should be shut down. Values are:                 Stopped                     Shuts down the Virtual Machine but retains the compute                     resources. You will continue to be billed for the resources                     that the stopped machine uses.                 StoppedDeallocated                     Shuts down the Virtual Machine and releases the compute                     resources. You are not billed for the compute resources that                     this Virtual Machine uses. If a static Virtual Network IP                     address is assigned to the Virtual Machine, it is reserved.
Adds a DNS server definition to an existing deployment.          service_name:             The name of the service.         deployment_name:             The name of the deployment.         dns_server_name:             Specifies the name of the DNS server.         address:             Specifies the IP address of the DNS server.
Updates the ip address of a DNS server.          service_name:             The name of the service.         deployment_name:             The name of the deployment.         dns_server_name:             Specifies the name of the DNS server.         address:             Specifies the IP address of the DNS server.
Deletes a DNS server from a deployment.          service_name:             The name of the service.         deployment_name:             The name of the deployment.         dns_server_name:             Name of the DNS server that you want to delete.
Lists the versions of a resource extension that are available to add         to a Virtual Machine.          publisher_name:             Name of the resource extension publisher.         extension_name:             Name of the resource extension.
Replicate a VM image to multiple target locations. This operation         is only for publishers. You have to be registered as image publisher         with Microsoft Azure to be able to call this.          vm_image_name:             Specifies the name of the VM Image that is to be used for             replication         regions:             Specified a list of regions to replicate the image to             Note: The regions in the request body are not additive. If a VM             Image has already been replicated to Regions A, B, and C, and             a request is made to replicate to Regions A and D, the VM             Image will remain in Region A, will be replicated in Region D,             and will be unreplicated from Regions B and C         offer:             Specifies the publisher defined name of the offer. The allowed             characters are uppercase or lowercase letters, digit,             hypen(-), period (.).The maximum allowed length is 64 characters.         sku:             Specifies the publisher defined name of the Sku. The allowed             characters are uppercase or lowercase letters, digit,             hypen(-), period (.). The maximum allowed length is 64 characters.         version:             Specifies the publisher defined version of the image.             The allowed characters are digit and period.             Format: <MajorVersion>.<MinorVersion>.<Patch>             Example: '1.0.0' or '1.1.0' The 3 version number to             follow standard of most of the RPs. See http://semver.org
Unreplicate a VM image from all regions This operation         is only for publishers. You have to be registered as image publisher         with Microsoft Azure to be able to call this          vm_image_name:             Specifies the name of the VM Image that is to be used for             unreplication. The VM Image Name should be the user VM Image,             not the published name of the VM Image.
Share an already replicated OS image. This operation is only for         publishers. You have to be registered as image publisher with Windows         Azure to be able to call this.          vm_image_name:             The name of the virtual machine image to share         permission:             The sharing permission: public, msdn, or private
Creates a VM Image in the image repository that is associated with the         specified subscription using a specified set of virtual hard disks.          vm_image:             An instance of VMImage class.         vm_image.name: Required. Specifies the name of the image.         vm_image.label: Required. Specifies an identifier for the image.         vm_image.description: Optional. Specifies the description of the image.         vm_image.os_disk_configuration:             Required. Specifies configuration information for the operating              system disk that is associated with the image.         vm_image.os_disk_configuration.host_caching:             Optional. Specifies the caching behavior of the operating system disk.             Possible values are: None, ReadOnly, ReadWrite          vm_image.os_disk_configuration.os_state:             Required. Specifies the state of the operating system in the image.             Possible values are: Generalized, Specialized             A Virtual Machine that is fully configured and running contains a             Specialized operating system. A Virtual Machine on which the             Sysprep command has been run with the generalize option contains a             Generalized operating system.         vm_image.os_disk_configuration.os:             Required. Specifies the operating system type of the image.         vm_image.os_disk_configuration.media_link:             Required. Specifies the location of the blob in Windows Azure             storage. The blob location belongs to a storage account in the             subscription specified by the <subscription-id> value in the             operation call.         vm_image.data_disk_configurations:             Optional. Specifies configuration information for the data disks             that are associated with the image. A VM Image might not have data             disks associated with it.         vm_image.data_disk_configurations[].host_caching:             Optional. Specifies the caching behavior of the data disk.             Possible values are: None, ReadOnly, ReadWrite          vm_image.data_disk_configurations[].lun:             Optional if the lun for the disk is 0. Specifies the Logical Unit             Number (LUN) for the data disk.         vm_image.data_disk_configurations[].media_link:             Required. Specifies the location of the blob in Windows Azure             storage. The blob location belongs to a storage account in the             subscription specified by the <subscription-id> value in the             operation call.         vm_image.data_disk_configurations[].logical_size_in_gb:             Required. Specifies the size, in GB, of the data disk.         vm_image.language: Optional. Specifies the language of the image.         vm_image.image_family:             Optional. Specifies a value that can be used to group VM Images.         vm_image.recommended_vm_size:             Optional. Specifies the size to use for the Virtual Machine that             is created from the VM Image.         vm_image.eula:             Optional. Specifies the End User License Agreement that is             associated with the image. The value for this element is a string,             but it is recommended that the value be a URL that points to a EULA.         vm_image.icon_uri:             Optional. Specifies the URI to the icon that is displayed for the             image in the Management Portal.         vm_image.small_icon_uri:             Optional. Specifies the URI to the small icon that is displayed for             the image in the Management Portal.         vm_image.privacy_uri:             Optional. Specifies the URI that points to a document that contains             the privacy policy related to the image.         vm_image.published_date:             Optional. Specifies the date when the image was added to the image             repository.         vm_image.show_in_gui:             Optional. Indicates whether the VM Images should be listed in the             portal.
Deletes the specified VM Image from the image repository that is         associated with the specified subscription.          vm_image_name:             The name of the image.         delete_vhd:             Deletes the underlying vhd blob in Azure storage.
Retrieves a list of the VM Images from the image repository that is         associated with the specified subscription.
Updates a VM Image in the image repository that is associated with the         specified subscription.          vm_image_name:             Name of image to update.         vm_image:             An instance of VMImage class.         vm_image.label: Optional. Specifies an identifier for the image.         vm_image.os_disk_configuration:             Required. Specifies configuration information for the operating              system disk that is associated with the image.         vm_image.os_disk_configuration.host_caching:             Optional. Specifies the caching behavior of the operating system disk.             Possible values are: None, ReadOnly, ReadWrite          vm_image.data_disk_configurations:             Optional. Specifies configuration information for the data disks             that are associated with the image. A VM Image might not have data             disks associated with it.         vm_image.data_disk_configurations[].name:             Required. Specifies the name of the data disk.         vm_image.data_disk_configurations[].host_caching:             Optional. Specifies the caching behavior of the data disk.             Possible values are: None, ReadOnly, ReadWrite          vm_image.data_disk_configurations[].lun:             Optional if the lun for the disk is 0. Specifies the Logical Unit             Number (LUN) for the data disk.         vm_image.description: Optional. Specifies the description of the image.         vm_image.language: Optional. Specifies the language of the image.         vm_image.image_family:             Optional. Specifies a value that can be used to group VM Images.         vm_image.recommended_vm_size:             Optional. Specifies the size to use for the Virtual Machine that             is created from the VM Image.         vm_image.eula:             Optional. Specifies the End User License Agreement that is             associated with the image. The value for this element is a string,             but it is recommended that the value be a URL that points to a EULA.         vm_image.icon_uri:             Optional. Specifies the URI to the icon that is displayed for the             image in the Management Portal.         vm_image.small_icon_uri:             Optional. Specifies the URI to the small icon that is displayed for             the image in the Management Portal.         vm_image.privacy_uri:             Optional. Specifies the URI that points to a document that contains             the privacy policy related to the image.         vm_image.published_date:             Optional. Specifies the date when the image was added to the image             repository.         vm_image.show_in_gui:             Optional. Indicates whether the VM Images should be listed in the             portal.
Adds an OS image that is currently stored in a storage account in your         subscription to the image repository.          label:             Specifies the friendly name of the image.         media_link:             Specifies the location of the blob in Windows Azure blob store             where the media for the image is located. The blob location must             belong to a storage account in the subscription specified by the             <subscription-id> value in the operation call. Example:             http://example.blob.core.windows.net/disks/mydisk.vhd         name:             Specifies a name for the OS image that Windows Azure uses to             identify the image when creating one or more virtual machines.         os:             The operating system type of the OS image. Possible values are:             Linux, Windows
Updates an OS image that in your image repository.          image_name:             The name of the image to update.         label:             Specifies the friendly name of the image to be updated. You cannot             use this operation to update images provided by the Windows Azure             platform.         media_link:             Specifies the location of the blob in Windows Azure blob store             where the media for the image is located. The blob location must             belong to a storage account in the subscription specified by the             <subscription-id> value in the operation call. Example:             http://example.blob.core.windows.net/disks/mydisk.vhd         name:             Specifies a name for the OS image that Windows Azure uses to             identify the image when creating one or more VM Roles.         os:             The operating system type of the OS image. Possible values are:             Linux, Windows
Updates metadata elements from a given OS image reference.          image_name:             The name of the image to update.         os_image:             An instance of OSImage class.         os_image.label: Optional. Specifies an identifier for the image.         os_image.description: Optional. Specifies the description of the image.         os_image.language: Optional. Specifies the language of the image.         os_image.image_family:             Optional. Specifies a value that can be used to group VM Images.         os_image.recommended_vm_size:             Optional. Specifies the size to use for the Virtual Machine that             is created from the VM Image.         os_image.eula:             Optional. Specifies the End User License Agreement that is             associated with the image. The value for this element is a string,             but it is recommended that the value be a URL that points to a EULA.         os_image.icon_uri:             Optional. Specifies the URI to the icon that is displayed for the             image in the Management Portal.         os_image.small_icon_uri:             Optional. Specifies the URI to the small icon that is displayed for             the image in the Management Portal.         os_image.privacy_uri:             Optional. Specifies the URI that points to a document that contains             the privacy policy related to the image.         os_image.published_date:             Optional. Specifies the date when the image was added to the image             repository.         os.image.media_link:             Required: Specifies the location of the blob in Windows Azure             blob store where the media for the image is located. The blob             location must belong to a storage account in the subscription             specified by the <subscription-id> value in the operation call.             Example:             http://example.blob.core.windows.net/disks/mydisk.vhd         os_image.name:             Specifies a name for the OS image that Windows Azure uses to             identify the image when creating one or more VM Roles.         os_image.os:             The operating system type of the OS image. Possible values are:             Linux, Windows
Deletes the specified OS image from your image repository.          image_name:             The name of the image.         delete_vhd:             Deletes the underlying vhd blob in Azure storage.
Retrieves the specified data disk from a virtual machine.          service_name:             The name of the service.         deployment_name:             The name of the deployment.         role_name:             The name of the role.         lun:             The Logical Unit Number (LUN) for the disk.
Adds a data disk to a virtual machine.          service_name:             The name of the service.         deployment_name:             The name of the deployment.         role_name:             The name of the role.         lun:             Specifies the Logical Unit Number (LUN) for the disk. The LUN             specifies the slot in which the data drive appears when mounted             for usage by the virtual machine. Valid LUN values are 0 through 15.         host_caching:             Specifies the platform caching behavior of data disk blob for             read/write efficiency. The default vault is ReadOnly. Possible             values are: None, ReadOnly, ReadWrite         media_link:             Specifies the location of the blob in Windows Azure blob store             where the media for the disk is located. The blob location must             belong to the storage account in the subscription specified by the             <subscription-id> value in the operation call. Example:             http://example.blob.core.windows.net/disks/mydisk.vhd         disk_label:             Specifies the description of the data disk. When you attach a disk,             either by directly referencing a media using the MediaLink element             or specifying the target disk size, you can use the DiskLabel             element to customize the name property of the target data disk.         disk_name:             Specifies the name of the disk. Windows Azure uses the specified             disk to create the data disk for the machine and populates this             field with the disk name.         logical_disk_size_in_gb:             Specifies the size, in GB, of an empty disk to be attached to the             role. The disk can be created as part of disk attach or create VM             role call by specifying the value for this property. Windows Azure             creates the empty disk based on size preference and attaches the             newly created disk to the Role.         source_media_link:             Specifies the location of a blob in account storage which is             mounted as a data disk when the virtual machine is created.
Updates the specified data disk attached to the specified virtual         machine.          service_name:             The name of the service.         deployment_name:             The name of the deployment.         role_name:             The name of the role.         lun:             Specifies the Logical Unit Number (LUN) for the disk. The LUN             specifies the slot in which the data drive appears when mounted             for usage by the virtual machine. Valid LUN values are 0 through             15.         host_caching:             Specifies the platform caching behavior of data disk blob for             read/write efficiency. The default vault is ReadOnly. Possible             values are: None, ReadOnly, ReadWrite         media_link:             Specifies the location of the blob in Windows Azure blob store             where the media for the disk is located. The blob location must             belong to the storage account in the subscription specified by             the <subscription-id> value in the operation call. Example:             http://example.blob.core.windows.net/disks/mydisk.vhd         updated_lun:             Specifies the Logical Unit Number (LUN) for the disk. The LUN             specifies the slot in which the data drive appears when mounted             for usage by the virtual machine. Valid LUN values are 0 through 15.         disk_label:             Specifies the description of the data disk. When you attach a disk,             either by directly referencing a media using the MediaLink element             or specifying the target disk size, you can use the DiskLabel             element to customize the name property of the target data disk.         disk_name:             Specifies the name of the disk. Windows Azure uses the specified             disk to create the data disk for the machine and populates this             field with the disk name.         logical_disk_size_in_gb:             Specifies the size, in GB, of an empty disk to be attached to the             role. The disk can be created as part of disk attach or create VM             role call by specifying the value for this property. Windows Azure             creates the empty disk based on size preference and attaches the             newly created disk to the Role.
Removes the specified data disk from a virtual machine.          service_name:             The name of the service.         deployment_name:             The name of the deployment.         role_name:             The name of the role.         lun:             The Logical Unit Number (LUN) for the disk.         delete_vhd:             Deletes the underlying vhd blob in Azure storage.
Adds a disk to the user image repository. The disk can be an OS disk         or a data disk.          has_operating_system:             Deprecated.         label:             Specifies the description of the disk.         media_link:             Specifies the location of the blob in Windows Azure blob store             where the media for the disk is located. The blob location must             belong to the storage account in the current subscription specified             by the <subscription-id> value in the operation call. Example:             http://example.blob.core.windows.net/disks/mydisk.vhd         name:             Specifies a name for the disk. Windows Azure uses the name to             identify the disk when creating virtual machines from the disk.         os:             The OS type of the disk. Possible values are: Linux, Windows
Updates an existing disk in your image repository.          disk_name:             The name of the disk to update.         has_operating_system:             Deprecated.         label:             Specifies the description of the disk.         media_link:             Deprecated.         name:             Deprecated.         os:             Deprecated.
Deletes the specified data or operating system disk from your image         repository.          disk_name:             The name of the disk to delete.         delete_vhd:             Deletes the underlying vhd blob in Azure storage.
Summarizes policy states for the resources under the management group.          :param management_group_name: Management group name.         :type management_group_name: str         :param query_options: Additional parameters for the operation         :type query_options: ~azure.mgmt.policyinsights.models.QueryOptions         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: SummarizeResults or ClientRawResponse if raw=true         :rtype: ~azure.mgmt.policyinsights.models.SummarizeResults or          ~msrest.pipeline.ClientRawResponse         :raises:          :class:`QueryFailureException<azure.mgmt.policyinsights.models.QueryFailureException>`
This is a temporary patch pending a fix in uAMQP.
Receive a batch of messages at once.          This approach it optimal if you wish to process multiple messages simultaneously. Note that the         number of messages retrieved in a single batch will be dependent on         whether `prefetch` was set for the receiver. This call will prioritize returning         quickly over meeting a specified batch size, and so will return as soon as at least         one message is received and there is a gap in incoming messages regardless         of the specified batch size.          :param max_batch_size: Maximum number of messages in the batch. Actual number          returned will depend on prefetch size and incoming stream rate.         :type max_batch_size: int         :param timeout: The time to wait in seconds for the first message to arrive.          If no messages arrive, and no timeout is specified, this call will not return          until the connection is closed. If specified, an no messages arrive within the          timeout period, an empty list will be returned.         :rtype: list[~azure.servicebus.common.message.Message]          Example:             .. literalinclude:: ../examples/test_examples.py                 :start-after: [START fetch_next_messages]                 :end-before: [END fetch_next_messages]                 :language: python                 :dedent: 4                 :caption: Get the messages in batch from the receiver
Renew the session lock.          This operation must be performed periodically in order to retain a lock on the         session to continue message processing.         Once the lock is lost the connection will be closed. This operation can         also be performed as a threaded background task by registering the session         with an `azure.servicebus.AutoLockRenew` instance.          Example:             .. literalinclude:: ../examples/test_examples.py                 :start-after: [START renew_lock]                 :end-before: [END renew_lock]                 :language: python                 :dedent: 4                 :caption: Renew the session lock before it expires
Create or update a VM scale set.          :param resource_group_name: The name of the resource group.         :type resource_group_name: str         :param vm_scale_set_name: The name of the VM scale set to create or          update.         :type vm_scale_set_name: str         :param parameters: The scale set object.         :type parameters:          ~azure.mgmt.compute.v2019_03_01.models.VirtualMachineScaleSet         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns VirtualMachineScaleSet          or ClientRawResponse<VirtualMachineScaleSet> if raw==True         :rtype:          ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.compute.v2019_03_01.models.VirtualMachineScaleSet]          or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.compute.v2019_03_01.models.VirtualMachineScaleSet]]         :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`
Converts SinglePlacementGroup property to false for a existing virtual         machine scale set.          :param resource_group_name: The name of the resource group.         :type resource_group_name: str         :param vm_scale_set_name: The name of the virtual machine scale set to          create or update.         :type vm_scale_set_name: str         :param active_placement_group_id: Id of the placement group in which          you want future virtual machine instances to be placed. To query          placement group Id, please use Virtual Machine Scale Set VMs - Get          API. If not provided, the platform will choose one with maximum number          of virtual machine instances.         :type active_placement_group_id: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: None or ClientRawResponse if raw=true         :rtype: None or ~msrest.pipeline.ClientRawResponse         :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`
Detect profanity and match against custom and shared blacklists.          Detects profanity in more than 100 languages and match against custom         and shared blacklists.          :param text_content_type: The content type. Possible values include:          'text/plain', 'text/html', 'text/xml', 'text/markdown'         :type text_content_type: str         :param text_content: Content to screen.         :type text_content: Generator         :param language: Language of the text.         :type language: str         :param autocorrect: Autocorrect text.         :type autocorrect: bool         :param pii: Detect personal identifiable information.         :type pii: bool         :param list_id: The list Id.         :type list_id: str         :param classify: Classify input.         :type classify: bool         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param callback: When specified, will be called with each chunk of          data that is streamed. The callback should take two arguments, the          bytes of the current chunk of data and the response object. If the          data is uploading, response will be None.         :type callback: Callable[Bytes, response=None]         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: Screen or ClientRawResponse if raw=true         :rtype: ~azure.cognitiveservices.vision.contentmoderator.models.Screen          or ~msrest.pipeline.ClientRawResponse         :raises:          :class:`APIErrorException<azure.cognitiveservices.vision.contentmoderator.models.APIErrorException>`
Creates a new key, stores it, then returns key parameters and         attributes to the client.          The create key operation can be used to create any key type in Azure         Key Vault. If the named key already exists, Azure Key Vault creates a         new version of the key. It requires the keys/create permission.          :param vault_base_url: The vault name, for example          https://myvault.vault.azure.net.         :type vault_base_url: str         :param key_name: The name for the new key. The system will generate          the version name for the new key.         :type key_name: str         :param kty: The type of key to create. For valid values, see          JsonWebKeyType. Possible values include: 'EC', 'EC-HSM', 'RSA',          'RSA-HSM', 'oct'         :type kty: str or ~azure.keyvault.v2016_10_01.models.JsonWebKeyType         :param key_size: The key size in bits. For example: 2048, 3072, or          4096 for RSA.         :type key_size: int         :param key_ops:         :type key_ops: list[str or          ~azure.keyvault.v2016_10_01.models.JsonWebKeyOperation]         :param key_attributes:         :type key_attributes: ~azure.keyvault.v2016_10_01.models.KeyAttributes         :param tags: Application specific metadata in the form of key-value          pairs.         :type tags: dict[str, str]         :param curve: Elliptic curve name. For valid values, see          JsonWebKeyCurveName. Possible values include: 'P-256', 'P-384',          'P-521', 'SECP256K1'         :type curve: str or          ~azure.keyvault.v2016_10_01.models.JsonWebKeyCurveName         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: KeyBundle or ClientRawResponse if raw=true         :rtype: ~azure.keyvault.v2016_10_01.models.KeyBundle or          ~msrest.pipeline.ClientRawResponse         :raises:          :class:`KeyVaultErrorException<azure.keyvault.v2016_10_01.models.KeyVaultErrorException>`
Imports an externally created key, stores it, and returns key         parameters and attributes to the client.          The import key operation may be used to import any key type into an         Azure Key Vault. If the named key already exists, Azure Key Vault         creates a new version of the key. This operation requires the         keys/import permission.          :param vault_base_url: The vault name, for example          https://myvault.vault.azure.net.         :type vault_base_url: str         :param key_name: Name for the imported key.         :type key_name: str         :param key: The Json web key         :type key: ~azure.keyvault.v2016_10_01.models.JsonWebKey         :param hsm: Whether to import as a hardware key (HSM) or software key.         :type hsm: bool         :param key_attributes: The key management attributes.         :type key_attributes: ~azure.keyvault.v2016_10_01.models.KeyAttributes         :param tags: Application specific metadata in the form of key-value          pairs.         :type tags: dict[str, str]         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: KeyBundle or ClientRawResponse if raw=true         :rtype: ~azure.keyvault.v2016_10_01.models.KeyBundle or          ~msrest.pipeline.ClientRawResponse         :raises:          :class:`KeyVaultErrorException<azure.keyvault.v2016_10_01.models.KeyVaultErrorException>`
The update key operation changes specified attributes of a stored key         and can be applied to any key type and key version stored in Azure Key         Vault.          In order to perform this operation, the key must already exist in the         Key Vault. Note: The cryptographic material of a key itself cannot be         changed. This operation requires the keys/update permission.          :param vault_base_url: The vault name, for example          https://myvault.vault.azure.net.         :type vault_base_url: str         :param key_name: The name of key to update.         :type key_name: str         :param key_version: The version of the key to update.         :type key_version: str         :param key_ops: Json web key operations. For more information on          possible key operations, see JsonWebKeyOperation.         :type key_ops: list[str or          ~azure.keyvault.v2016_10_01.models.JsonWebKeyOperation]         :param key_attributes:         :type key_attributes: ~azure.keyvault.v2016_10_01.models.KeyAttributes         :param tags: Application specific metadata in the form of key-value          pairs.         :type tags: dict[str, str]         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: KeyBundle or ClientRawResponse if raw=true         :rtype: ~azure.keyvault.v2016_10_01.models.KeyBundle or          ~msrest.pipeline.ClientRawResponse         :raises:          :class:`KeyVaultErrorException<azure.keyvault.v2016_10_01.models.KeyVaultErrorException>`
Sets a secret in a specified key vault.          The SET operation adds a secret to the Azure Key Vault. If the named         secret already exists, Azure Key Vault creates a new version of that         secret. This operation requires the secrets/set permission.          :param vault_base_url: The vault name, for example          https://myvault.vault.azure.net.         :type vault_base_url: str         :param secret_name: The name of the secret.         :type secret_name: str         :param value: The value of the secret.         :type value: str         :param tags: Application specific metadata in the form of key-value          pairs.         :type tags: dict[str, str]         :param content_type: Type of the secret value such as a password.         :type content_type: str         :param secret_attributes: The secret management attributes.         :type secret_attributes:          ~azure.keyvault.v2016_10_01.models.SecretAttributes         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: SecretBundle or ClientRawResponse if raw=true         :rtype: ~azure.keyvault.v2016_10_01.models.SecretBundle or          ~msrest.pipeline.ClientRawResponse         :raises:          :class:`KeyVaultErrorException<azure.keyvault.v2016_10_01.models.KeyVaultErrorException>`
Sets the specified certificate issuer.          The SetCertificateIssuer operation adds or updates the specified         certificate issuer. This operation requires the certificates/setissuers         permission.          :param vault_base_url: The vault name, for example          https://myvault.vault.azure.net.         :type vault_base_url: str         :param issuer_name: The name of the issuer.         :type issuer_name: str         :param provider: The issuer provider.         :type provider: str         :param credentials: The credentials to be used for the issuer.         :type credentials:          ~azure.keyvault.v2016_10_01.models.IssuerCredentials         :param organization_details: Details of the organization as provided          to the issuer.         :type organization_details:          ~azure.keyvault.v2016_10_01.models.OrganizationDetails         :param attributes: Attributes of the issuer object.         :type attributes: ~azure.keyvault.v2016_10_01.models.IssuerAttributes         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: IssuerBundle or ClientRawResponse if raw=true         :rtype: ~azure.keyvault.v2016_10_01.models.IssuerBundle or          ~msrest.pipeline.ClientRawResponse         :raises:          :class:`KeyVaultErrorException<azure.keyvault.v2016_10_01.models.KeyVaultErrorException>`
Create a Service Bus client from a connection string.          :param conn_str: The connection string.         :type conn_str: str          Example:             .. literalinclude:: ../examples/async_examples/test_examples_async.py                 :start-after: [START create_async_servicebus_client_connstr]                 :end-before: [END create_async_servicebus_client_connstr]                 :language: python                 :dedent: 4                 :caption: Create a ServiceBusClient via a connection string.
Get an async client for a subscription entity.          :param topic_name: The name of the topic.         :type topic_name: str         :param subscription_name: The name of the subscription.         :type subscription_name: str         :rtype: ~azure.servicebus.aio.async_client.SubscriptionClient         :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.         :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the subscription is not found.          Example:             .. literalinclude:: ../examples/async_examples/test_examples_async.py                 :start-after: [START get_async_subscription_client]                 :end-before: [END get_async_subscription_client]                 :language: python                 :dedent: 4                 :caption: Get a TopicClient for the specified topic.
Get an async client for all subscription entities in the topic.          :param topic_name: The topic to list subscriptions for.         :type topic_name: str         :rtype: list[~azure.servicebus.aio.async_client.SubscriptionClient]         :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.         :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the topic is not found.
Send one or more messages to the current entity.          This operation will open a single-use connection, send the supplied messages, and close         connection. If the entity requires sessions, a session ID must be either         provided here, or set on each outgoing message.          :param messages: One or more messages to be sent.         :type messages: ~azure.servicebus.aio.async_message.Message or          list[~azure.servicebus.aio.async_message.Message]         :param message_timeout: The period in seconds during which the Message must be          sent. If the send is not completed in this time it will return a failure result.         :type message_timeout: int         :param session: An optional session ID. If supplied this session ID will be          applied to every outgoing message sent with this Sender.          If an individual message already has a session ID, that will be          used instead. If no session ID is supplied here, nor set on an outgoing          message, a ValueError will be raised if the entity is sessionful.         :type session: str or ~uuid.Guid         :raises: ~azure.servicebus.common.errors.MessageSendFailed         :returns: A list of the send results of all the messages. Each          send result is a tuple with two values. The first is a boolean, indicating `True`          if the message sent, or `False` if it failed. The second is an error if the message          failed, otherwise it will be `None`.         :rtype: list[tuple[bool, ~azure.servicebus.common.errors.MessageSendFailed]]          Example:             .. literalinclude:: ../examples/async_examples/test_examples_async.py                 :start-after: [START queue_client_send]                 :end-before: [END queue_client_send]                 :language: python                 :dedent: 4                 :caption: Send a single message.              .. literalinclude:: ../examples/async_examples/test_examples_async.py                 :start-after: [START queue_client_send_multiple]                 :end-before: [END queue_client_send_multiple]                 :language: python                 :dedent: 4                 :caption: Send multiple messages.
Get a Sender for the Service Bus endpoint.          A Sender represents a single open connection within which multiple send operations can be made.          :param message_timeout: The period in seconds during which messages sent with          this Sender must be sent. If the send is not completed in this time it will fail.         :type message_timeout: int         :param session: An optional session ID. If supplied this session ID will be          applied to every outgoing message sent with this Sender.          If an individual message already has a session ID, that will be          used instead. If no session ID is supplied here, nor set on an outgoing          message, a ValueError will be raised if the entity is sessionful.         :type session: str or ~uuid.Guid         :returns: A Sender instance with an unopened connection.         :rtype: ~azure.servicebus.aio.async_send_handler.Sender          Example:             .. literalinclude:: ../examples/async_examples/test_examples_async.py                 :start-after: [START open_close_sender_context]                 :end-before: [END open_close_sender_context]                 :language: python                 :dedent: 4                 :caption: Send multiple messages with a Sender.
Get a Receiver for the Service Bus endpoint.          A Receiver represents a single open connection with which multiple receive operations can be made.          :param session: A specific session from which to receive. This must be specified for a          sessionful entity, otherwise it must be None. In order to receive the next available          session, set this to NEXT_AVAILABLE.         :type session: str or ~azure.servicebus.common.constants.NEXT_AVAILABLE         :param prefetch: The maximum number of messages to cache with each request to the service.          The default value is 0, meaning messages will be received from the service and processed          one at a time. Increasing this value will improve message throughput performance but increase          the chance that messages will expire while they are cached if they're not processed fast enough.         :type prefetch: int         :param mode: The mode with which messages will be retrieved from the entity. The two options          are PeekLock and ReceiveAndDelete. Messages received with PeekLock must be settled within a given          lock period before they will be removed from the queue. Messages received with ReceiveAndDelete          will be immediately removed from the queue, and cannot be subsequently rejected or re-received if          the client fails to process the message. The default mode is PeekLock.         :type mode: ~azure.servicebus.common.constants.ReceiveSettleMode         :param idle_timeout: The timeout in seconds between received messages after which the receiver will          automatically shutdown. The default value is 0, meaning no timeout.         :type idle_timeout: int         :returns: A Receiver instance with an unopened connection.         :rtype: ~azure.servicebus.aio.async_receive_handler.Receiver          Example:             .. literalinclude:: ../examples/async_examples/test_examples_async.py                 :start-after: [START open_close_receiver_context]                 :end-before: [END open_close_receiver_context]                 :language: python                 :dedent: 4                 :caption: Receive messages with a Receiver.
Get a Receiver for the deadletter endpoint of the entity.          A Receiver represents a single open connection with which multiple receive operations can be made.          :param transfer_deadletter: Whether to connect to the transfer deadletter queue, or the standard          deadletter queue. Default is False, using the standard deadletter endpoint.         :type transfer_deadletter: bool         :param prefetch: The maximum number of messages to cache with each request to the service.          The default value is 0, meaning messages will be received from the service and processed          one at a time. Increasing this value will improve message throughput performance but increase          the change that messages will expire while they are cached if they're not processed fast enough.         :type prefetch: int         :param mode: The mode with which messages will be retrieved from the entity. The two options          are PeekLock and ReceiveAndDelete. Messages received with PeekLock must be settled within a given          lock period before they will be removed from the queue. Messages received with ReceiveAndDelete          will be immediately removed from the queue, and cannot be subsequently rejected or re-received if          the client fails to process the message. The default mode is PeekLock.         :type mode: ~azure.servicebus.common.constants.ReceiveSettleMode         :param idle_timeout: The timeout in seconds between received messages after which the receiver will          automatically shutdown. The default value is 0, meaning no timeout.         :type idle_timeout: int         :returns: A Receiver instance with an unopened Connection.         :rtype: ~azure.servicebus.aio.async_receive_handler.Receiver          Example:             .. literalinclude:: ../examples/async_examples/test_examples_async.py                 :start-after: [START receiver_deadletter_messages]                 :end-before: [END receiver_deadletter_messages]                 :language: python                 :dedent: 4                 :caption: Receive dead-lettered messages.
Extracts request id from response header.
Performs a GET request and returns the response.          path:             Path to the resource.             Ex: '/<subscription-id>/services/hostedservices/<service-name>'         x_ms_version:             If specified, this is used for the x-ms-version header.             Otherwise, self.x_ms_version is used.
Performs a PUT request and returns the response.          path:             Path to the resource.             Ex: '/<subscription-id>/services/hostedservices/<service-name>'         body:             Body for the PUT request.         x_ms_version:             If specified, this is used for the x-ms-version header.             Otherwise, self.x_ms_version is used.
Waits for an asynchronous operation to complete.          This calls get_operation_status in a loop and returns when the expected         status is reached. The result of get_operation_status is returned. By         default, an exception is raised on timeout or error status.          request_id:             The request ID for the request you wish to track.         wait_for_status:             Status to wait for. Default is 'Succeeded'.         timeout:             Total timeout in seconds. Default is 30s.         sleep_interval:             Sleep time in seconds for each iteration. Default is 5s.         progress_callback:             Callback for each iteration.             Default prints '.'.             Set it to None for no progress notification.         success_callback:             Callback on success. Default prints newline.             Set it to None for no success notification.         failure_callback:             Callback on failure. Default prints newline+error details then             raises exception.             Set it to None for no failure notification.
Returns the status of the specified operation. After calling an         asynchronous operation, you can call Get Operation Status to determine         whether the operation has succeeded, failed, or is still in progress.          request_id:             The request ID for the request you wish to track.
Add additional headers for management.
Assumed called on Travis, to prepare a package to be deployed      This method prints on stdout for Travis.     Return is obj to pass to sys.exit() directly
List certificates in a specified key vault.          The GetCertificates operation returns the set of certificates resources         in the specified key vault. This operation requires the         certificates/list permission.          :param vault_base_url: The vault name, for example          https://myvault.vault.azure.net.         :type vault_base_url: str         :param maxresults: Maximum number of results to return in a page. If          not specified the service will return up to 25 results.         :type maxresults: int         :param include_pending: Specifies whether to include certificates          which are not completely provisioned.         :type include_pending: bool         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: An iterator like instance of CertificateItem         :rtype:          ~azure.keyvault.v7_0.models.CertificateItemPaged[~azure.keyvault.v7_0.models.CertificateItem]         :raises:          :class:`KeyVaultErrorException<azure.keyvault.v7_0.models.KeyVaultErrorException>`
Get list of available service bus regions.
List the service bus namespaces defined on the account.
Get details about a specific namespace.          name:             Name of the service bus namespace.
Create a new service bus namespace.          name:             Name of the service bus namespace to create.         region:             Region to create the namespace in.
Delete a service bus namespace.          name:             Name of the service bus namespace to delete.
Checks to see if the specified service bus namespace is available, or         if it has already been taken.          name:             Name of the service bus namespace to validate.
Retrieves the topics in the service namespace.          name:             Name of the service bus namespace.
Retrieves the notification hubs in the service namespace.          name:             Name of the service bus namespace.
Retrieves the relays in the service namespace.          name:             Name of the service bus namespace.
This operation gets rollup data for Service Bus metrics queue.         Rollup data includes the time granularity for the telemetry aggregation as well as         the retention settings for each time granularity.          name:             Name of the service bus namespace.         queue_name:             Name of the service bus queue in this namespace.         metric:             name of a supported metric
This operation gets rollup data for Service Bus metrics topic.         Rollup data includes the time granularity for the telemetry aggregation as well as         the retention settings for each time granularity.          name:             Name of the service bus namespace.         topic_name:             Name of the service bus queue in this namespace.         metric:             name of a supported metric
This operation gets rollup data for Service Bus metrics notification hub.         Rollup data includes the time granularity for the telemetry aggregation as well as         the retention settings for each time granularity.          name:             Name of the service bus namespace.         hub_name:             Name of the service bus notification hub in this namespace.         metric:             name of a supported metric
This operation gets rollup data for Service Bus metrics relay.         Rollup data includes the time granularity for the telemetry aggregation as well as         the retention settings for each time granularity.          name:             Name of the service bus namespace.         relay_name:             Name of the service bus relay in this namespace.         metric:             name of a supported metric
Create a virtual environment in a directory.
Create a venv with these packages in a temp dir and yielf the env.      packages should be an iterable of pip version instructio (e.g. package~=1.2.3)
Create a new Azure SQL Database server.          admin_login:             The administrator login name for the new server.         admin_password:             The administrator login password for the new server.         location:             The region to deploy the new server.
Reset the administrator password for a server.          server_name:             Name of the server to change the password.         admin_password:             The new administrator password for the server.
Gets quotas for an Azure SQL Database Server.          server_name:             Name of the server.
Gets the event logs for an Azure SQL Database Server.          server_name:             Name of the server to retrieve the event logs from.         start_date:             The starting date and time of the events to retrieve in UTC format,             for example '2011-09-28 16:05:00'.         interval_size_in_minutes:             Size of the event logs to retrieve (in minutes).             Valid values are: 5, 60, or 1440.         event_types:             The event type of the log entries you want to retrieve.             Valid values are:                  - connection_successful                 - connection_failed                 - connection_terminated                 - deadlock                 - throttling                 - throttling_long_transaction             To return all event types pass in an empty string.
Creates an Azure SQL Database server firewall rule.          server_name:             Name of the server to set the firewall rule on.          name:             The name of the new firewall rule.         start_ip_address:             The lowest IP address in the range of the server-level firewall             setting. IP addresses equal to or greater than this can attempt to             connect to the server. The lowest possible IP address is 0.0.0.0.         end_ip_address:             The highest IP address in the range of the server-level firewall             setting. IP addresses equal to or less than this can attempt to             connect to the server. The highest possible IP address is             255.255.255.255.
Update a firewall rule for an Azure SQL Database server.          server_name:             Name of the server to set the firewall rule on.          name:             The name of the firewall rule to update.         start_ip_address:             The lowest IP address in the range of the server-level firewall             setting. IP addresses equal to or greater than this can attempt to             connect to the server. The lowest possible IP address is 0.0.0.0.         end_ip_address:             The highest IP address in the range of the server-level firewall             setting. IP addresses equal to or less than this can attempt to             connect to the server. The highest possible IP address is             255.255.255.255.
Deletes an Azure SQL Database server firewall rule.          server_name:             Name of the server with the firewall rule you want to delete.         name:             Name of the firewall rule you want to delete.
Retrieves the set of firewall rules for an Azure SQL Database Server.          server_name:             Name of the server.
Gets the service level objectives for an Azure SQL Database server.          server_name:             Name of the server.
Creates a new Azure SQL Database.          server_name:             Name of the server to contain the new database.         name:             Required. The name for the new database. See Naming Requirements             in Azure SQL Database General Guidelines and Limitations and             Database Identifiers for more information.         service_objective_id:             Required. The GUID corresponding to the performance level for             Edition. See List Service Level Objectives for current values.         edition:             Optional. The Service Tier (Edition) for the new database. If             omitted, the default is Web. Valid values are Web, Business,             Basic, Standard, and Premium. See Azure SQL Database Service Tiers             (Editions) and Web and Business Edition Sunset FAQ for more             information.         collation_name:             Optional. The database collation. This can be any collation             supported by SQL. If omitted, the default collation is used. See             SQL Server Collation Support in Azure SQL Database General             Guidelines and Limitations for more information.         max_size_bytes:             Optional. Sets the maximum size, in bytes, for the database. This             value must be within the range of allowed values for Edition. If             omitted, the default value for the edition is used. See Azure SQL             Database Service Tiers (Editions) for current maximum databases             sizes. Convert MB or GB values to bytes.             1 MB = 1048576 bytes. 1 GB = 1073741824 bytes.
Updates existing database details.          server_name:             Name of the server to contain the new database.         name:             Required. The name for the new database. See Naming Requirements             in Azure SQL Database General Guidelines and Limitations and             Database Identifiers for more information.         new_database_name:             Optional. The new name for the new database.         service_objective_id:             Optional. The new service level to apply to the database. For more             information about service levels, see Azure SQL Database Service             Tiers and Performance Levels. Use List Service Level Objectives to             get the correct ID for the desired service objective.         edition:             Optional. The new edition for the new database.         max_size_bytes:             Optional. The new size of the database in bytes. For information on             available sizes for each edition, see Azure SQL Database Service             Tiers (Editions).
Deletes an Azure SQL Database.          server_name:             Name of the server where the database is located.         name:             Name of the database to delete.
List the SQL databases defined on the specified server name
Gets all legal agreements that user needs to accept before purchasing a         domain.          Gets all legal agreements that user needs to accept before purchasing a         domain.          :param name: Name of the top-level domain.         :type name: str         :param include_privacy: If <code>true</code>, then the list of          agreements will include agreements for domain privacy as well;          otherwise, <code>false</code>.         :type include_privacy: bool         :param for_transfer: If <code>true</code>, then the list of agreements          will include agreements for domain transfer as well; otherwise,          <code>false</code>.         :type for_transfer: bool         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: An iterator like instance of TldLegalAgreement         :rtype:          ~azure.mgmt.web.models.TldLegalAgreementPaged[~azure.mgmt.web.models.TldLegalAgreement]         :raises:          :class:`DefaultErrorResponseException<azure.mgmt.web.models.DefaultErrorResponseException>`
Close down the handler connection.          If the handler has already closed,         this operation will do nothing. An optional exception can be passed in to         indicate that the handler was shutdown due to error.         It is recommended to open a handler within a context manager as         opposed to calling the method directly.          .. note:: This operation is not thread-safe.          :param exception: An optional exception if the handler is closing          due to an error.         :type exception: Exception          Example:             .. literalinclude:: ../examples/async_examples/test_examples_async.py                 :start-after: [START open_close_sender_directly]                 :end-before: [END open_close_sender_directly]                 :language: python                 :dedent: 4                 :caption: Explicitly open and close a Sender.
Close down the receiver connection.          If the receiver has already closed, this operation will do nothing. An optional         exception can be passed in to indicate that the handler was shutdown due to error.         It is recommended to open a handler within a context manager as         opposed to calling the method directly.         The receiver will be implicitly closed on completion of the message iterator,         however this method will need to be called explicitly if the message iterator is not run         to completion.          .. note:: This operation is not thread-safe.          :param exception: An optional exception if the handler is closing          due to an error.         :type exception: Exception          Example:             .. literalinclude:: ../examples/async_examples/test_examples_async.py                 :start-after: [START open_close_receiver_directly]                 :end-before: [END open_close_receiver_directly]                 :language: python                 :dedent: 4                 :caption: Iterate then explicitly close a Receiver.
Get the session state.          Returns None if no state has been set.          :rtype: str          Example:             .. literalinclude:: ../examples/async_examples/test_examples_async.py                 :start-after: [START set_session_state]                 :end-before: [END set_session_state]                 :language: python                 :dedent: 4                 :caption: Getting and setting the state of a session.
Set the session state.          :param state: The state value.         :type state: str or bytes or bytearray          Example:             .. literalinclude:: ../examples/async_examples/test_examples_async.py                 :start-after: [START set_session_state]                 :end-before: [END set_session_state]                 :language: python                 :dedent: 4                 :caption: Getting and setting the state of a session.
Receive messages that have previously been deferred.          This operation can only receive deferred messages from the current session.         When receiving deferred messages from a partitioned entity, all of the supplied         sequence numbers must be messages from the same partition.          :param sequence_numbers: A list of the sequence numbers of messages that have been          deferred.         :type sequence_numbers: list[int]         :param mode: The receive mode, default value is PeekLock.         :type mode: ~azure.servicebus.common.constants.ReceiveSettleMode         :rtype: list[~azure.servicebus.aio.async_message.DeferredMessage]          Example:             .. literalinclude:: ../examples/async_examples/test_examples_async.py                 :start-after: [START receiver_defer_session_messages]                 :end-before: [END receiver_defer_session_messages]                 :language: python                 :dedent: 8                 :caption: Defer messages, then retrieve them by sequence number.
Merges two `Reservation`s.          Merge the specified `Reservation`s into a new `Reservation`. The two         `Reservation`s being merged must have same properties.          :param reservation_order_id: Order Id of the reservation         :type reservation_order_id: str         :param sources: Format of the resource id should be          /providers/Microsoft.Capacity/reservationOrders/{reservationOrderId}/reservations/{reservationId}         :type sources: list[str]         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns list or          ClientRawResponse<list> if raw==True         :rtype:          ~msrestazure.azure_operation.AzureOperationPoller[list[~azure.mgmt.reservations.models.ReservationResponse]]          or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[list[~azure.mgmt.reservations.models.ReservationResponse]]]         :raises:          :class:`ErrorException<azure.mgmt.reservations.models.ErrorException>`
Verifies that the challenge is a Bearer challenge and returns the key=value pairs.
Purges data in an Log Analytics workspace by a set of user-defined         filters.          :param resource_group_name: The name of the resource group to get. The          name is case insensitive.         :type resource_group_name: str         :param workspace_name: Log Analytics workspace name         :type workspace_name: str         :param table: Table from which to purge data.         :type table: str         :param filters: The set of columns and filters (queries) to run over          them to purge the resulting data.         :type filters:          list[~azure.mgmt.loganalytics.models.WorkspacePurgeBodyFilters]         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns object or          ClientRawResponse<object> if raw==True         :rtype: ~msrestazure.azure_operation.AzureOperationPoller[object] or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[object]]         :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`
Handle connection and service errors.      Called internally when an event has failed to send so we     can parse the error to determine whether we should attempt     to retry sending the event again.     Returns the action to take according to error type.      :param error: The error received in the send attempt.     :type error: Exception     :rtype: ~uamqp.errors.ErrorAction
Creates a new queue. Once created, this queue's resource manifest is         immutable.          queue_name:             Name of the queue to create.         queue:             Queue object to create.         fail_on_exist:             Specify whether to throw an exception when the queue exists.
Deletes an existing queue. This operation will also remove all         associated state including messages in the queue.          queue_name:             Name of the queue to delete.         fail_not_exist:             Specify whether to throw an exception if the queue doesn't exist.
Retrieves an existing queue.          queue_name:             Name of the queue.
Creates a new topic. Once created, this topic resource manifest is         immutable.          topic_name:             Name of the topic to create.         topic:             Topic object to create.         fail_on_exist:             Specify whether to throw an exception when the topic exists.
Retrieves the description for the specified topic.          topic_name:             Name of the topic.
Creates a new rule. Once created, this rule's resource manifest is         immutable.          topic_name:             Name of the topic.         subscription_name:             Name of the subscription.         rule_name:             Name of the rule.         fail_on_exist:             Specify whether to throw an exception when the rule exists.
Retrieves the description for the specified rule.          topic_name:             Name of the topic.         subscription_name:             Name of the subscription.         rule_name:             Name of the rule.
Retrieves the rules that exist under the specified subscription.          topic_name:             Name of the topic.         subscription_name:             Name of the subscription.
Creates a new subscription. Once created, this subscription resource         manifest is immutable.          topic_name:             Name of the topic.         subscription_name:             Name of the subscription.         fail_on_exist:             Specify whether throw exception when subscription exists.
Gets an existing subscription.          topic_name:             Name of the topic.         subscription_name:             Name of the subscription.
Retrieves the subscriptions in the specified topic.          topic_name:             Name of the topic.
Enqueues a message into the specified topic. The limit to the number         of messages which may be present in the topic is governed by the         message size in MaxTopicSizeInBytes. If this message causes the topic         to exceed its quota, a quota exceeded error is returned and the         message will be rejected.          topic_name:             Name of the topic.         message:             Message object containing message body and properties.
Unlock a message for processing by other receivers on a given         subscription. This operation deletes the lock object, causing the         message to be unlocked. A message must have first been locked by a         receiver before this operation is called.          topic_name:             Name of the topic.         subscription_name:             Name of the subscription.         sequence_number:             The sequence number of the message to be unlocked as returned in             BrokerProperties['SequenceNumber'] by the Peek Message operation.         lock_token:             The ID of the lock as returned by the Peek Message operation in             BrokerProperties['LockToken']
Sends a batch of messages into the specified queue. The limit to the number of         messages which may be present in the topic is governed by the message         size the MaxTopicSizeInMegaBytes. If this message will cause the queue         to exceed its quota, a quota exceeded error is returned and the         message will be rejected.          queue_name:             Name of the queue.         messages:             List of message objects containing message body and properties.
Unlocks a message for processing by other receivers on a given         queue. This operation deletes the lock object, causing the         message to be unlocked. A message must have first been locked by a         receiver before this operation is called.          queue_name:             Name of the queue.         sequence_number:             The sequence number of the message to be unlocked as returned in             BrokerProperties['SequenceNumber'] by the Peek Message operation.         lock_token:             The ID of the lock as returned by the Peek Message operation in             BrokerProperties['LockToken']
Receive a message from a queue for processing.          queue_name:             Name of the queue.         peek_lock:             Optional. True to retrieve and lock the message. False to read and             delete the message. Default is True (lock).         timeout:             Optional. The timeout parameter is expressed in seconds.
Receive a message from a subscription for processing.          topic_name:             Name of the topic.         subscription_name:             Name of the subscription.         peek_lock:             Optional. True to retrieve and lock the message. False to read and             delete the message. Default is True (lock).         timeout:             Optional. The timeout parameter is expressed in seconds.
Creates a new Event Hub.          hub_name:             Name of event hub.         hub:             Optional. Event hub properties. Instance of EventHub class.         hub.message_retention_in_days:             Number of days to retain the events for this Event Hub.         hub.status:             Status of the Event Hub (enabled or disabled).         hub.user_metadata:             User metadata.         hub.partition_count:             Number of shards on the Event Hub.         fail_on_exist:             Specify whether to throw an exception when the event hub exists.
Updates an Event Hub.          hub_name:             Name of event hub.         hub:             Optional. Event hub properties. Instance of EventHub class.         hub.message_retention_in_days:             Number of days to retain the events for this Event Hub.
Retrieves an existing event hub.          hub_name:             Name of the event hub.
Sends a new message event to an Event Hub.
Add additional headers for Service Bus.
return the signed string with token.
Check if token expires or not.
Returns token for the request.          host:             the Service Bus service request.         path:             the Service Bus service request.
pulls the query string out of the URI and moves it into         the query portion of the request object.  If there are already         query parameters on the request the parameters in the URI will         appear after the existing parameters
Reset Service Principal Profile of a managed cluster.          Update the service principal Profile for a managed cluster.          :param resource_group_name: The name of the resource group.         :type resource_group_name: str         :param resource_name: The name of the managed cluster resource.         :type resource_name: str         :param client_id: The ID for the service principal.         :type client_id: str         :param secret: The secret password associated with the service          principal in plain text.         :type secret: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns None or          ClientRawResponse<None> if raw==True         :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]         :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`
Deletes itself if find queue name or topic name and subscription         name.
Unlocks itself if find queue name or topic name and subscription         name.
Renew lock on itself if find queue name or topic name and subscription         name.
add addtional headers to request for message request.
return the current message as expected by batch body format
Gets the health of a Service Fabric cluster.          Use EventsHealthStateFilter to filter the collection of health events         reported on the cluster based on the health state.         Similarly, use NodesHealthStateFilter and ApplicationsHealthStateFilter         to filter the collection of nodes and applications returned based on         their aggregated health state.          :param nodes_health_state_filter: Allows filtering of the node health          state objects returned in the result of cluster health query          based on their health state. The possible values for this parameter          include integer value of one of the          following health states. Only nodes that match the filter are          returned. All nodes are used to evaluate the aggregated health state.          If not specified, all entries are returned.          The state values are flag-based enumeration, so the value could be a          combination of these values obtained using bitwise 'OR' operator.          For example, if the provided value is 6 then health state of nodes          with HealthState value of OK (2) and Warning (4) are returned.          - Default - Default value. Matches any HealthState. The value is zero.          - None - Filter that doesn't match any HealthState value. Used in          order to return no results on a given collection of states. The value          is 1.          - Ok - Filter that matches input with HealthState value Ok. The value          is 2.          - Warning - Filter that matches input with HealthState value Warning.          The value is 4.          - Error - Filter that matches input with HealthState value Error. The          value is 8.          - All - Filter that matches input with any HealthState value. The          value is 65535.         :type nodes_health_state_filter: int         :param applications_health_state_filter: Allows filtering of the          application health state objects returned in the result of cluster          health          query based on their health state.          The possible values for this parameter include integer value obtained          from members or bitwise operations          on members of HealthStateFilter enumeration. Only applications that          match the filter are returned.          All applications are used to evaluate the aggregated health state. If          not specified, all entries are returned.          The state values are flag-based enumeration, so the value could be a          combination of these values obtained using bitwise 'OR' operator.          For example, if the provided value is 6 then health state of          applications with HealthState value of OK (2) and Warning (4) are          returned.          - Default - Default value. Matches any HealthState. The value is zero.          - None - Filter that doesn't match any HealthState value. Used in          order to return no results on a given collection of states. The value          is 1.          - Ok - Filter that matches input with HealthState value Ok. The value          is 2.          - Warning - Filter that matches input with HealthState value Warning.          The value is 4.          - Error - Filter that matches input with HealthState value Error. The          value is 8.          - All - Filter that matches input with any HealthState value. The          value is 65535.         :type applications_health_state_filter: int         :param events_health_state_filter: Allows filtering the collection of          HealthEvent objects returned based on health state.          The possible values for this parameter include integer value of one of          the following health states.          Only events that match the filter are returned. All events are used to          evaluate the aggregated health state.          If not specified, all entries are returned. The state values are          flag-based enumeration, so the value could be a combination of these          values, obtained using the bitwise 'OR' operator. For example, If the          provided value is 6 then all of the events with HealthState value of          OK (2) and Warning (4) are returned.          - Default - Default value. Matches any HealthState. The value is zero.          - None - Filter that doesn't match any HealthState value. Used in          order to return no results on a given collection of states. The value          is 1.          - Ok - Filter that matches input with HealthState value Ok. The value          is 2.          - Warning - Filter that matches input with HealthState value Warning.          The value is 4.          - Error - Filter that matches input with HealthState value Error. The          value is 8.          - All - Filter that matches input with any HealthState value. The          value is 65535.         :type events_health_state_filter: int         :param exclude_health_statistics: Indicates whether the health          statistics should be returned as part of the query result. False by          default.          The statistics show the number of children entities in health state          Ok, Warning, and Error.         :type exclude_health_statistics: bool         :param include_system_application_health_statistics: Indicates whether          the health statistics should include the fabric:/System application          health statistics. False by default.          If IncludeSystemApplicationHealthStatistics is set to true, the health          statistics include the entities that belong to the fabric:/System          application.          Otherwise, the query result includes health statistics only for user          applications.          The health statistics must be included in the query result for this          parameter to be applied.         :type include_system_application_health_statistics: bool         :param timeout: The server timeout for performing the operation in          seconds. This timeout specifies the time duration that the client is          willing to wait for the requested operation to complete. The default          value for this parameter is 60 seconds.         :type timeout: long         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: ClusterHealth or ClientRawResponse if raw=true         :rtype: ~azure.servicefabric.models.ClusterHealth or          ~msrest.pipeline.ClientRawResponse         :raises:          :class:`FabricErrorException<azure.servicefabric.models.FabricErrorException>`
Gets the health of a Service Fabric cluster using the specified policy.          Use EventsHealthStateFilter to filter the collection of health events         reported on the cluster based on the health state.         Similarly, use NodesHealthStateFilter and ApplicationsHealthStateFilter         to filter the collection of nodes and applications returned based on         their aggregated health state.         Use ClusterHealthPolicies to override the health policies used to         evaluate the health.          :param nodes_health_state_filter: Allows filtering of the node health          state objects returned in the result of cluster health query          based on their health state. The possible values for this parameter          include integer value of one of the          following health states. Only nodes that match the filter are          returned. All nodes are used to evaluate the aggregated health state.          If not specified, all entries are returned.          The state values are flag-based enumeration, so the value could be a          combination of these values obtained using bitwise 'OR' operator.          For example, if the provided value is 6 then health state of nodes          with HealthState value of OK (2) and Warning (4) are returned.          - Default - Default value. Matches any HealthState. The value is zero.          - None - Filter that doesn't match any HealthState value. Used in          order to return no results on a given collection of states. The value          is 1.          - Ok - Filter that matches input with HealthState value Ok. The value          is 2.          - Warning - Filter that matches input with HealthState value Warning.          The value is 4.          - Error - Filter that matches input with HealthState value Error. The          value is 8.          - All - Filter that matches input with any HealthState value. The          value is 65535.         :type nodes_health_state_filter: int         :param applications_health_state_filter: Allows filtering of the          application health state objects returned in the result of cluster          health          query based on their health state.          The possible values for this parameter include integer value obtained          from members or bitwise operations          on members of HealthStateFilter enumeration. Only applications that          match the filter are returned.          All applications are used to evaluate the aggregated health state. If          not specified, all entries are returned.          The state values are flag-based enumeration, so the value could be a          combination of these values obtained using bitwise 'OR' operator.          For example, if the provided value is 6 then health state of          applications with HealthState value of OK (2) and Warning (4) are          returned.          - Default - Default value. Matches any HealthState. The value is zero.          - None - Filter that doesn't match any HealthState value. Used in          order to return no results on a given collection of states. The value          is 1.          - Ok - Filter that matches input with HealthState value Ok. The value          is 2.          - Warning - Filter that matches input with HealthState value Warning.          The value is 4.          - Error - Filter that matches input with HealthState value Error. The          value is 8.          - All - Filter that matches input with any HealthState value. The          value is 65535.         :type applications_health_state_filter: int         :param events_health_state_filter: Allows filtering the collection of          HealthEvent objects returned based on health state.          The possible values for this parameter include integer value of one of          the following health states.          Only events that match the filter are returned. All events are used to          evaluate the aggregated health state.          If not specified, all entries are returned. The state values are          flag-based enumeration, so the value could be a combination of these          values, obtained using the bitwise 'OR' operator. For example, If the          provided value is 6 then all of the events with HealthState value of          OK (2) and Warning (4) are returned.          - Default - Default value. Matches any HealthState. The value is zero.          - None - Filter that doesn't match any HealthState value. Used in          order to return no results on a given collection of states. The value          is 1.          - Ok - Filter that matches input with HealthState value Ok. The value          is 2.          - Warning - Filter that matches input with HealthState value Warning.          The value is 4.          - Error - Filter that matches input with HealthState value Error. The          value is 8.          - All - Filter that matches input with any HealthState value. The          value is 65535.         :type events_health_state_filter: int         :param exclude_health_statistics: Indicates whether the health          statistics should be returned as part of the query result. False by          default.          The statistics show the number of children entities in health state          Ok, Warning, and Error.         :type exclude_health_statistics: bool         :param include_system_application_health_statistics: Indicates whether          the health statistics should include the fabric:/System application          health statistics. False by default.          If IncludeSystemApplicationHealthStatistics is set to true, the health          statistics include the entities that belong to the fabric:/System          application.          Otherwise, the query result includes health statistics only for user          applications.          The health statistics must be included in the query result for this          parameter to be applied.         :type include_system_application_health_statistics: bool         :param timeout: The server timeout for performing the operation in          seconds. This timeout specifies the time duration that the client is          willing to wait for the requested operation to complete. The default          value for this parameter is 60 seconds.         :type timeout: long         :param application_health_policy_map: Defines a map that contains          specific application health policies for different applications.          Each entry specifies as key the application name and as value an          ApplicationHealthPolicy used to evaluate the application health.          If an application is not specified in the map, the application health          evaluation uses the ApplicationHealthPolicy found in its application          manifest or the default application health policy (if no health policy          is defined in the manifest).          The map is empty by default.         :type application_health_policy_map:          list[~azure.servicefabric.models.ApplicationHealthPolicyMapItem]         :param cluster_health_policy: Defines a health policy used to evaluate          the health of the cluster or of a cluster node.         :type cluster_health_policy:          ~azure.servicefabric.models.ClusterHealthPolicy         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: ClusterHealth or ClientRawResponse if raw=true         :rtype: ~azure.servicefabric.models.ClusterHealth or          ~msrest.pipeline.ClientRawResponse         :raises:          :class:`FabricErrorException<azure.servicefabric.models.FabricErrorException>`
Removes or unregisters a Service Fabric application type from the         cluster.          This operation can only be performed if all application instances of         the application type have been deleted. Once the application type is         unregistered, no new application instances can be created for this         particular application type.          :param application_type_name: The name of the application type.         :type application_type_name: str         :param application_type_version: The version of the application type          as defined in the application manifest.         :type application_type_version: str         :param timeout: The server timeout for performing the operation in          seconds. This timeout specifies the time duration that the client is          willing to wait for the requested operation to complete. The default          value for this parameter is 60 seconds.         :type timeout: long         :param async_parameter: The flag indicating whether or not unprovision          should occur asynchronously. When set to true, the unprovision          operation returns when the request is accepted by the system, and the          unprovision operation continues without any timeout limit. The default          value is false. However, we recommend setting it to true for large          application packages that were provisioned.         :type async_parameter: bool         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: None or ClientRawResponse if raw=true         :rtype: None or ~msrest.pipeline.ClientRawResponse         :raises:          :class:`FabricErrorException<azure.servicefabric.models.FabricErrorException>`
Gets a list of repair tasks matching the given filters.          This API supports the Service Fabric platform; it is not meant to be         used directly from your code.          :param task_id_filter: The repair task ID prefix to be matched.         :type task_id_filter: str         :param state_filter: A bitwise-OR of the following values, specifying          which task states should be included in the result list.          - 1 - Created          - 2 - Claimed          - 4 - Preparing          - 8 - Approved          - 16 - Executing          - 32 - Restoring          - 64 - Completed         :type state_filter: int         :param executor_filter: The name of the repair executor whose claimed          tasks should be included in the list.         :type executor_filter: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: list or ClientRawResponse if raw=true         :rtype: list[~azure.servicefabric.models.RepairTask] or          ~msrest.pipeline.ClientRawResponse         :raises:          :class:`FabricErrorException<azure.servicefabric.models.FabricErrorException>`
Submits a property batch.          Submits a batch of property operations. Either all or none of the         operations will be committed.          :param name_id: The Service Fabric name, without the 'fabric:' URI          scheme.         :type name_id: str         :param timeout: The server timeout for performing the operation in          seconds. This timeout specifies the time duration that the client is          willing to wait for the requested operation to complete. The default          value for this parameter is 60 seconds.         :type timeout: long         :param operations: A list of the property batch operations to be          executed.         :type operations:          list[~azure.servicefabric.models.PropertyBatchOperation]         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: PropertyBatchInfo or ClientRawResponse if raw=true         :rtype: ~azure.servicefabric.models.PropertyBatchInfo or          ~msrest.pipeline.ClientRawResponse         :raises:          :class:`FabricErrorException<azure.servicefabric.models.FabricErrorException>`
Simple error handler for azure.
Start capturing network packets for the site.          Start capturing network packets for the site.          :param resource_group_name: Name of the resource group to which the          resource belongs.         :type resource_group_name: str         :param name: The name of the web app.         :type name: str         :param duration_in_seconds: The duration to keep capturing in seconds.         :type duration_in_seconds: int         :param max_frame_length: The maximum frame length in bytes (Optional).         :type max_frame_length: int         :param sas_url: The Blob URL to store capture file.         :type sas_url: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns list or          ClientRawResponse<list> if raw==True         :rtype:          ~msrestazure.azure_operation.AzureOperationPoller[list[~azure.mgmt.web.models.NetworkTrace]]          or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[list[~azure.mgmt.web.models.NetworkTrace]]]         :raises:          :class:`DefaultErrorResponseException<azure.mgmt.web.models.DefaultErrorResponseException>`
Get the difference in configuration settings between two web app slots.          Get the difference in configuration settings between two web app slots.          :param resource_group_name: Name of the resource group to which the          resource belongs.         :type resource_group_name: str         :param name: Name of the app.         :type name: str         :param slot: Name of the source slot. If a slot is not specified, the          production slot is used as the source slot.         :type slot: str         :param target_slot: Destination deployment slot during swap operation.         :type target_slot: str         :param preserve_vnet: <code>true</code> to preserve Virtual Network to          the slot during swap; otherwise, <code>false</code>.         :type preserve_vnet: bool         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: An iterator like instance of SlotDifference         :rtype:          ~azure.mgmt.web.models.SlotDifferencePaged[~azure.mgmt.web.models.SlotDifference]         :raises:          :class:`DefaultErrorResponseException<azure.mgmt.web.models.DefaultErrorResponseException>`
Swaps two deployment slots of an app.          Swaps two deployment slots of an app.          :param resource_group_name: Name of the resource group to which the          resource belongs.         :type resource_group_name: str         :param name: Name of the app.         :type name: str         :param slot: Name of the source slot. If a slot is not specified, the          production slot is used as the source slot.         :type slot: str         :param target_slot: Destination deployment slot during swap operation.         :type target_slot: str         :param preserve_vnet: <code>true</code> to preserve Virtual Network to          the slot during swap; otherwise, <code>false</code>.         :type preserve_vnet: bool         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns None or          ClientRawResponse<None> if raw==True         :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]         :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`
Execute OData query.          Executes an OData query for events.          :param app_id: ID of the application. This is Application ID from the          API Access settings blade in the Azure portal.         :type app_id: str         :param event_type: The type of events to query; either a standard          event type (`traces`, `customEvents`, `pageViews`, `requests`,          `dependencies`, `exceptions`, `availabilityResults`) or `$all` to          query across all event types. Possible values include: '$all',          'traces', 'customEvents', 'pageViews', 'browserTimings', 'requests',          'dependencies', 'exceptions', 'availabilityResults',          'performanceCounters', 'customMetrics'         :type event_type: str or ~azure.applicationinsights.models.EventType         :param timespan: Optional. The timespan over which to retrieve events.          This is an ISO8601 time period value.  This timespan is applied in          addition to any that are specified in the Odata expression.         :type timespan: str         :param filter: An expression used to filter the returned events         :type filter: str         :param search: A free-text search expression to match for whether a          particular event should be returned         :type search: str         :param orderby: A comma-separated list of properties with \\"asc\\"          (the default) or \\"desc\\" to control the order of returned events         :type orderby: str         :param select: Limits the properties to just those requested on each          returned event         :type select: str         :param skip: The number of items to skip over before returning events         :type skip: int         :param top: The number of events to return         :type top: int         :param format: Format for the returned events         :type format: str         :param count: Request a count of matching items included with the          returned events         :type count: bool         :param apply: An expression used for aggregation over returned events         :type apply: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: EventsResults or ClientRawResponse if raw=true         :rtype: ~azure.applicationinsights.models.EventsResults or          ~msrest.pipeline.ClientRawResponse         :raises:          :class:`ErrorResponseException<azure.applicationinsights.models.ErrorResponseException>`
Add a face to a large face list. The input face is specified as an         image with a targetFace rectangle. It returns a persistedFaceId         representing the added face, and persistedFaceId will not expire.          :param large_face_list_id: Id referencing a particular large face          list.         :type large_face_list_id: str         :param image: An image stream.         :type image: Generator         :param user_data: User-specified data about the face for any purpose.          The maximum length is 1KB.         :type user_data: str         :param target_face: A face rectangle to specify the target face to be          added to a person in the format of "targetFace=left,top,width,height".          E.g. "targetFace=10,10,100,100". If there is more than one face in the          image, targetFace is required to specify which face to add. No          targetFace means there is only one face detected in the entire image.         :type target_face: list[int]         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param callback: When specified, will be called with each chunk of          data that is streamed. The callback should take two arguments, the          bytes of the current chunk of data and the response object. If the          data is uploading, response will be None.         :type callback: Callable[Bytes, response=None]         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: PersistedFace or ClientRawResponse if raw=true         :rtype: ~azure.cognitiveservices.vision.face.models.PersistedFace or          ~msrest.pipeline.ClientRawResponse         :raises:          :class:`APIErrorException<azure.cognitiveservices.vision.face.models.APIErrorException>`
Reset auth_attempted on redirects.
Creates Migration configuration and starts migration of entities from         Standard to Premium namespace.          :param resource_group_name: Name of the Resource group within the          Azure subscription.         :type resource_group_name: str         :param namespace_name: The namespace name         :type namespace_name: str         :param target_namespace: Existing premium Namespace ARM Id name which          has no entities, will be used for migration         :type target_namespace: str         :param post_migration_name: Name to access Standard Namespace after          migration         :type post_migration_name: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns          MigrationConfigProperties or          ClientRawResponse<MigrationConfigProperties> if raw==True         :rtype:          ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.servicebus.models.MigrationConfigProperties]          or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.servicebus.models.MigrationConfigProperties]]         :raises:          :class:`ErrorResponseException<azure.mgmt.servicebus.models.ErrorResponseException>`
Publishes a batch of events to an Azure Event Grid topic.          :param topic_hostname: The host name of the topic, e.g.          topic1.westus2-1.eventgrid.azure.net         :type topic_hostname: str         :param events: An array of events to be published to Event Grid.         :type events: list[~azure.eventgrid.models.EventGridEvent]         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: None or ClientRawResponse if raw=true         :rtype: None or ~msrest.pipeline.ClientRawResponse         :raises:          :class:`HttpOperationError<msrest.exceptions.HttpOperationError>`
Moves resources from one resource group to another resource group.          The resources to move must be in the same source resource group. The         target resource group may be in a different subscription. When moving         resources, both the source group and the target group are locked for         the duration of the operation. Write and delete operations are blocked         on the groups until the move completes. .          :param source_resource_group_name: The name of the resource group          containing the resources to move.         :type source_resource_group_name: str         :param resources: The IDs of the resources.         :type resources: list[str]         :param target_resource_group: The target resource group.         :type target_resource_group: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns None or          ClientRawResponse<None> if raw==True         :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]         :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`
Define a new default profile.
Queries policy tracked resources under the management group.          :param management_group_name: Management group name.         :type management_group_name: str         :param query_options: Additional parameters for the operation         :type query_options: ~azure.mgmt.policyinsights.models.QueryOptions         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: An iterator like instance of PolicyTrackedResource         :rtype:          ~azure.mgmt.policyinsights.models.PolicyTrackedResourcePaged[~azure.mgmt.policyinsights.models.PolicyTrackedResource]         :raises:          :class:`QueryFailureException<azure.mgmt.policyinsights.models.QueryFailureException>`
Create a queue entity.          :param queue_name: The name of the new queue.         :type queue_name: str         :param lock_duration: The lock durection in seconds for each message in the queue.         :type lock_duration: int         :param max_size_in_megabytes: The max size to allow the queue to grow to.         :type max_size_in_megabytes: int         :param requires_duplicate_detection: Whether the queue will require every message with          a specified time frame to have a unique ID. Non-unique messages will be discarded.          Default value is False.         :type requires_duplicate_detection: bool         :param requires_session: Whether the queue will be sessionful, and therefore require all          message to have a Session ID and be received by a sessionful receiver.          Default value is False.         :type requires_session: bool         :param default_message_time_to_live: The length of time a message will remain in the queue          before it is either discarded or moved to the dead letter queue.         :type default_message_time_to_live: ~datetime.timedelta         :param dead_lettering_on_message_expiration: Whether to move expired messages to the          dead letter queue. Default value is False.         :type dead_lettering_on_message_expiration: bool         :param duplicate_detection_history_time_window: The period within which all incoming messages          must have a unique message ID.         :type duplicate_detection_history_time_window: ~datetime.timedelta         :param max_delivery_count: The maximum number of times a message will attempt to be delivered          before it is moved to the dead letter queue.         :type max_delivery_count: int         :param enable_batched_operations:         :type: enable_batched_operations: bool         :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.         :raises: ~azure.common.AzureConflictHttpError if a queue of the same name already exists.
Delete a queue entity.          :param queue_name: The name of the queue to delete.         :type queue_name: str         :param fail_not_exist: Whether to raise an exception if the named queue is not          found. If set to True, a ServiceBusResourceNotFound will be raised.          Default value is False.         :type fail_not_exist: bool         :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namesapce is not found.         :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the queue is not found          and `fail_not_exist` is set to True.
Create a topic entity.          :param topic_name: The name of the new topic.         :type topic_name: str         :param max_size_in_megabytes: The max size to allow the topic to grow to.         :type max_size_in_megabytes: int         :param requires_duplicate_detection: Whether the topic will require every message with          a specified time frame to have a unique ID. Non-unique messages will be discarded.          Default value is False.         :type requires_duplicate_detection: bool         :param default_message_time_to_live: The length of time a message will remain in the topic          before it is either discarded or moved to the dead letter queue.         :type default_message_time_to_live: ~datetime.timedelta         :param duplicate_detection_history_time_window: The period within which all incoming messages          must have a unique message ID.         :type duplicate_detection_history_time_window: ~datetime.timedelta         :param enable_batched_operations:         :type: enable_batched_operations: bool         :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.         :raises: ~azure.common.AzureConflictHttpError if a topic of the same name already exists.
Delete a topic entity.          :param topic_name: The name of the topic to delete.         :type topic_name: str         :param fail_not_exist: Whether to raise an exception if the named topic is not          found. If set to True, a ServiceBusResourceNotFound will be raised.          Default value is False.         :type fail_not_exist: bool         :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namesapce is not found.         :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the topic is not found          and `fail_not_exist` is set to True.
Create a subscription entity.          :param topic_name: The name of the topic under which to create the subscription.         :param subscription_name: The name of the new subscription.         :type subscription_name: str         :param lock_duration: The lock durection in seconds for each message in the subscription.         :type lock_duration: int         :param requires_session: Whether the subscription will be sessionful, and therefore require all          message to have a Session ID and be received by a sessionful receiver.          Default value is False.         :type requires_session: bool         :param default_message_time_to_live: The length of time a message will remain in the subscription          before it is either discarded or moved to the dead letter queue.         :type default_message_time_to_live: ~datetime.timedelta         :param dead_lettering_on_message_expiration: Whether to move expired messages to the          dead letter queue. Default value is False.         :type dead_lettering_on_message_expiration: bool         :param dead_lettering_on_filter_evaluation_exceptions: Whether to move messages that error on          filtering into the dead letter queue. Default is False, and the messages will be discarded.         :type dead_lettering_on_filter_evaluation_exceptions: bool         :param max_delivery_count: The maximum number of times a message will attempt to be delivered          before it is moved to the dead letter queue.         :type max_delivery_count: int         :param enable_batched_operations:         :type: enable_batched_operations: bool         :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.         :raises: ~azure.common.AzureConflictHttpError if a queue of the same name already exists.
Create a Client from a Service Bus connection string.          :param conn_str: The connection string.         :type conn_str: str         :param name: The name of the entity, if the 'EntityName' property is          not included in the connection string.
Perform an operation to update the properties of the entity.          :returns: The properties of the entity as a dictionary.         :rtype: dict[str, Any]         :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the entity does not exist.         :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the endpoint cannot be reached.         :raises: ~azure.common.AzureHTTPError if the credentials are invalid.
Whether the receivers lock on a particular session has expired.          :rtype: bool
Creates a session for a node.          :param resource_group_name: The resource group name uniquely          identifies the resource group within the user subscriptionId.         :type resource_group_name: str         :param node_name: The node name (256 characters maximum).         :type node_name: str         :param session: The sessionId from the user.         :type session: str         :param user_name: Encrypted User name to be used to connect to node.         :type user_name: str         :param password: Encrypted Password associated with user name.         :type password: str         :param retention_period: Session retention period. Possible values          include: 'Session', 'Persistent'         :type retention_period: str or          ~azure.mgmt.servermanager.models.RetentionPeriod         :param credential_data_format: Credential data format. Possible values          include: 'RsaEncrypted'         :type credential_data_format: str or          ~azure.mgmt.servermanager.models.CredentialDataFormat         :param encryption_certificate_thumbprint: Encryption certificate          thumbprint.         :type encryption_certificate_thumbprint: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns SessionResource or          ClientRawResponse<SessionResource> if raw==True         :rtype:          ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.servermanager.models.SessionResource]          or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.servermanager.models.SessionResource]]         :raises:          :class:`ErrorException<azure.mgmt.servermanager.models.ErrorException>`
Creates an Azure subscription.          :param billing_account_name: The name of the commerce root billing          account.         :type billing_account_name: str         :param invoice_section_name: The name of the invoice section.         :type invoice_section_name: str         :param body: The subscription creation parameters.         :type body:          ~azure.mgmt.subscription.models.SubscriptionCreationParameters         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns          SubscriptionCreationResult or          ClientRawResponse<SubscriptionCreationResult> if raw==True         :rtype:          ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.subscription.models.SubscriptionCreationResult]          or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.subscription.models.SubscriptionCreationResult]]         :raises:          :class:`ErrorResponseException<azure.mgmt.subscription.models.ErrorResponseException>`
Export logs that show Api requests made by this subscription in the         given time window to show throttling activities.          :param parameters: Parameters supplied to the LogAnalytics          getRequestRateByInterval Api.         :type parameters:          ~azure.mgmt.compute.v2018_04_01.models.RequestRateByIntervalInput         :param location: The location upon which virtual-machine-sizes is          queried.         :type location: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns          LogAnalyticsOperationResult or          ClientRawResponse<LogAnalyticsOperationResult> if raw==True         :rtype:          ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.compute.v2018_04_01.models.LogAnalyticsOperationResult]          or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.compute.v2018_04_01.models.LogAnalyticsOperationResult]]         :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`
Scan output for exceptions      If there is an output from an add task collection call add it to the results.      :param results_queue: Queue containing results of attempted add_collection's     :type results_queue: collections.deque     :return: list of TaskAddResults     :rtype: list[~TaskAddResult]
Adds a chunk of tasks to the job          Retry chunk if body exceeds the maximum request size and retry tasks         if failed due to server errors.          :param results_queue: Queue to place the return value of the request         :type results_queue: collections.deque         :param chunk_tasks_to_add: Chunk of at most 100 tasks with retry details         :type chunk_tasks_to_add: list[~TrackedCloudTask]
Main method for worker to run          Pops a chunk of tasks off the collection of pending tasks to be added and submits them to be added.          :param collections.deque results_queue: Queue for worker to output results to
Will build the actual config for Jinja2, based on SDK config.
Resets the user password on an environment This operation can take a         while to complete.          :param user_name: The name of the user.         :type user_name: str         :param reset_password_payload: Represents the payload for resetting          passwords.         :type reset_password_payload:          ~azure.mgmt.labservices.models.ResetPasswordPayload         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns None or          ClientRawResponse<None> if raw==True         :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]         :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`
Starts an environment by starting all resources inside the environment.         This operation can take a while to complete.          :param user_name: The name of the user.         :type user_name: str         :param environment_id: The resourceId of the environment         :type environment_id: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns None or          ClientRawResponse<None> if raw==True         :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]         :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`
Create message from response.      response:         response from Service Bus cloud server.     service_instance:         the Service Bus client.
Converts entry element to rule object.      The format of xml for rule: <entry xmlns='http://www.w3.org/2005/Atom'> <content type='application/xml'> <RuleDescription     xmlns:i="http://www.w3.org/2001/XMLSchema-instance"     xmlns="http://schemas.microsoft.com/netservices/2010/10/servicebus/connect">     <Filter i:type="SqlFilterExpression">         <SqlExpression>MyProperty='XYZ'</SqlExpression>     </Filter>     <Action i:type="SqlFilterAction">         <SqlExpression>set MyProperty2 = 'ABC'</SqlExpression>     </Action> </RuleDescription> </content> </entry>
Converts entry element to queue object.      The format of xml response for queue: <QueueDescription     xmlns=\"http://schemas.microsoft.com/netservices/2010/10/servicebus/connect\">     <MaxSizeInBytes>10000</MaxSizeInBytes>     <DefaultMessageTimeToLive>PT5M</DefaultMessageTimeToLive>     <LockDuration>PT2M</LockDuration>     <RequiresGroupedReceives>False</RequiresGroupedReceives>     <SupportsDuplicateDetection>False</SupportsDuplicateDetection>     ... </QueueDescription>
Converts entry element to topic      The xml format for topic: <entry xmlns='http://www.w3.org/2005/Atom'>     <content type='application/xml'>     <TopicDescription         xmlns:i="http://www.w3.org/2001/XMLSchema-instance"         xmlns="http://schemas.microsoft.com/netservices/2010/10/servicebus/connect">         <DefaultMessageTimeToLive>P10675199DT2H48M5.4775807S</DefaultMessageTimeToLive>         <MaxSizeInMegabytes>1024</MaxSizeInMegabytes>         <RequiresDuplicateDetection>false</RequiresDuplicateDetection>         <DuplicateDetectionHistoryTimeWindow>P7D</DuplicateDetectionHistoryTimeWindow>         <DeadLetteringOnFilterEvaluationExceptions>true</DeadLetteringOnFilterEvaluationExceptions>     </TopicDescription>     </content> </entry>
Converts entry element to subscription      The xml format for subscription: <entry xmlns='http://www.w3.org/2005/Atom'>     <content type='application/xml'>     <SubscriptionDescription         xmlns:i="http://www.w3.org/2001/XMLSchema-instance"         xmlns="http://schemas.microsoft.com/netservices/2010/10/servicebus/connect">         <LockDuration>PT5M</LockDuration>         <RequiresSession>false</RequiresSession>         <DefaultMessageTimeToLive>P10675199DT2H48M5.4775807S</DefaultMessageTimeToLive>         <DeadLetteringOnMessageExpiration>false</DeadLetteringOnMessageExpiration>         <DeadLetteringOnFilterEvaluationExceptions>true</DeadLetteringOnFilterEvaluationExceptions>     </SubscriptionDescription>     </content> </entry>
Creates a new certificate inside the specified account.          :param resource_group_name: The name of the resource group that          contains the Batch account.         :type resource_group_name: str         :param account_name: The name of the Batch account.         :type account_name: str         :param certificate_name: The identifier for the certificate. This must          be made up of algorithm and thumbprint separated by a dash, and must          match the certificate data in the request. For example SHA1-a3d1c5.         :type certificate_name: str         :param parameters: Additional parameters for certificate creation.         :type parameters:          ~azure.mgmt.batch.models.CertificateCreateOrUpdateParameters         :param if_match: The entity state (ETag) version of the certificate to          update. A value of "*" can be used to apply the operation only if the          certificate already exists. If omitted, this operation will always be          applied.         :type if_match: str         :param if_none_match: Set to '*' to allow a new certificate to be          created, but to prevent updating an existing certificate. Other values          will be ignored.         :type if_none_match: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :return: An instance of AzureOperationPoller that returns Certificate          or ClientRawResponse if raw=true         :rtype:          ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.batch.models.Certificate]          or ~msrest.pipeline.ClientRawResponse         :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`
Deletes the specified certificate.          :param resource_group_name: The name of the resource group that          contains the Batch account.         :type resource_group_name: str         :param account_name: The name of the Batch account.         :type account_name: str         :param certificate_name: The identifier for the certificate. This must          be made up of algorithm and thumbprint separated by a dash, and must          match the certificate data in the request. For example SHA1-a3d1c5.         :type certificate_name: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :return: An instance of AzureOperationPoller that returns None or          ClientRawResponse if raw=true         :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or          ~msrest.pipeline.ClientRawResponse         :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`
Return a SDK client initialized with current CLI credentials, CLI default subscription and CLI default cloud.      This method will fill automatically the following client parameters:     - credentials     - subscription_id     - base_url      Parameters provided in kwargs will override CLI parameters and be passed directly to the client.      :Example:      .. code:: python          from azure.common.client_factory import get_client_from_cli_profile         from azure.mgmt.compute import ComputeManagementClient         client = get_client_from_cli_profile(ComputeManagementClient)      .. versionadded:: 1.1.6      :param client_class: A SDK client class     :return: An instantiated client     :raises: ImportError if azure-cli-core package is not available
Return a SDK client initialized with a JSON auth dict.      The easiest way to obtain this content is to call the following CLI commands:      .. code:: bash          az ad sp create-for-rbac --sdk-auth      This method will fill automatically the following client parameters:     - credentials     - subscription_id     - base_url     - tenant_id      Parameters provided in kwargs will override parameters and be passed directly to the client.      :Example:      .. code:: python          from azure.common.client_factory import get_client_from_auth_file         from azure.mgmt.compute import ComputeManagementClient         config_dict = {             "clientId": "ad735158-65ca-11e7-ba4d-ecb1d756380e",             "clientSecret": "b70bb224-65ca-11e7-810c-ecb1d756380e",             "subscriptionId": "bfc42d3a-65ca-11e7-95cf-ecb1d756380e",             "tenantId": "c81da1d8-65ca-11e7-b1d1-ecb1d756380e",             "activeDirectoryEndpointUrl": "https://login.microsoftonline.com",             "resourceManagerEndpointUrl": "https://management.azure.com/",             "activeDirectoryGraphResourceId": "https://graph.windows.net/",             "sqlManagementEndpointUrl": "https://management.core.windows.net:8443/",             "galleryEndpointUrl": "https://gallery.azure.com/",             "managementEndpointUrl": "https://management.core.windows.net/"         }         client = get_client_from_json_dict(ComputeManagementClient, config_dict)      .. versionadded:: 1.1.7      :param client_class: A SDK client class     :param dict config_dict: A config dict.     :return: An instantiated client
Return a SDK client initialized with auth file.      The easiest way to obtain this file is to call the following CLI commands:      .. code:: bash          az ad sp create-for-rbac --sdk-auth      You can specific the file path directly, or fill the environment variable AZURE_AUTH_LOCATION.     File must be UTF-8.      This method will fill automatically the following client parameters:     - credentials     - subscription_id     - base_url      Parameters provided in kwargs will override parameters and be passed directly to the client.      :Example:      .. code:: python          from azure.common.client_factory import get_client_from_auth_file         from azure.mgmt.compute import ComputeManagementClient         client = get_client_from_auth_file(ComputeManagementClient)      Example of file:      .. code:: json          {             "clientId": "ad735158-65ca-11e7-ba4d-ecb1d756380e",             "clientSecret": "b70bb224-65ca-11e7-810c-ecb1d756380e",             "subscriptionId": "bfc42d3a-65ca-11e7-95cf-ecb1d756380e",             "tenantId": "c81da1d8-65ca-11e7-b1d1-ecb1d756380e",             "activeDirectoryEndpointUrl": "https://login.microsoftonline.com",             "resourceManagerEndpointUrl": "https://management.azure.com/",             "activeDirectoryGraphResourceId": "https://graph.windows.net/",             "sqlManagementEndpointUrl": "https://management.core.windows.net:8443/",             "galleryEndpointUrl": "https://gallery.azure.com/",             "managementEndpointUrl": "https://management.core.windows.net/"         }      .. versionadded:: 1.1.7      :param client_class: A SDK client class     :param str auth_path: Path to the file.     :return: An instantiated client     :raises: KeyError if AZURE_AUTH_LOCATION is not an environment variable and no path is provided     :raises: FileNotFoundError if provided file path does not exists     :raises: json.JSONDecodeError if provided file is not JSON valid     :raises: UnicodeDecodeError if file is not UTF8 compliant
resp_body is the XML we received         resp_type is a string, such as Containers,         return_type is the type we're constructing, such as ContainerEnumResults         item_type is the type object of the item to be created, such as Container          This function then returns a ContainerEnumResults object with the         containers member populated with the results.
get properties from element tree element
Delete the Provisioning Service Certificate.          Deletes the specified certificate assosciated with the Provisioning         Service.          :param resource_group_name: Resource group identifier.         :type resource_group_name: str         :param if_match: ETag of the certificate         :type if_match: str         :param provisioning_service_name: The name of the provisioning          service.         :type provisioning_service_name: str         :param certificate_name: This is a mandatory field, and is the logical          name of the certificate that the provisioning service will access by.         :type certificate_name: str         :param certificatename: This is optional, and it is the Common Name of          the certificate.         :type certificatename: str         :param certificateraw_bytes: Raw data within the certificate.         :type certificateraw_bytes: bytearray         :param certificateis_verified: Indicates if certificate has been          verified by owner of the private key.         :type certificateis_verified: bool         :param certificatepurpose: A description that mentions the purpose of          the certificate. Possible values include: 'clientAuthentication',          'serverAuthentication'         :type certificatepurpose: str or          ~azure.mgmt.iothubprovisioningservices.models.CertificatePurpose         :param certificatecreated: Time the certificate is created.         :type certificatecreated: datetime         :param certificatelast_updated: Time the certificate is last updated.         :type certificatelast_updated: datetime         :param certificatehas_private_key: Indicates if the certificate          contains a private key.         :type certificatehas_private_key: bool         :param certificatenonce: Random number generated to indicate Proof of          Possession.         :type certificatenonce: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: returns the direct response alongside the          deserialized response         :param operation_config: :ref:`Operation configuration          overrides<msrest:optionsforoperations>`.         :return: None or ClientRawResponse if raw=true         :rtype: None or ~msrest.pipeline.ClientRawResponse         :raises:          :class:`ErrorDetailsException<azure.mgmt.iothubprovisioningservices.models.ErrorDetailsException>`
Get a client for a queue entity.          :param queue_name: The name of the queue.         :type queue_name: str         :rtype: ~azure.servicebus.servicebus_client.QueueClient         :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.         :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the queue is not found.          Example:             .. literalinclude:: ../examples/test_examples.py                 :start-after: [START get_queue_client]                 :end-before: [END get_queue_client]                 :language: python                 :dedent: 8                 :caption: Get the specific queue client from Service Bus client
Get clients for all queue entities in the namespace.          :rtype: list[~azure.servicebus.servicebus_client.QueueClient]         :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.          Example:             .. literalinclude:: ../examples/test_examples.py                 :start-after: [START list_queues]                 :end-before: [END list_queues]                 :language: python                 :dedent: 4                 :caption: List the queues from Service Bus client
Get a client for a topic entity.          :param topic_name: The name of the topic.         :type topic_name: str         :rtype: ~azure.servicebus.servicebus_client.TopicClient         :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.         :raises: ~azure.servicebus.common.errors.ServiceBusResourceNotFound if the topic is not found.          Example:             .. literalinclude:: ../examples/test_examples.py                 :start-after: [START get_topic_client]                 :end-before: [END get_topic_client]                 :language: python                 :dedent: 8                 :caption: Get the specific topic client from Service Bus client
Get a client for all topic entities in the namespace.          :rtype: list[~azure.servicebus.servicebus_client.TopicClient]         :raises: ~azure.servicebus.common.errors.ServiceBusConnectionError if the namespace is not found.          Example:             .. literalinclude:: ../examples/test_examples.py                 :start-after: [START list_topics]                 :end-before: [END list_topics]                 :language: python                 :dedent: 4                 :caption: List the topics from Service Bus client
Receive messages by sequence number that have been previously deferred.          When receiving deferred messages from a partitioned entity, all of the supplied         sequence numbers must be messages from the same partition.          :param sequence_numbers: A list of the sequence numbers of messages that have been          deferred.         :type sequence_numbers: list[int]         :param mode: The mode with which messages will be retrieved from the entity. The two options          are PeekLock and ReceiveAndDelete. Messages received with PeekLock must be settled within a given          lock period before they will be removed from the queue. Messages received with ReceiveAndDelete          will be immediately removed from the queue, and cannot be subsequently rejected or re-received if          the client fails to process the message. The default mode is PeekLock.         :type mode: ~azure.servicebus.common.constants.ReceiveSettleMode         :rtype: list[~azure.servicebus.common.message.Message]          Example:             .. literalinclude:: ../examples/test_examples.py                 :start-after: [START receive_deferred_messages_service_bus]                 :end-before: [END receive_deferred_messages_service_bus]                 :language: python                 :dedent: 8                 :caption: Get the messages which were deferred using their sequence numbers
Settle messages that have been previously deferred.          :param settlement: How the messages are to be settled. This must be a string          of one of the following values: 'completed', 'suspended', 'abandoned'.         :type settlement: str         :param messages: A list of deferred messages to be settled.         :type messages: list[~azure.servicebus.common.message.DeferredMessage]          Example:             .. literalinclude:: ../examples/test_examples.py                 :start-after: [START settle_deferred_messages_service_bus]                 :end-before: [END settle_deferred_messages_service_bus]                 :language: python                 :dedent: 8                 :caption: Settle deferred messages.
List the web sites defined on this webspace.          webspace_name:             The name of the webspace.         website_name:             The name of the website.
Create a website.          webspace_name:             The name of the webspace.         website_name:             The name of the website.         geo_region:             The geographical region of the webspace that will be created.         host_names:             An array of fully qualified domain names for website. Only one             hostname can be specified in the azurewebsites.net domain.             The hostname should match the name of the website. Custom domains             can only be specified for Shared or Standard websites.         plan:             This value must be 'VirtualDedicatedPlan'.         compute_mode:             This value should be 'Shared' for the Free or Paid Shared             offerings, or 'Dedicated' for the Standard offering. The default             value is 'Shared'. If you set it to 'Dedicated', you must specify             a value for the server_farm parameter.         server_farm:             The name of the Server Farm associated with this website. This is             a required value for Standard mode.         site_mode:             Can be None, 'Limited' or 'Basic'. This value is 'Limited' for the             Free offering, and 'Basic' for the Paid Shared offering. Standard             mode does not use the site_mode parameter; it uses the compute_mode             parameter.
Delete a website.          webspace_name:             The name of the webspace.         website_name:             The name of the website.         delete_empty_server_farm:             If the site being deleted is the last web site in a server farm,             you can delete the server farm by setting this to True.         delete_metrics:             To also delete the metrics for the site that you are deleting, you             can set this to True.
Update a web site.          webspace_name:             The name of the webspace.         website_name:             The name of the website.         state:             The wanted state ('Running' or 'Stopped' accepted)
Restart a web site.          webspace_name:             The name of the webspace.         website_name:             The name of the website.
Get historical usage metrics.          webspace_name:             The name of the webspace.         website_name:             The name of the website.         metrics:             Optional. List of metrics name. Otherwise, all metrics returned.         start_time:             Optional. An ISO8601 date. Otherwise, current hour is used.         end_time:             Optional. An ISO8601 date. Otherwise, current time is used.         time_grain:             Optional. A rollup name, as P1D. OTherwise, default rollup for the metrics is used.         More information and metrics name at:         http://msdn.microsoft.com/en-us/library/azure/dn166964.aspx
Get metric definitions of metrics available of this web site.          webspace_name:             The name of the webspace.         website_name:             The name of the website.
Get a site's publish profile as a string          webspace_name:             The name of the webspace.         website_name:             The name of the website.
Get a site's publish profile as an object          webspace_name:             The name of the webspace.         website_name:             The name of the website.
Updates the policies for the specified container registry.          :param resource_group_name: The name of the resource group to which          the container registry belongs.         :type resource_group_name: str         :param registry_name: The name of the container registry.         :type registry_name: str         :param quarantine_policy: An object that represents quarantine policy          for a container registry.         :type quarantine_policy:          ~azure.mgmt.containerregistry.v2018_02_01_preview.models.QuarantinePolicy         :param trust_policy: An object that represents content trust policy          for a container registry.         :type trust_policy:          ~azure.mgmt.containerregistry.v2018_02_01_preview.models.TrustPolicy         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns RegistryPolicies or          ClientRawResponse<RegistryPolicies> if raw==True         :rtype:          ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.containerregistry.v2018_02_01_preview.models.RegistryPolicies]          or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.containerregistry.v2018_02_01_preview.models.RegistryPolicies]]         :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`
The Create Cloud Service request creates a new cloud service. When job         collections are created, they are hosted within a cloud service.         A cloud service groups job collections together in a given region.         Once a cloud service has been created, job collections can then be         created and contained within it.          cloud_service_id:             The cloud service id         label:             The name of the cloud service.         description:             The description of the cloud service.         geo_region:             The geographical region of the webspace that will be created.
The Check Name Availability operation checks if a new job collection with         the given name may be created, or if it is unavailable. The result of the         operation is a Boolean true or false.          cloud_service_id:             The cloud service id         job_collection_id:             The name of the job_collection_id.
The Get Job Collection operation gets the details of a job collection          cloud_service_id:             The cloud service id         job_collection_id:             Name of the hosted service.
Completes the restore operation on a managed database.          :param location_name: The name of the region where the resource is          located.         :type location_name: str         :param operation_id: Management operation id that this request tries          to complete.         :type operation_id: str         :param last_backup_name: The last backup name to apply         :type last_backup_name: str         :param dict custom_headers: headers that will be added to the request         :param bool raw: The poller return type is ClientRawResponse, the          direct response alongside the deserialized response         :param polling: True for ARMPolling, False for no polling, or a          polling object for personal polling strategy         :return: An instance of LROPoller that returns None or          ClientRawResponse<None> if raw==True         :rtype: ~msrestazure.azure_operation.AzureOperationPoller[None] or          ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[None]]         :raises: :class:`CloudError<msrestazure.azure_exceptions.CloudError>`
Cancel one or more messages that have previsouly been scheduled and are still pending.          :param sequence_numbers: The seqeuence numbers of the scheduled messages.         :type sequence_numbers: int          Example:             .. literalinclude:: ../examples/async_examples/test_examples_async.py                 :start-after: [START cancel_schedule_messages]                 :end-before: [END cancel_schedule_messages]                 :language: python                 :dedent: 4                 :caption: Schedule messages.
Wait until all pending messages have been sent.          :returns: A list of the send results of all the pending messages. Each          send result is a tuple with two values. The first is a boolean, indicating `True`          if the message sent, or `False` if it failed. The second is an error if the message          failed, otherwise it will be `None`.         :rtype: list[tuple[bool, ~azure.servicebus.common.errors.MessageSendFailed]]          Example:             .. literalinclude:: ../examples/async_examples/test_examples_async.py                 :start-after: [START queue_sender_messages]                 :end-before: [END queue_sender_messages]                 :language: python                 :dedent: 4                 :caption: Schedule messages.
Reconnect the handler.          If the handler was disconnected from the service with         a retryable error - attempt to reconnect.         This method will be called automatically for most retryable errors.         Also attempts to re-queue any messages that were pending before the reconnect.
Writes a certificate file to the specified location.  This can then be used      to instantiate ServiceManagementService.  Returns the subscription ID.      publish_settings_path:          Path to subscription file downloaded from          http://go.microsoft.com/fwlink/?LinkID=301775     path_to_write_certificate:         Path to write the certificate file.     subscription_id:         (optional)  Provide a subscription id here if you wish to use a          specific subscription under the publish settings file.
Load any stored cookies for the plugin that have not expired.          :return: list of the restored cookie names
Returns the width of the string it would be when displayed.
Drops Characters by unicode not by bytes.
Clears out the previous line and prints a new one.
Formats the file size into a human readable format.
Formats elapsed seconds into a human readable format.
Creates a status line with appropriate size.
Progress an iterator and updates a pretty status line to the terminal.      The status line contains:      - Amount of data read from the iterator      - Time elapsed      - Average speed, based on the last few seconds.
yield the segment number and when it will be available         There are two cases for segment number generation, static and dynamic.          In the case of static stream, the segment number starts at the startNumber and counts         up to the number of segments that are represented by the periods duration.          In the case of dynamic streams, the segments should appear at the specified time         in the simplest case the segment number is based on the time since the availabilityStartTime         :return:
Segments are yielded when they are available          Segments appear on a time line, for dynamic content they are only available at a certain time         and sometimes for a limited time. For static content they are all available at the same time.          :param kwargs: extra args to pass to the segment template         :return: yields Segments
Pauses the thread for a specified time.          Returns False if interrupted by another thread and True if the         time runs out normally.
Adds a segment to the download pool and write queue.
Puts a value into a queue but aborts if this thread is closed.
Returns any parameters needed for Akamai HD player verification.          Algorithm originally documented by KSV, source:         http://stream-recorder.com/forum/showpost.php?p=43761&postcount=13
Given an HTTP response from the sessino endpoint, extract the nonce, so we can "sign" requests with it.         We don't really sign the requests in the traditional sense of a nonce, we just incude them in the auth requests.          :param http_result: HTTP response from the bbc session endpoint.         :type http_result: requests.Response         :return: nonce to "sign" url requests with         :rtype: string
Find the Video Packet ID in the HTML for the provided URL          :param url: URL to download, if res is not provided.         :param res: Provide a cached version of the HTTP response to search         :type url: string         :type res: requests.Response         :return: Video Packet ID for a Programme in iPlayer         :rtype: string
Wrapper around json.loads.      Wraps errors in custom exception with a snippet of the data in the message.
Wrapper around ElementTree.fromstring with some extras.      Provides these extra features:      - Handles incorrectly encoded XML      - Allows stripping namespace information      - Wraps errors in custom exception with a snippet of the data in the message
Parses a query string into a dict.      Unlike parse_qs and parse_qsl, duplicate keys are not preserved in     favor of a simpler return value.
Search for a key in a nested dict, or list of nested dicts, and return the values.      :param data: dict/list to search     :param key: key to find     :return: matches for key
Spawn the process defined in `cmd`          parameters is converted to options the short and long option prefixes         if a list is given as the value, the parameter is repeated with each         value          If timeout is set the spawn will block until the process returns or         the timeout expires.          :param parameters: optional parameters         :param arguments: positional arguments         :param stderr: where to redirect stderr to         :param timeout: timeout for short lived process         :param long_option_prefix: option prefix, default -         :param short_option_prefix: long option prefix, default --         :return: spawned process
Brute force regex based HTML tag parser. This is a rough-and-ready searcher to find HTML tags when     standards compliance is not required. Will find tags that are commented out, or inside script tag etc.      :param html: HTML page     :param tag: tag name to find     :return: generator with Tags
Attempt to parse a DASH manifest file and return its streams          :param session: Streamlink session instance         :param url_or_manifest: URL of the manifest file or an XML manifest string         :return: a dict of name -> DASHStream instances
Determine which Unicode encoding the JSON text sample is encoded with          RFC4627 (http://www.ietf.org/rfc/rfc4627.txt) suggests that the encoding of JSON text can be determined         by checking the pattern of NULL bytes in first 4 octets of the text.         :param sample: a sample of at least 4 bytes of the JSON text         :return: the most likely encoding of the JSON text
Parses JSON from a response.
Parses XML from a response.
Parses a semi-colon delimited list of cookies.          Example: foo=bar;baz=qux
Parses a semi-colon delimited list of headers.          Example: foo=bar;baz=qux
Parses a semi-colon delimited list of query parameters.          Example: foo=bar;baz=qux
Return the message for this LogRecord.          Return the message for this LogRecord after merging any user-supplied         arguments with the message.
A factory method which can be overridden in subclasses to create         specialized LogRecords.
Attempt a login to LiveEdu.tv
Loads a plugin from the same directory as the calling plugin.      The path used is extracted from the last call in module scope,     therefore this must be called only from module level in the     originating plugin or the correct plugin path will not be found.
Update or remove keys from a query string in a URL      :param url: URL to update     :param qsd: dict of keys to update, a None value leaves it unchanged     :param remove: list of keys to remove, or "*" to remove all                    note: updated keys are never removed, even if unchanged     :return: updated URL
Reads FLV tags from fd or buf and returns them with adjusted            timestamps.
Find all the arguments required by name          :param name: name of the argument the find the dependencies          :return: list of dependant arguments
Checks if file already exists and ask the user if it should     be overwritten if it does.
Decides where to write the stream.      Depending on arguments it can be one of these:      - The stdout pipe      - A subprocess' stdin pipe      - A named pipe that the subprocess reads from      - A regular file
Creates a HTTP server listening on a given host and port.      If host is empty, listen on all available interfaces, and if port is 0,     listen on a random high port.
Repeatedly accept HTTP connections on a server.      Forever if the serving externally, or while a player is running if it is not     empty.
Continuously output the stream over HTTP.
Prepares a filename to be passed to the player.
Opens a stream and reads 8192 bytes from it.      This is useful to check if a stream actually has data     before opening the output.
Open stream, create output and finally write the stream to output.
Reads data from stream and then writes it to the output.
Decides what to do with the selected stream.      Depending on arguments it can be one of these:      - Output internal command-line      - Output JSON represenation      - Continuously output the stream over HTTP      - Output stream data to selected output
Fetches streams using correct parameters.
Attempts to fetch streams repeatedly        until some are returned or limit hit.
Returns the real stream name of a synonym.
Formats a dict of streams.      Filters out synonyms and displays them next to     the stream they point to.      Streams are sorted according to their quality     (based on plugin.stream_weight).
The URL handler.      Attempts to resolve the URL to a plugin and then attempts     to fetch a list of available streams.      Proceeds to handle stream if user specified a valid one,     otherwise output list of valid streams.
Outputs a list of all plugins Streamlink has loaded.
Opens a web browser to allow the user to grant Streamlink        access to their Twitch account.
Attempts to load plugins from a list of directories.
Parses arguments.
Console setup.
Sets the global HTTP settings, such as proxy and headers.
Loads any additional plugins.
Sets Streamlink options.
Show current installed versions
Try to find a stream_id
Fallback if no stream_id was found before
Sets general options used by plugins and streams originating         from this session object.          :param key: key of the option         :param value: value to set the option to           **Available options**:          ======================== =========================================         hds-live-edge            ( float) Specify the time live HDS                                  streams will start from the edge of                                  stream, default: ``10.0``          hds-segment-attempts     (int) How many attempts should be done                                  to download each HDS segment, default: ``3``          hds-segment-threads      (int) The size of the thread pool used                                  to download segments, default: ``1``          hds-segment-timeout      (float) HDS segment connect and read                                  timeout, default: ``10.0``          hds-timeout              (float) Timeout for reading data from                                  HDS streams, default: ``60.0``          hls-live-edge            (int) How many segments from the end                                  to start live streams on, default: ``3``          hls-segment-attempts     (int) How many attempts should be done                                  to download each HLS segment, default: ``3``          hls-segment-threads      (int) The size of the thread pool used                                  to download segments, default: ``1``          hls-segment-timeout      (float) HLS segment connect and read                                  timeout, default: ``10.0``          hls-timeout              (float) Timeout for reading data from                                  HLS streams, default: ``60.0``          http-proxy               (str) Specify a HTTP proxy to use for                                  all HTTP requests          https-proxy              (str) Specify a HTTPS proxy to use for                                  all HTTPS requests          http-cookies             (dict or str) A dict or a semi-colon (;)                                  delimited str of cookies to add to each                                  HTTP request, e.g. ``foo=bar;baz=qux``          http-headers             (dict or str) A dict or semi-colon (;)                                  delimited str of headers to add to each                                  HTTP request, e.g. ``foo=bar;baz=qux``          http-query-params        (dict or str) A dict or a ampersand (&)                                  delimited string of query parameters to                                  add to each HTTP request,                                  e.g. ``foo=bar&baz=qux``          http-trust-env           (bool) Trust HTTP settings set in the                                  environment, such as environment                                  variables (HTTP_PROXY, etc) and                                  ~/.netrc authentication          http-ssl-verify          (bool) Verify SSL certificates,                                  default: ``True``          http-ssl-cert            (str or tuple) SSL certificate to use,                                  can be either a .pem file (str) or a                                  .crt/.key pair (tuple)          http-timeout             (float) General timeout used by all HTTP                                  requests except the ones covered by                                  other options, default: ``20.0``          http-stream-timeout      (float) Timeout for reading data from                                  HTTP streams, default: ``60.0``          subprocess-errorlog      (bool) Log errors from subprocesses to                                  a file located in the temp directory          subprocess-errorlog-path (str) Log errors from subprocesses to                                  a specific file          ringbuffer-size          (int) The size of the internal ring                                  buffer used by most stream types,                                  default: ``16777216`` (16MB)          rtmp-proxy               (str) Specify a proxy (SOCKS) that RTMP                                  streams will use          rtmp-rtmpdump            (str) Specify the location of the                                  rtmpdump executable used by RTMP streams,                                  e.g. ``/usr/local/bin/rtmpdump``          rtmp-timeout             (float) Timeout for reading data from                                  RTMP streams, default: ``60.0``          ffmpeg-ffmpeg            (str) Specify the location of the                                  ffmpeg executable use by Muxing streams                                  e.g. ``/usr/local/bin/ffmpeg``          ffmpeg-verbose           (bool) Log stderr from ffmpeg to the                                  console          ffmpeg-verbose-path      (str) Specify the location of the                                  ffmpeg stderr log file          ffmpeg-video-transcode   (str) The codec to use if transcoding                                  video when muxing with ffmpeg                                  e.g. ``h264``          ffmpeg-audio-transcode   (str) The codec to use if transcoding                                  audio when muxing with ffmpeg                                  e.g. ``aac``          stream-segment-attempts  (int) How many attempts should be done                                  to download each segment, default: ``3``.                                  General option used by streams not                                  covered by other options.          stream-segment-threads   (int) The size of the thread pool used                                  to download segments, default: ``1``.                                  General option used by streams not                                  covered by other options.          stream-segment-timeout   (float) Segment connect and read                                  timeout, default: ``10.0``.                                  General option used by streams not                                  covered by other options.          stream-timeout           (float) Timeout for reading data from                                  stream, default: ``60.0``.                                  General option used by streams not                                  covered by other options.          locale                   (str) Locale setting, in the RFC 1766 format                                  eg. en_US or es_ES                                  default: ``system locale``.          user-input-requester     (UserInputRequester) instance of UserInputRequester                                  to collect input from the user at runtime. Must be                                  set before the plugins are loaded.                                  default: ``UserInputRequester``.         ======================== =========================================
Returns current value of specified option.          :param key: key of the option
Sets plugin specific options used by plugins originating         from this session object.          :param plugin: name of the plugin         :param key: key of the option         :param value: value to set the option to
Returns current value of plugin specific option.          :param plugin: name of the plugin         :param key: key of the option
Attempts to find a plugin that can use this URL.          The default protocol (http) will be prefixed to the URL if         not specified.          Raises :exc:`NoPluginError` on failure.          :param url: a URL to match against loaded plugins         :param follow_redirect: follow redirects
Attempt to load plugins from the path specified.          :param path: full path to a directory where to look for plugins
converts a timestamp to seconds        - hours:minutes:seconds to seconds       - minutes:seconds to seconds       - 11h22m33s to seconds       - 11h to seconds       - 20h15m to seconds       - seconds to seconds      :param value: hh:mm:ss ; 00h00m00s ; seconds     :return: seconds
Checks if the string value starts with another string.
Checks if the string value ends with another string.
Checks if the string value contains another string.
Get a named attribute from an object.      When a default argument is given, it is returned when the attribute     doesn't exist.
Filters out unwanted items using the specified function.      Supports both dicts and sequences, key/value pairs are     expanded when applied to a dict.
Apply function to each value inside the sequence or dict.      Supports both dicts and sequences, key/value pairs are     expanded when applied to a dict.
Parses an URL and validates its attributes.
Find a XML element via xpath.
Find a list of XML elements via xpath.
Finds embedded player url in HTTP response.      :param response: Response object.     :returns: Player url (str).
Attempts to parse a M3U8 playlist from a string of data.      If specified, *base_uri* is the base URI that relative URIs will     be joined together with, otherwise relative URIs will be as is.      If specified, *parser* can be a M3U8Parser subclass to be used     to parse the data.
Check if the current player supports adding a title          :param cmd: command to test         :return: name of the player|None
Logs in to Steam
Returns the stream_id contained in the HTML.
Returns a nested list of different stream options.          Each entry in the list will contain a stream_url and stream_quality_name         for each stream occurrence that was found in the JS.
login and update cached cookies
Creates a key-function mapping.          The return value from the function should be either           - A tuple containing a name and stream           - A iterator of tuples containing a name and stream          Any extra arguments will be passed to the function.
Makes a call against the api.          :param entrypoint: API method to call.         :param params: parameters to include in the request data.         :param schema: schema to use to validate the data
Starts a session against Crunchyroll's server.             Is recommended that you call this method before making any other calls             to make sure you have a valid session against the server.
Returns the data for a certain media item.              :param media_id: id that identifies the media item to be accessed.             :param fields: list of the media"s field to be returned. By default the             API returns some fields, but others are not returned unless they are             explicity asked for. I have no real documentation on the fields, but             they all seem to start with the "media." prefix (e.g. media.name,             media.stream_data).             :param schema: validation schema to use
Creates a new CrunchyrollAPI object, initiates it's session and         tries to authenticate it either by using saved credentials or the         user's username and password.
Compress a byte string.      Args:       string (bytes): The input data.       mode (int, optional): The compression mode can be MODE_GENERIC (default),         MODE_TEXT (for UTF-8 format text input) or MODE_FONT (for WOFF 2.0).       quality (int, optional): Controls the compression-speed vs compression-         density tradeoff. The higher the quality, the slower the compression.         Range is 0 to 11. Defaults to 11.       lgwin (int, optional): Base 2 logarithm of the sliding window size. Range         is 10 to 24. Defaults to 22.       lgblock (int, optional): Base 2 logarithm of the maximum input block size.         Range is 16 to 24. If set to 0, the value will be set based on the         quality. Defaults to 0.      Returns:       The compressed byte string.      Raises:       brotli.error: If arguments are invalid, or compressor fails.
Show character in readable format
Show string or char.
Read n bytes from the stream on a byte boundary.
The value used for processing. Can be a tuple.         with optional extra bits
Long explanation of the value from the numeric value         with optional extra bits         Used by Layout.verboseRead when printing the value
Store decodeTable,         and compute lengthTable, minLength, maxLength from encodings.
Given the bit pattern lengths for symbols given in lengthTable,         set decodeTable, minLength, maxLength
Show all words of the code in a nice format.
Read symbol from stream. Returns symbol, length.
Expanded version of Code.explanation supporting extra bits.         If you don't supply extra, it is not mentioned.
Override if you don't define value0 and extraTable
Give the range of possible values in a tuple         Useful for mnemonic and explanation
Give count and value.
Make a nice mnemonic
Give mnemonic representation of meaning.         verbose compresses strings of x's
Build the action table from the text above
Perform the proper action
Produce hex dump of all data containing the bits         from pos to stream.pos
Process a brotli stream.
Read MNIBBLES and meta block length;         if empty block, skip block and return true.
If true, handle uncompressed data
Read block type switch descriptor for given kind of blockType.
In place inverse move to front transform.
Read prefix code array
Turns a intensity array to a monochrome 'image' by replacing each intensity by a scaled 'color'      Values in I between vmin  and vmax get scaled between 0 and 1, and values outside this range are clipped to this.      Example     >>> I = np.arange(16.).reshape(4,4)     >>> color = (0, 0, 1) # red     >>> rgb = vx.image.monochrome(I, color) # shape is (4,4,3)      :param I: ndarray of any shape (2d for image)     :param color: sequence of a (r, g and b) value     :param vmin: normalization minimum for I, or np.nanmin(I) when None     :param vmax: normalization maximum for I, or np.nanmax(I) when None     :return:
Similar to monochrome, but now do it for multiple colors      Example     >>> I = np.arange(32.).reshape(4,4,2)     >>> colors = [(0, 0, 1), (0, 1, 0)] # red and green     >>> rgb = vx.image.polychrome(I, colors) # shape is (4,4,3)      :param I: ndarray of any shape (3d will result in a 2d image)     :param colors: sequence of [(r,g,b), ...] values     :param vmin: normalization minimum for I, or np.nanmin(I) when None     :param vmax: normalization maximum for I, or np.nanmax(I) when None     :param axis: axis which to sum over, by default the last     :return:
Implementation of Dataset.to_arrow_table
Adds method f to the Dataset class
Concert velocities from a cartesian system to proper motions and radial velocities      TODO: errors      :param x: name of x column (input)     :param y:         y     :param z:         z     :param vx:       vx     :param vy:       vy     :param vz:       vz     :param vr: name of the column for the radial velocity in the r direction (output)     :param pm_long: name of the column for the proper motion component in the longitude direction  (output)     :param pm_lat: name of the column for the proper motion component in the latitude direction, positive points to the north pole (output)     :param distance: Expression for distance, if not given defaults to sqrt(x**2+y**2+z**2), but if this column already exists, passing this expression may lead to a better performance     :return:
Convert proper motion to perpendicular velocities.      :param distance:     :param pm_long:     :param pm_lat:     :param vl:     :param vb:     :param cov_matrix_distance_pm_long_pm_lat:     :param uncertainty_postfix:     :param covariance_postfix:     :param radians:     :return:
Return a graphviz.Digraph object with a graph of the expression
Computes counts of unique values.           WARNING:           * If the expression/column is not categorical, it will be converted on the fly           * dropna is False by default, it is True by default in pandas          :param dropna: when True, it will not report the missing values         :param ascending: when False (default) it will report the most frequent occuring item first         :returns: Pandas series containing the counts
Map values of an expression or in memory column accoring to an input         dictionary or a custom callable function.          Example:          >>> import vaex         >>> df = vaex.from_arrays(color=['red', 'red', 'blue', 'red', 'green'])         >>> mapper = {'red': 1, 'blue': 2, 'green': 3}         >>> df['color_mapped'] = df.color.map(mapper)         >>> df         #  color      color_mapped         0  red                   1         1  red                   1         2  blue                  2         3  red                   1         4  green                 3         >>> import numpy as np         >>> df = vaex.from_arrays(type=[0, 1, 2, 2, 2, np.nan])         >>> df['role'] = df['type'].map({0: 'admin', 1: 'maintainer', 2: 'user', np.nan: 'unknown'})         >>> df         #    type  role         0       0  admin         1       1  maintainer         2       2  user         3       2  user         4       2  user         5     nan  unknown                  :param mapper: dict like object used to map the values from keys to values         :param nan_mapping: value to be used when a nan is present (and not in the mapper)         :param null_mapping: value to use used when there is a missing value         :return: A vaex expression         :rtype: vaex.expression.Expression
Create a vaex app, the QApplication mainloop must be started.      In ipython notebook/jupyter do the following:      >>> import vaex.ui.main # this causes the qt api level to be set properly     >>> import vaex      Next cell:      >>> %gui qt      Next cell:      >>> app = vaex.app()      From now on, you can run the app along with jupyter
Open a list of filenames, and return a DataFrame with all DataFrames cocatenated.      :param list[str] filenames: list of filenames/paths     :rtype: DataFrame
Connect to a SAMP Hub and wait for a single table load event, disconnect, download the table and return the DataFrame.      Useful if you want to send a single table from say TOPCAT to vaex in a python console or notebook.
Create a vaex DataFrame from an Astropy Table.
Create an in memory DataFrame from numpy arrays.      Example      >>> import vaex, numpy as np     >>> x = np.arange(5)     >>> y = x ** 2     >>> vaex.from_arrays(x=x, y=y)       #    x    y       0    0    0       1    1    1       2    2    4       3    3    9       4    4   16     >>> some_dict = {'x': x, 'y': y}     >>> vaex.from_arrays(**some_dict)  # in case you have your columns in a dict       #    x    y       0    0    0       1    1    1       2    2    4       3    3    9       4    4   16      :param arrays: keyword arguments with arrays     :rtype: DataFrame
Similar to from_arrays, but convenient for a DataFrame of length 1.      Example:      >>> import vaex     >>> df = vaex.from_scalars(x=1, y=2)      :rtype: DataFrame
Create an in memory DataFrame from a pandas DataFrame.      :param: pandas.DataFrame df: Pandas DataFrame     :param: name: unique for the DataFrame      >>> import vaex, pandas as pd     >>> df_pandas = pd.from_csv('test.csv')     >>> df = vaex.from_pandas(df_pandas)      :rtype: DataFrame
Shortcut to read a csv file using pandas and convert to a DataFrame directly.      :rtype: DataFrame
Connect to hostname supporting the vaex web api.      :param str hostname: hostname or ip address of server     :return vaex.dataframe.ServerRest: returns a server object, note that it does not connect to the server yet, so this will always succeed     :rtype: ServerRest
Creates a zeldovich DataFrame.
Concatenate a list of DataFrames.      :rtype: DataFrame
Creates a virtual column which is the equivalent of numpy.arange, but uses 0 memory
Add a dataset and add it to the UI
basic support for evaluate at server, at least to run some unittest, do not expect this to work from strings
Decorator to transparantly accept delayed computation.      Example:      >>> delayed_sum = ds.sum(ds.E, binby=ds.x, limits=limits,     >>>                   shape=4, delay=True)     >>> @vaex.delayed     >>> def total_sum(sums):     >>>     return sums.sum()     >>> sum_of_sums = total_sum(delayed_sum)     >>> ds.execute()     >>> sum_of_sums.get()     See the tutorial for a more complete example https://docs.vaex.io/en/latest/tutorial.html#Parallel-computations
Find all columns that this selection depends on for df ds
Helper function for returning tasks results, result when immediate is True, otherwise the task itself, which is a promise
Sort table by given column number.
Read header data from Gadget data file 'filename' with Gadget file  type 'gtype'. Returns offsets of positions and velocities.
clear the cursor
Used for unittesting to make sure the plots are all done
Open document by the default handler of the OS, could be a url opened by a browser, a text file by an editor etc
Flexible writing, where f can be a filename or f object, if filename, closed after writing
Combines all masks from a list of arrays, and logically ors them into a single mask
Evaluates expression, and drop the result, usefull for benchmarking, since vaex is usually lazy
Return the first element of a binned `expression`, where the values each bin are sorted by `order_expression`.          Example:          >>> import vaex         >>> df = vaex.example()         >>> df.first(df.x, df.y, shape=8)         >>> df.first(df.x, df.y, shape=8, binby=[df.y])         >>> df.first(df.x, df.y, shape=8, binby=[df.y])         array([-4.81883764, 11.65378   ,  9.70084476, -7.3025589 ,  4.84954977,                 8.47446537, -5.73602629, 10.18783   ])          :param expression: The value to be placed in the bin.         :param order_expression: Order the values in the bins by this expression.         :param binby: {binby}         :param limits: {limits}         :param shape: {shape}         :param selection: {selection}         :param delay: {delay}         :param progress: {progress}         :param edges: {edges}         :return: Ndarray containing the first elements.         :rtype: numpy.array
Calculate the mean for expression, possibly on a grid defined by binby.          Example:          >>> df.mean("x")         -0.067131491264005971         >>> df.mean("(x**2+y**2)**0.5", binby="E", shape=4)         array([  2.43483742,   4.41840721,   8.26742458,  15.53846476])          :param expression: {expression}         :param binby: {binby}         :param limits: {limits}         :param shape: {shape}         :param selection: {selection}         :param delay: {delay}         :param progress: {progress}         :return: {return_stat_scalar}
Calculate the sum for the given expression, possible on a grid defined by binby          Example:          >>> df.sum("L")         304054882.49378014         >>> df.sum("L", binby="E", shape=4)         array([  8.83517994e+06,   5.92217598e+07,   9.55218726e+07,                          1.40008776e+08])          :param expression: {expression}         :param binby: {binby}         :param limits: {limits}         :param shape: {shape}         :param selection: {selection}         :param delay: {delay}         :param progress: {progress}         :return: {return_stat_scalar}
Calculate the standard deviation for the given expression, possible on a grid defined by binby           >>> df.std("vz")         110.31773397535071         >>> df.std("vz", binby=["(x**2+y**2)**0.5"], shape=4)         array([ 123.57954851,   85.35190177,   61.14345748,   38.0740619 ])          :param expression: {expression}         :param binby: {binby}         :param limits: {limits}         :param shape: {shape}         :param selection: {selection}         :param delay: {delay}         :param progress: {progress}         :return: {return_stat_scalar}
Calculate the covariance matrix for x and y or more expressions, possibly on a grid defined by binby.          Either x and y are expressions, e.g:          >>> df.cov("x", "y")          Or only the x argument is given with a list of expressions, e,g.:          >>> df.cov(["x, "y, "z"])          Example:          >>> df.cov("x", "y")         array([[ 53.54521742,  -3.8123135 ],         [ -3.8123135 ,  60.62257881]])         >>> df.cov(["x", "y", "z"])         array([[ 53.54521742,  -3.8123135 ,  -0.98260511],         [ -3.8123135 ,  60.62257881,   1.21381057],         [ -0.98260511,   1.21381057,  25.55517638]])          >>> df.cov("x", "y", binby="E", shape=2)         array([[[  9.74852878e+00,  -3.02004780e-02],         [ -3.02004780e-02,   9.99288215e+00]],         [[  8.43996546e+01,  -6.51984181e+00],         [ -6.51984181e+00,   9.68938284e+01]]])           :param x: {expression}         :param y: {expression_single}         :param binby: {binby}         :param limits: {limits}         :param shape: {shape}         :param selection: {selection}         :param delay: {delay}         :return: {return_stat_scalar}, the last dimensions are of shape (2,2)
Calculate the minimum and maximum for expressions, possibly on a grid defined by binby.           Example:          >>> df.minmax("x")         array([-128.293991,  271.365997])         >>> df.minmax(["x", "y"])         array([[-128.293991 ,  271.365997 ],                    [ -71.5523682,  146.465836 ]])         >>> df.minmax("x", binby="x", shape=5, limits=[-10, 10])         array([[-9.99919128, -6.00010443],                    [-5.99972439, -2.00002384],                    [-1.99991322,  1.99998057],                    [ 2.0000093 ,  5.99983597],                    [ 6.0004878 ,  9.99984646]])          :param expression: {expression}         :param binby: {binby}         :param limits: {limits}         :param shape: {shape}         :param selection: {selection}         :param delay: {delay}         :param progress: {progress}         :return: {return_stat_scalar}, the last dimension is of shape (2)
Calculate the minimum for given expressions, possibly on a grid defined by binby.           Example:          >>> df.min("x")         array(-128.293991)         >>> df.min(["x", "y"])         array([-128.293991 ,  -71.5523682])         >>> df.min("x", binby="x", shape=5, limits=[-10, 10])         array([-9.99919128, -5.99972439, -1.99991322,  2.0000093 ,  6.0004878 ])          :param expression: {expression}         :param binby: {binby}         :param limits: {limits}         :param shape: {shape}         :param selection: {selection}         :param delay: {delay}         :param progress: {progress}         :return: {return_stat_scalar}, the last dimension is of shape (2)
Calculate the median , possibly on a grid defined by binby.          NOTE: this value is approximated by calculating the cumulative distribution on a grid defined by         percentile_shape and percentile_limits           :param expression: {expression}         :param binby: {binby}         :param limits: {limits}         :param shape: {shape}         :param percentile_limits: {percentile_limits}         :param percentile_shape: {percentile_shape}         :param selection: {selection}         :param delay: {delay}         :return: {return_stat_scalar}
Viz 1d, 2d or 3d in a Jupyter notebook          .. note::             This API is not fully settled and may change in the future          Example:          >>> df.plot_widget(df.x, df.y, backend='bqplot')         >>> df.plot_widget(df.pickup_longitude, df.pickup_latitude, backend='ipyleaflet')          :param backend: Widget backend to use: 'bqplot', 'ipyleaflet', 'ipyvolume', 'matplotlib'
Count non missing value for expression on an array which represents healpix data.          :param expression: Expression or column for which to count non-missing values, or None or '*' for counting the rows         :param healpix_expression: {healpix_max_level}         :param healpix_max_level: {healpix_max_level}         :param healpix_level: {healpix_level}         :param binby: {binby}, these dimension follow the first healpix dimension.         :param limits: {limits}         :param shape: {shape}         :param selection: {selection}         :param delay: {delay}         :param progress: {progress}         :return:
Viz data in 2d using a healpix column.          :param healpix_expression: {healpix_max_level}         :param healpix_max_level: {healpix_max_level}         :param healpix_level: {healpix_level}         :param what: {what}         :param selection: {selection}         :param grid: {grid}         :param healpix_input: Specificy if the healpix index is in "equatorial", "galactic" or "ecliptic".         :param healpix_output: Plot in "equatorial", "galactic" or "ecliptic".         :param f: function to apply to the data         :param colormap: matplotlib colormap         :param grid_limits: Optional sequence [minvalue, maxvalue] that determine the min and max value that map to the colormap (values below and above these are clipped to the the min/max). (default is [min(f(grid)), max(f(grid)))         :param image_size: size for the image that healpy uses for rendering         :param nest: If the healpix data is in nested (True) or ring (False)         :param figsize: If given, modify the matplotlib figure size. Example (14,9)         :param interactive: (Experimental, uses healpy.mollzoom is True)         :param title: Title of figure         :param smooth: apply gaussian smoothing, in degrees         :param show: Call matplotlib's show (True) or not (False, defaut)         :param rotation: Rotatate the plot, in format (lon, lat, psi) such that (lon, lat) is the center, and rotate on the screen by angle psi. All angles are degrees.         :return:
Use at own risk, requires ipyvolume
Return the numpy dtype for the given expression, if not a column, the first row will be evaluated to get the dtype.
Each DataFrame has a directory where files are stored for metadata etc.          Example          >>> import vaex         >>> ds = vaex.example()         >>> vaex.get_private_dir()         '/Users/users/breddels/.vaex/dfs/_Users_users_breddels_vaex-testing_data_helmi-dezeeuw-2000-10p.hdf5'          :param bool create: is True, it will create the directory if it does not exist
Return the internal state of the DataFrame in a dictionary          Example:          >>> import vaex         >>> df = vaex.from_scalars(x=1, y=2)         >>> df['r'] = (df.x**2 + df.y**2)**0.5         >>> df.state_get()         {'active_range': [0, 1],         'column_names': ['x', 'y', 'r'],         'description': None,         'descriptions': {},         'functions': {},         'renamed_columns': [],         'selections': {'__filter__': None},         'ucds': {},         'units': {},         'variables': {},         'virtual_columns': {'r': '(((x ** 2) + (y ** 2)) ** 0.5)'}}
Sets the internal state of the df          Example:          >>> import vaex         >>> df = vaex.from_scalars(x=1, y=2)         >>> df           #    x    y        r           0    1    2  2.23607         >>> df['r'] = (df.x**2 + df.y**2)**0.5         >>> state = df.state_get()         >>> state         {'active_range': [0, 1],         'column_names': ['x', 'y', 'r'],         'description': None,         'descriptions': {},         'functions': {},         'renamed_columns': [],         'selections': {'__filter__': None},         'ucds': {},         'units': {},         'variables': {},         'virtual_columns': {'r': '(((x ** 2) + (y ** 2)) ** 0.5)'}}         >>> df2 = vaex.from_scalars(x=3, y=4)         >>> df2.state_set(state)  # now the virtual functions are 'copied'         >>> df2           #    x    y    r           0    3    4    5          :param state: dict as returned by :meth:`DataFrame.state_get`.         :param bool use_active_range: Whether to use the active range or not.
Removes the file with the virtual column etc, it does not change the current virtual columns etc.
Writes virtual columns, variables and their ucd,description and units.          The default implementation is to write this to a file called virtual_meta.yaml in the directory defined by         :func:`DataFrame.get_private_dir`. Other implementation may store this in the DataFrame file itself.          This method is called after virtual columns or variables are added. Upon opening a file, :func:`DataFrame.update_virtual_meta`         is called, so that the information is not lost between sessions.          Note: opening a DataFrame twice may result in corruption of this file.
Writes all meta data, ucd,description and units          The default implementation is to write this to a file called meta.yaml in the directory defined by         :func:`DataFrame.get_private_dir`. Other implementation may store this in the DataFrame file itself.         (For instance the vaex hdf5 implementation does this)          This method is called after virtual columns or variables are added. Upon opening a file, :func:`DataFrame.update_meta`         is called, so that the information is not lost between sessions.          Note: opening a DataFrame twice may result in corruption of this file.
Generate a Subspaces object, based on a custom list of expressions or all possible combinations based on         dimension          :param expressions_list: list of list of expressions, where the inner list defines the subspace         :param dimensions: if given, generates a subspace with all possible combinations for that dimension         :param exclude: list of
Set the variable to an expression or value defined by expression_or_value.          Example          >>> df.set_variable("a", 2.)         >>> df.set_variable("b", "a**2")         >>> df.get_variable("b")         'a**2'         >>> df.evaluate_variable("b")         4.0          :param name: Name of the variable         :param write: write variable to meta file         :param expression: value or expression
Evaluates the variable given by name.
Internal use, ignores the filter
Return a dict containing the ndarray corresponding to the evaluated data          :param column_names: list of column names, to export, when None DataFrame.get_column_names(strings=strings, virtual=virtual) is used         :param selection: {selection}         :param strings: argument passed to DataFrame.get_column_names when column_names is None         :param virtual: argument passed to DataFrame.get_column_names when column_names is None         :return: dict
Return a copy of the DataFrame, if selection is None, it does not copy the data, it just has a reference          :param column_names: list of column names, to copy, when None DataFrame.get_column_names(strings=strings, virtual=virtual) is used         :param selection: {selection}         :param strings: argument passed to DataFrame.get_column_names when column_names is None         :param virtual: argument passed to DataFrame.get_column_names when column_names is None         :param selections: copy selections to a new DataFrame         :return: dict
Return a pandas DataFrame containing the ndarray corresponding to the evaluated data           If index is given, that column is used for the index of the dataframe.           Example           >>> df_pandas = df.to_pandas_df(["x", "y", "z"])          >>> df_copy = vaex.from_pandas(df_pandas)          :param column_names: list of column names, to export, when None DataFrame.get_column_names(strings=strings, virtual=virtual) is used         :param selection: {selection}         :param strings: argument passed to DataFrame.get_column_names when column_names is None         :param virtual: argument passed to DataFrame.get_column_names when column_names is None         :param index_column: if this column is given it is used for the index of the DataFrame         :return: pandas.DataFrame object
Returns an arrow Table object containing the arrays corresponding to the evaluated data          :param column_names: list of column names, to export, when None DataFrame.get_column_names(strings=strings, virtual=virtual) is used         :param selection: {selection}         :param strings: argument passed to DataFrame.get_column_names when column_names is None         :param virtual: argument passed to DataFrame.get_column_names when column_names is None         :return: pyarrow.Table object
Returns a astropy table object containing the ndarrays corresponding to the evaluated data          :param column_names: list of column names, to export, when None DataFrame.get_column_names(strings=strings, virtual=virtual) is used         :param selection: {selection}         :param strings: argument passed to DataFrame.get_column_names when column_names is None         :param virtual: argument passed to DataFrame.get_column_names when column_names is None         :param index: if this column is given it is used for the index of the DataFrame         :return: astropy.table.Table object
Add an in memory array as a column.
Renames a column, not this is only the in memory name, this will not be reflected on disk
Convert cartesian to polar coordinates          :param x: expression for x         :param y: expression for y         :param radius_out: name for the virtual column for the radius         :param azimuth_out: name for the virtual column for the azimuth angle         :param propagate_uncertainties: {propagate_uncertainties}         :param radians: if True, azimuth is in radians, defaults to degrees         :return:
Concert velocities from a cartesian to a spherical coordinate system          TODO: errors          :param x: name of x column (input)         :param y:         y         :param z:         z         :param vx:       vx         :param vy:       vy         :param vz:       vz         :param vr: name of the column for the radial velocity in the r direction (output)         :param vlong: name of the column for the velocity component in the longitude direction  (output)         :param vlat: name of the column for the velocity component in the latitude direction, positive points to the north pole (output)         :param distance: Expression for distance, if not given defaults to sqrt(x**2+y**2+z**2), but if this column already exists, passing this expression may lead to a better performance         :return:
Convert cartesian to polar velocities.          :param x:         :param y:         :param vx:         :param radius_polar: Optional expression for the radius, may lead to a better performance when given.         :param vy:         :param vr_out:         :param vazimuth_out:         :param propagate_uncertainties: {propagate_uncertainties}         :return:
Convert cylindrical polar velocities to Cartesian.          :param x:         :param y:         :param azimuth: Optional expression for the azimuth in degrees , may lead to a better performance when given.         :param vr:         :param vazimuth:         :param vx_out:         :param vy_out:         :param propagate_uncertainties: {propagate_uncertainties}
Rotation in 2d.          :param str x: Name/expression of x column         :param str y: idem for y         :param str xnew: name of transformed x column         :param str ynew:         :param float angle_degrees: rotation in degrees, anti clockwise         :return:
Convert spherical to cartesian coordinates.            :param alpha:         :param delta: polar angle, ranging from the -90 (south pole) to 90 (north pole)         :param distance: radial distance, determines the units of x, y and z         :param xname:         :param yname:         :param zname:         :param propagate_uncertainties: {propagate_uncertainties}         :param center:         :param center_name:         :param radians:         :return:
Convert cartesian to spherical coordinates.            :param x:         :param y:         :param z:         :param alpha:         :param delta: name for polar angle, ranges from -90 to 90 (or -pi to pi when radians is True).         :param distance:         :param radians:         :param center:         :param center_name:         :return:
Add a virtual column to the DataFrame.          Example:          >>> df.add_virtual_column("r", "sqrt(x**2 + y**2 + z**2)")         >>> df.select("r < 10")          :param: str name: name of virtual column         :param: expression: expression for the column         :param str unique: if name is already used, make it unique by adding a postfix, e.g. _1, or _2
Deletes a virtual column from a DataFrame.
Add a variable to to a DataFrame.          A variable may refer to other variables, and virtual columns and expression may refer to variables.          Example          >>> df.add_variable('center', 0)         >>> df.add_virtual_column('x_prime', 'x-center')         >>> df.select('x_prime < 0')          :param: str name: name of virtual varible         :param: expression: expression for the variable
Deletes a variable from a DataFrame.
Return a shallow copy a DataFrame with the last n rows.
Display the first and last n elements of a DataFrame.
Give a description of the DataFrame.          >>> import vaex         >>> df = vaex.example()[['x', 'y', 'z']]         >>> df.describe()                          x          y          z         dtype      float64    float64    float64         count       330000     330000     330000         missing          0          0          0         mean    -0.0671315 -0.0535899  0.0169582         std        7.31746    7.78605    5.05521         min       -128.294   -71.5524   -44.3342         max        271.366    146.466    50.7185         >>> df.describe(selection=df.x > 0)                            x         y          z         dtype        float64   float64    float64         count         164060    164060     164060         missing       165940    165940     165940         mean         5.13572 -0.486786 -0.0868073         std          5.18701   7.61621    5.02831         min      1.51635e-05  -71.5524   -44.3342         max          271.366   78.0724    40.2191          :param bool strings: Describe string columns or not         :param bool virtual: Describe virtual columns or not         :param selection: Optional selection to use.         :return: Pandas dataframe
Display the DataFrame from row i1 till i2          For format, see https://pypi.org/project/tabulate/          :param int i1: Start row         :param int i2: End row.         :param str format: Format to use, e.g. 'html', 'plain', 'latex'
Set the current row, and emit the signal signal_pick.
Return a list of column names          Example:          >>> import vaex         >>> df = vaex.from_scalars(x=1, x2=2, y=3, s='string')         >>> df['r'] = (df.x**2 + df.y**2)**2         >>> df.get_column_names()         ['x', 'x2', 'y', 's', 'r']         >>> df.get_column_names(virtual=False)         ['x', 'x2', 'y', 's']         >>> df.get_column_names(regex='x.*')         ['x', 'x2']          :param virtual: If False, skip virtual columns         :param hidden: If False, skip hidden columns         :param strings: If False, skip string columns         :param regex: Only return column names matching the (optional) regular expression         :rtype: list of str          Example:         >>> import vaex         >>> df = vaex.from_scalars(x=1, x2=2, y=3, s='string')         >>> df['r'] = (df.x**2 + df.y**2)**2         >>> df.get_column_names()         ['x', 'x2', 'y', 's', 'r']         >>> df.get_column_names(virtual=False)         ['x', 'x2', 'y', 's']         >>> df.get_column_names(regex='x.*')         ['x', 'x2']
Return a DataFrame, where all columns are 'trimmed' by the active range.          For the returned DataFrame, df.get_active_range() returns (0, df.length_original()).          {note_copy}          :param inplace: {inplace}         :rtype: DataFrame
Returns a DataFrame containing only rows indexed by indices          {note_copy}          Example:          >>> import vaex, numpy as np         >>> df = vaex.from_arrays(s=np.array(['a', 'b', 'c', 'd']), x=np.arange(1,5))         >>> df.take([0,2])          #  s      x          0  a      1          1  c      3          :param indices: sequence (list or numpy array) with row numbers         :return: DataFrame which is a shallow copy of the original data.         :rtype: DataFrame
Return a DataFrame containing only the filtered rows.          {note_copy}          The resulting DataFrame may be more efficient to work with when the original DataFrame is         heavily filtered (contains just a small number of rows).          If no filtering is applied, it returns a trimmed view.         For the returned df, len(df) == df.length_original() == df.length_unfiltered()          :rtype: DataFrame
Returns a DataFrame with a random set of rows          {note_copy}          Provide either n or frac.          Example:          >>> import vaex, numpy as np         >>> df = vaex.from_arrays(s=np.array(['a', 'b', 'c', 'd']), x=np.arange(1,5))         >>> df           #  s      x           0  a      1           1  b      2           2  c      3           3  d      4         >>> df.sample(n=2, random_state=42) # 2 random rows, fixed seed           #  s      x           0  b      2           1  d      4         >>> df.sample(frac=1, random_state=42) # 'shuffling'           #  s      x           0  c      3           1  a      1           2  d      4           3  b      2         >>> df.sample(frac=1, replace=True, random_state=42) # useful for bootstrap (may contain repeated samples)           #  s      x           0  d      4           1  a      1           2  a      1           3  d      4          :param int n: number of samples to take (default 1 if frac is None)         :param float frac: fractional number of takes to take         :param bool replace: If true, a row may be drawn multiple times         :param str or expression weights: (unnormalized) probability that a row can be drawn         :param int or RandomState: seed or RandomState for reproducability, when None a random seed it chosen         :return: {return_shallow_copy}         :rtype: DataFrame
Returns a list containing random portions of the DataFrame.          {note_copy}          Example:          >>> import vaex, import numpy as np         >>> np.random.seed(111)         >>> df = vaex.from_arrays(x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])         >>> for dfs in df.split_random(frac=0.3, random_state=42):         ...     print(dfs.x.values)         ...         [8 1 5]         [0 7 2 9 4 3 6]         >>> for split in df.split_random(frac=[0.2, 0.3, 0.5], random_state=42):         ...     print(dfs.x.values)         [8 1]         [5 0 7]         [2 9 4 3 6]          :param int/list frac: If int will split the DataFrame in two portions, the first of which will have size as specified by this parameter. If list, the generator will generate as many portions as elements in the list, where each element defines the relative fraction of that portion.         :param int random_state: (default, None) Random number seed for reproducibility.         :return: A list of DataFrames.         :rtype: list
Returns a list containing ordered subsets of the DataFrame.          {note_copy}          Example:          >>> import vaex         >>> df = vaex.from_arrays(x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])         >>> for dfs in df.split(frac=0.3):         ...     print(dfs.x.values)         ...         [0 1 3]         [3 4 5 6 7 8 9]         >>> for split in df.split(frac=[0.2, 0.3, 0.5]):         ...     print(dfs.x.values)         [0 1]         [2 3 4]         [5 6 7 8 9]          :param int/list frac: If int will split the DataFrame in two portions, the first of which will have size as specified by this parameter. If list, the generator will generate as many portions as elements in the list, where each element defines the relative fraction of that portion.         :return: A list of DataFrames.         :rtype: list
Return a sorted DataFrame, sorted by the expression 'by'          {note_copy}          {note_filter}          Example:          >>> import vaex, numpy as np         >>> df = vaex.from_arrays(s=np.array(['a', 'b', 'c', 'd']), x=np.arange(1,5))         >>> df['y'] = (df.x-1.8)**2         >>> df           #  s      x     y           0  a      1  0.64           1  b      2  0.04           2  c      3  1.44           3  d      4  4.84         >>> df.sort('y', ascending=False)  # Note: passing '(x-1.8)**2' gives the same result           #  s      x     y           0  d      4  4.84           1  c      3  1.44           2  a      1  0.64           3  b      2  0.04          :param str or expression by: expression to sort by         :param bool ascending: ascending (default, True) or descending (False)         :param str kind: kind of algorithm to use (passed to numpy.argsort)
Returns a new DataFrame where the virtual column is turned into an in memory numpy array.          Example:          >>> x = np.arange(1,4)         >>> y = np.arange(2,5)         >>> df = vaex.from_arrays(x=x, y=y)         >>> df['r'] = (df.x**2 + df.y**2)**0.5 # 'r' is a virtual column (computed on the fly)         >>> df = df.materialize('r')  # now 'r' is a 'real' column (i.e. a numpy array)          :param inplace: {inplace}
Undo selection, for the name.
Redo selection, for the name.
Can selection name be redone?
Perform a selection, defined by the boolean expression, and combined with the previous selection using the given mode.          Selections are recorded in a history tree, per name, undo/redo can be done for them separately.          :param str boolean_expression: Any valid column expression, with comparison operators         :param str mode: Possible boolean operator: replace/and/or/xor/subtract         :param str name: history tree or selection 'slot' to use         :param executor:         :return:
Create a selection that selects rows having non missing values for all columns in column_names.          The name reflect Panda's, no rows are really dropped, but a mask is kept to keep track of the selection          :param drop_nan: drop rows when there is a NaN in any of the columns (will only affect float values)         :param drop_masked: drop rows when there is a masked value in any of the columns         :param column_names: The columns to consider, default: all (real, non-virtual) columns         :param str mode: Possible boolean operator: replace/and/or/xor/subtract         :param str name: history tree or selection 'slot' to use         :return:
Create a shallow copy of a DataFrame, with filtering set using select_non_missing.          :param drop_nan: drop rows when there is a NaN in any of the columns (will only affect float values)         :param drop_masked: drop rows when there is a masked value in any of the columns         :param column_names: The columns to consider, default: all (real, non-virtual) columns         :rtype: DataFrame
Select a 2d rectangular box in the space given by x and y, bounds by limits.          Example:          >>> df.select_box('x', 'y', [(0, 10), (0, 1)])          :param x: expression for the x space         :param y: expression fo the y space         :param limits: sequence of shape [(x1, x2), (y1, y2)]         :param mode:
Select a n-dimensional rectangular box bounded by limits.          The following examples are equivalent:          >>> df.select_box(['x', 'y'], [(0, 10), (0, 1)])         >>> df.select_rectangle('x', 'y', [(0, 10), (0, 1)])          :param spaces: list of expressions         :param limits: sequence of shape [(x1, x2), (y1, y2)]         :param mode:         :param name:         :return:
Select a circular region centred on xc, yc, with a radius of r.          Example:          >>> df.select_circle('x','y',2,3,1)          :param x: expression for the x space         :param y: expression for the y space         :param xc: location of the centre of the circle in x         :param yc: location of the centre of the circle in y         :param r: the radius of the circle         :param name: name of the selection         :param mode:         :return:
Select an elliptical region centred on xc, yc, with a certain width, height         and angle.          Example:          >>> df.select_ellipse('x','y', 2, -1, 5,1, 30, name='my_ellipse')          :param x: expression for the x space         :param y: expression for the y space         :param xc: location of the centre of the ellipse in x         :param yc: location of the centre of the ellipse in y         :param width: the width of the ellipse (diameter)         :param height: the width of the ellipse (diameter)         :param angle: (degrees) orientation of the ellipse, counter-clockwise                       measured from the y axis         :param name: name of the selection         :param mode:         :return:
For performance reasons, a lasso selection is handled differently.          :param str expression_x: Name/expression for the x coordinate         :param str expression_y: Name/expression for the y coordinate         :param xsequence: list of x numbers defining the lasso, together with y         :param ysequence:         :param str mode: Possible boolean operator: replace/and/or/xor/subtract         :param str name:         :param executor:         :return:
Invert the selection, i.e. what is selected will not be, and vice versa          :param str name:         :param executor:         :return:
Sets the selection object          :param selection: Selection object         :param name: selection 'slot'         :param executor:         :return:
select_lasso and select almost share the same code
Finds a non-colliding name by optional postfixing
Returns a list of string which are the virtual columns that are not used in any other virtual column.
Return a graphviz.Digraph object with a graph of all virtual columns
Mark column as categorical, with given labels, assuming zero indexing
Encode column as ordinal values and mark it as categorical.          The existing column is renamed to a hidden column and replaced by a numerical columns         with values between [0, len(values)-1].
Gives direct access to the data as numpy arrays.          Convenient when working with IPython in combination with small DataFrames, since this gives tab-completion.         Only real columns (i.e. no virtual) columns can be accessed, for getting the data from virtual columns, use         DataFrame.evalulate(...).          Columns can be accesed by there names, which are attributes. The attribues are of type numpy.ndarray.          Example:          >>> df = vaex.example()         >>> r = np.sqrt(df.data.x**2 + df.data.y**2)
Get the length of the DataFrames, for the selection of the whole DataFrame.          If selection is False, it returns len(df).          TODO: Implement this in DataFrameRemote, and move the method up in :func:`DataFrame.length`          :param selection: When True, will return the number of selected rows         :return:
Join the columns of the other DataFrame to this one, assuming the ordering is the same
Concatenates two DataFrames, adding the rows of one the other DataFrame to the current, returned in a new DataFrame.          No copy of the data is made.          :param other: The other DataFrame that is concatenated with this DataFrame         :return: New DataFrame with the rows concatenated         :rtype: DataFrameConcatenated
Exports the DataFrame to a vaex hdf5 file          :param DataFrameLocal df: DataFrame to export         :param str path: path for file         :param lis[str] column_names: list of column names to export or None for all columns         :param str byteorder: = for native, < for little endian and > for big endian         :param bool shuffle: export rows in random order         :param bool selection: export selection or not         :param progress: progress callback that gets a progress fraction as argument and should return True to continue,                 or a default progress bar when progress=True         :param: bool virtual: When True, export virtual columns         :param str sort: expression used for sorting the output         :param bool ascending: sort ascending (True) or descending         :return:
Add a column to the DataFrame          :param str name: name of column         :param data: numpy array with the data
Adds method f to the DataFrame class
Decorator to register a new function with vaex.      Example:      >>> import vaex     >>> df = vaex.example()     >>> @vaex.register_function()     >>> def invert(x):     >>>     return 1/x     >>> df.x.invert()       >>> import numpy as np     >>> df = vaex.from_arrays(departure=np.arange('2015-01-01', '2015-12-05', dtype='datetime64'))     >>> @vaex.register_function(as_property=True, scope='dt')     >>> def dt_relative_day(x):     >>>     return vaex.functions.dt_dayofyear(x)/365.     >>> df.departure.dt.relative_day
Returns an array where missing values are replaced by value.      If the dtype is object, nan values and 'nan' string values     are replaced by value when fill_nan==True.
Obtain the day of the week with Monday=0 and Sunday=6      :returns: an expression containing the day of week.      Example:      >>> import vaex     >>> import numpy as np     >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)     >>> df = vaex.from_arrays(date=date)     >>> df       #  date       0  2009-10-12 03:31:00       1  2016-02-11 10:17:34       2  2015-11-12 11:34:22      >>> df.date.dt.dayofweek     Expression = dt_dayofweek(date)     Length: 3 dtype: int64 (expression)     -----------------------------------     0  0     1  3     2  3
The ordinal day of the year.      :returns: an expression containing the ordinal day of the year.      Example:      >>> import vaex     >>> import numpy as np     >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)     >>> df = vaex.from_arrays(date=date)     >>> df       #  date       0  2009-10-12 03:31:00       1  2016-02-11 10:17:34       2  2015-11-12 11:34:22      >>> df.date.dt.dayofyear     Expression = dt_dayofyear(date)     Length: 3 dtype: int64 (expression)     -----------------------------------     0  285     1   42     2  316
Check whether a year is a leap year.      :returns: an expression which evaluates to True if a year is a leap year, and to False otherwise.      Example:      >>> import vaex     >>> import numpy as np     >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)     >>> df = vaex.from_arrays(date=date)     >>> df       #  date       0  2009-10-12 03:31:00       1  2016-02-11 10:17:34       2  2015-11-12 11:34:22      >>> df.date.dt.is_leap_year     Expression = dt_is_leap_year(date)     Length: 3 dtype: bool (expression)     ----------------------------------     0  False     1   True     2  False
Extracts the year out of a datetime sample.      :returns: an expression containing the year extracted from a datetime column.      Example:      >>> import vaex     >>> import numpy as np     >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)     >>> df = vaex.from_arrays(date=date)     >>> df       #  date       0  2009-10-12 03:31:00       1  2016-02-11 10:17:34       2  2015-11-12 11:34:22      >>> df.date.dt.year     Expression = dt_year(date)     Length: 3 dtype: int64 (expression)     -----------------------------------     0  2009     1  2016     2  2015
Extracts the month out of a datetime sample.      :returns: an expression containing the month extracted from a datetime column.      Example:      >>> import vaex     >>> import numpy as np     >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)     >>> df = vaex.from_arrays(date=date)     >>> df       #  date       0  2009-10-12 03:31:00       1  2016-02-11 10:17:34       2  2015-11-12 11:34:22      >>> df.date.dt.month     Expression = dt_month(date)     Length: 3 dtype: int64 (expression)     -----------------------------------     0  10     1   2     2  11
Returns the month names of a datetime sample in English.      :returns: an expression containing the month names extracted from a datetime column.      Example:      >>> import vaex     >>> import numpy as np     >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)     >>> df = vaex.from_arrays(date=date)     >>> df       #  date       0  2009-10-12 03:31:00       1  2016-02-11 10:17:34       2  2015-11-12 11:34:22      >>> df.date.dt.month_name     Expression = dt_month_name(date)     Length: 3 dtype: str (expression)     ---------------------------------     0   October     1  February     2  November
Extracts the day from a datetime sample.      :returns: an expression containing the day extracted from a datetime column.      Example:      >>> import vaex     >>> import numpy as np     >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)     >>> df = vaex.from_arrays(date=date)     >>> df       #  date       0  2009-10-12 03:31:00       1  2016-02-11 10:17:34       2  2015-11-12 11:34:22      >>> df.date.dt.day     Expression = dt_day(date)     Length: 3 dtype: int64 (expression)     -----------------------------------     0  12     1  11     2  12
Returns the day names of a datetime sample in English.      :returns: an expression containing the day names extracted from a datetime column.      Example:      >>> import vaex     >>> import numpy as np     >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)     >>> df = vaex.from_arrays(date=date)     >>> df       #  date       0  2009-10-12 03:31:00       1  2016-02-11 10:17:34       2  2015-11-12 11:34:22      >>> df.date.dt.day_name     Expression = dt_day_name(date)     Length: 3 dtype: str (expression)     ---------------------------------     0    Monday     1  Thursday     2  Thursday
Returns the week ordinal of the year.      :returns: an expression containing the week ordinal of the year, extracted from a datetime column.      Example:      >>> import vaex     >>> import numpy as np     >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)     >>> df = vaex.from_arrays(date=date)     >>> df       #  date       0  2009-10-12 03:31:00       1  2016-02-11 10:17:34       2  2015-11-12 11:34:22      >>> df.date.dt.weekofyear     Expression = dt_weekofyear(date)     Length: 3 dtype: int64 (expression)     -----------------------------------     0  42     1   6     2  46
Extracts the hour out of a datetime samples.      :returns: an expression containing the hour extracted from a datetime column.      Example:      >>> import vaex     >>> import numpy as np     >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)     >>> df = vaex.from_arrays(date=date)     >>> df       #  date       0  2009-10-12 03:31:00       1  2016-02-11 10:17:34       2  2015-11-12 11:34:22      >>> df.date.dt.hour     Expression = dt_hour(date)     Length: 3 dtype: int64 (expression)     -----------------------------------     0   3     1  10     2  11
Extracts the minute out of a datetime samples.      :returns: an expression containing the minute extracted from a datetime column.      Example:      >>> import vaex     >>> import numpy as np     >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)     >>> df = vaex.from_arrays(date=date)     >>> df       #  date       0  2009-10-12 03:31:00       1  2016-02-11 10:17:34       2  2015-11-12 11:34:22      >>> df.date.dt.minute     Expression = dt_minute(date)     Length: 3 dtype: int64 (expression)     -----------------------------------     0  31     1  17     2  34
Extracts the second out of a datetime samples.      :returns: an expression containing the second extracted from a datetime column.      Example:      >>> import vaex     >>> import numpy as np     >>> date = np.array(['2009-10-12T03:31:00', '2016-02-11T10:17:34', '2015-11-12T11:34:22'], dtype=np.datetime64)     >>> df = vaex.from_arrays(date=date)     >>> df       #  date       0  2009-10-12 03:31:00       1  2016-02-11 10:17:34       2  2015-11-12 11:34:22      >>> df.date.dt.second     Expression = dt_second(date)     Length: 3 dtype: int64 (expression)     -----------------------------------     0   0     1  34     2  22
Capitalize the first letter of a string sample.      :returns: an expression containing the capitalized strings.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.capitalize()     Expression = str_capitalize(text)     Length: 5 dtype: str (expression)     ---------------------------------     0    Something     1  Very pretty     2    Is coming     3          Our     4         Way.
Concatenate two string columns on a row-by-row basis.      :param expression other: The expression of the other column to be concatenated.     :returns: an expression containing the concatenated columns.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.cat(df.text)     Expression = str_cat(text, text)     Length: 5 dtype: str (expression)     ---------------------------------     0      SomethingSomething     1  very prettyvery pretty     2      is comingis coming     3                  ourour     4                way.way.
Check if a string pattern or regex is contained within a sample of a string column.      :param str pattern: A string or regex pattern     :param bool regex: If True,     :returns: an expression which is evaluated to True if the pattern is found in a given sample, and it is False otherwise.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.contains('very')     Expression = str_contains(text, 'very')     Length: 5 dtype: bool (expression)     ----------------------------------     0  False     1   True     2  False     3  False     4  False
Count the occurences of a pattern in sample of a string column.      :param str pat: A string or regex pattern     :param bool regex: If True,     :returns: an expression containing the number of times a pattern is found in each sample.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.count(pat="et", regex=False)     Expression = str_count(text, pat='et', regex=False)     Length: 5 dtype: int64 (expression)     -----------------------------------     0  1     1  1     2  0     3  0     4  0
Returns the lowest indices in each string in a column, where the provided substring is fully contained between within a     sample. If the substring is not found, -1 is returned.      :param str sub: A substring to be found in the samples     :param int start:     :param int end:     :returns: an expression containing the lowest indices specifying the start of the substring.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.find(sub="et")     Expression = str_find(text, sub='et')     Length: 5 dtype: int64 (expression)     -----------------------------------     0   3     1   7     2  -1     3  -1     4  -1
Extract a character from each sample at the specified position from a string column.     Note that if the specified position is out of bound of the string sample, this method returns '', while pandas retunrs nan.      :param int i: The index location, at which to extract the character.     :returns: an expression containing the extracted characters.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.get(5)     Expression = str_get(text, 5)     Length: 5 dtype: str (expression)     ---------------------------------     0    h     1    p     2    m     3     4
Returns the lowest indices in each string in a column, where the provided substring is fully contained between within a     sample. If the substring is not found, -1 is returned. It is the same as `str.find`.      :param str sub: A substring to be found in the samples     :param int start:     :param int end:     :returns: an expression containing the lowest indices specifying the start of the substring.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.index(sub="et")     Expression = str_find(text, sub='et')     Length: 5 dtype: int64 (expression)     -----------------------------------     0   3     1   7     2  -1     3  -1     4  -1
Converts string samples to lower case.      :returns: an expression containing the converted strings.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.lower()     Expression = str_lower(text)     Length: 5 dtype: str (expression)     ---------------------------------     0    something     1  very pretty     2    is coming     3          our     4         way.
Remove leading characters from a string sample.      :param str to_strip: The string to be removed     :returns: an expression containing the modified string column.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.lstrip(to_strip='very ')     Expression = str_lstrip(text, to_strip='very ')     Length: 5 dtype: str (expression)     ---------------------------------     0  Something     1     pretty     2  is coming     3        our     4       way.
Pad strings in a given column.      :param int width: The total width of the string     :param str side: If 'left' than pad on the left, if 'right' than pad on the right side the string.     :param str fillchar: The character used for padding.     :returns: an expression containing the padded strings.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.pad(width=10, side='left', fillchar='!')     Expression = str_pad(text, width=10, side='left', fillchar='!')     Length: 5 dtype: str (expression)     ---------------------------------     0   !Something     1  very pretty     2   !is coming     3   !!!!!!!our     4   !!!!!!way.
Duplicate each string in a column.      :param int repeats: number of times each string sample is to be duplicated.     :returns: an expression containing the duplicated strings      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.repeat(3)     Expression = str_repeat(text, 3)     Length: 5 dtype: str (expression)     ---------------------------------     0        SomethingSomethingSomething     1  very prettyvery prettyvery pretty     2        is comingis comingis coming     3                          ourourour     4                       way.way.way.
Returns the highest indices in each string in a column, where the provided substring is fully contained between within a     sample. If the substring is not found, -1 is returned.      :param str sub: A substring to be found in the samples     :param int start:     :param int end:     :returns: an expression containing the highest indices specifying the start of the substring.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.rfind(sub="et")     Expression = str_rfind(text, sub='et')     Length: 5 dtype: int64 (expression)     -----------------------------------     0   3     1   7     2  -1     3  -1     4  -1
Returns the highest indices in each string in a column, where the provided substring is fully contained between within a     sample. If the substring is not found, -1 is returned. Same as `str.rfind`.      :param str sub: A substring to be found in the samples     :param int start:     :param int end:     :returns: an expression containing the highest indices specifying the start of the substring.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.rindex(sub="et")     Expression = str_rindex(text, sub='et')     Length: 5 dtype: int64 (expression)     -----------------------------------     0   3     1   7     2  -1     3  -1     4  -1
Fills the left side of string samples with a specified character such that the strings are left-hand justified.      :param int width: The minimal width of the strings.     :param str fillchar: The character used for filling.     :returns: an expression containing the filled strings.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.rjust(width=10, fillchar='!')     Expression = str_rjust(text, width=10, fillchar='!')     Length: 5 dtype: str (expression)     ---------------------------------     0   !Something     1  very pretty     2   !is coming     3   !!!!!!!our     4   !!!!!!way.
Remove trailing characters from a string sample.      :param str to_strip: The string to be removed     :returns: an expression containing the modified string column.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.rstrip(to_strip='ing')     Expression = str_rstrip(text, to_strip='ing')     Length: 5 dtype: str (expression)     ---------------------------------     0       Someth     1  very pretty     2       is com     3          our     4         way.
Slice substrings from each string element in a column.      :param int start: The start position for the slice operation.     :param int end: The stop position for the slice operation.     :returns: an expression containing the sliced substrings.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.slice(start=2, stop=5)     Expression = str_pandas_slice(text, start=2, stop=5)     Length: 5 dtype: str (expression)     ---------------------------------     0  met     1   ry     2   co     3    r     4   y.
Removes leading and trailing characters.      Strips whitespaces (including new lines), or a set of specified     characters from each string saple in a column, both from the left     right sides.      :param str to_strip: The characters to be removed. All combinations of the characters will be removed.                          If None, it removes whitespaces.     :param returns: an expression containing the modified string samples.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.strip(to_strip='very')     Expression = str_strip(text, to_strip='very')     Length: 5 dtype: str (expression)     ---------------------------------     0  Something     1      prett     2  is coming     3         ou     4       way.
Converts all string samples to titlecase.      :returns: an expression containing the converted strings.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.      >>> df.text.str.title()     Expression = str_title(text)     Length: 5 dtype: str (expression)     ---------------------------------     0    Something     1  Very Pretty     2    Is Coming     3          Our     4         Way.
Converts all strings in a column to uppercase.      :returns: an expression containing the converted strings.      Example:      >>> import vaex     >>> text = ['Something', 'very pretty', 'is coming', 'our', 'way.']     >>> df = vaex.from_arrays(text=text)     >>> df       #  text       0  Something       1  very pretty       2  is coming       3  our       4  way.       >>> df.text.str.upper()     Expression = str_upper(text)     Length: 5 dtype: str (expression)     ---------------------------------     0    SOMETHING     1  VERY PRETTY     2    IS COMING     3          OUR     4         WAY.
Attempts to return a numpy array converted to the most sensible dtype     Value errors will be caught and simply return the original array     Tries to make dtype int, then float, then no change
Convert into numpy recordarray
Writes properties to the file in Java properties format.      :param fh: a writable file-like object     :param props: a mapping (dict) or iterable of key/value pairs     :param comment: comment to write to the beginning of the file     :param timestamp: boolean indicating whether to write a timestamp comment
Writes a comment to the file in Java properties format.      Newlines in the comment text are automatically turned into a continuation     of the comment by adding a "#" to the beginning of each line.      :param fh: a writable file-like object     :param comment: comment string to write
Write a single property to the file in Java properties format.      :param fh: a writable file-like object     :param key: the key to write     :param value: the value to write
Incrementally read properties from a Java .properties file.      Yields tuples of key/value pairs.      If ``comments`` is `True`, comments will be included with ``jprops.COMMENT``     in place of the key.      :param fh: a readable file-like object     :param comments: should include comments (default: False)
Wrap a file to convert newlines regardless of whether the file was opened     with the "universal newlines" option or not.
Return the version information for all librosa dependencies.
Handle renamed arguments.      Parameters     ----------     old_name : str     old_value         The name and value of the old argument      new_name : str     new_value         The name and value of the new argument      version_deprecated : str         The version at which the old name became deprecated      version_removed : str         The version at which the old name will be removed      Returns     -------     value         - `new_value` if `old_value` of type `Deprecated`         - `old_value` otherwise      Warnings     --------     if `old_value` is not of type `Deprecated`
Set the FFT library used by librosa.      Parameters     ----------     lib : None or module         Must implement an interface compatible with `numpy.fft`.         If `None`, reverts to `numpy.fft`.      Examples     --------     Use `pyfftw`:      >>> import pyfftw     >>> librosa.set_fftlib(pyfftw.interfaces.numpy_fft)      Reset to default `numpy` implementation      >>> librosa.set_fftlib()
Beat tracking function      :parameters:       - input_file : str           Path to input audio file (wav, mp3, m4a, flac, etc.)        - output_file : str           Path to save beat event timestamps as a CSV file
Load audio, estimate tuning, apply pitch correction, and save.
Converts frame indices to audio sample indices.      Parameters     ----------     frames     : number or np.ndarray [shape=(n,)]         frame index or vector of frame indices      hop_length : int > 0 [scalar]         number of samples between successive frames      n_fft : None or int > 0 [scalar]         Optional: length of the FFT window.         If given, time conversion will include an offset of `n_fft / 2`         to counteract windowing effects when using a non-centered STFT.      Returns     -------     times : number or np.ndarray         time (in samples) of each given frame number:         `times[i] = frames[i] * hop_length`      See Also     --------     frames_to_time : convert frame indices to time values     samples_to_frames : convert sample indices to frame indices      Examples     --------     >>> y, sr = librosa.load(librosa.util.example_audio_file())     >>> tempo, beats = librosa.beat.beat_track(y, sr=sr)     >>> beat_samples = librosa.frames_to_samples(beats)
Converts sample indices into STFT frames.      Examples     --------     >>> # Get the frame numbers for every 256 samples     >>> librosa.samples_to_frames(np.arange(0, 22050, 256))     array([ 0,  0,  1,  1,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,             7,  7,  8,  8,  9,  9, 10, 10, 11, 11, 12, 12, 13, 13,            14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20,            21, 21, 22, 22, 23, 23, 24, 24, 25, 25, 26, 26, 27, 27,            28, 28, 29, 29, 30, 30, 31, 31, 32, 32, 33, 33, 34, 34,            35, 35, 36, 36, 37, 37, 38, 38, 39, 39, 40, 40, 41, 41,            42, 42, 43])      Parameters     ----------     samples : int or np.ndarray [shape=(n,)]         sample index or vector of sample indices      hop_length : int > 0 [scalar]         number of samples between successive frames      n_fft : None or int > 0 [scalar]         Optional: length of the FFT window.         If given, time conversion will include an offset of `- n_fft / 2`         to counteract windowing effects in STFT.          .. note:: This may result in negative frame indices.      Returns     -------     frames : int or np.ndarray [shape=(n,), dtype=int]         Frame numbers corresponding to the given times:         `frames[i] = floor( samples[i] / hop_length )`      See Also     --------     samples_to_time : convert sample indices to time values     frames_to_samples : convert frame indices to sample indices
Converts time stamps into STFT frames.      Parameters     ----------     times : np.ndarray [shape=(n,)]         time (in seconds) or vector of time values      sr : number > 0 [scalar]         audio sampling rate      hop_length : int > 0 [scalar]         number of samples between successive frames      n_fft : None or int > 0 [scalar]         Optional: length of the FFT window.         If given, time conversion will include an offset of `- n_fft / 2`         to counteract windowing effects in STFT.          .. note:: This may result in negative frame indices.      Returns     -------     frames : np.ndarray [shape=(n,), dtype=int]         Frame numbers corresponding to the given times:         `frames[i] = floor( times[i] * sr / hop_length )`      See Also     --------     frames_to_time : convert frame indices to time values     time_to_samples : convert time values to sample indices      Examples     --------     Get the frame numbers for every 100ms      >>> librosa.time_to_frames(np.arange(0, 1, 0.1),     ...                         sr=22050, hop_length=512)     array([ 0,  4,  8, 12, 17, 21, 25, 30, 34, 38])
Convert one or more MIDI numbers to note strings.      MIDI numbers will be rounded to the nearest integer.      Notes will be of the format 'C0', 'C#0', 'D0', ...      Examples     --------     >>> librosa.midi_to_note(0)     'C-1'     >>> librosa.midi_to_note(37)     'C#2'     >>> librosa.midi_to_note(-2)     'A#-2'     >>> librosa.midi_to_note(104.7)     'A7'     >>> librosa.midi_to_note(104.7, cents=True)     'A7-30'     >>> librosa.midi_to_note(list(range(12, 24)))     ['C0', 'C#0', 'D0', 'D#0', 'E0', 'F0', 'F#0', 'G0', 'G#0', 'A0', 'A#0', 'B0']      Parameters     ----------     midi : int or iterable of int         Midi numbers to convert.      octave: bool         If True, include the octave number      cents: bool         If true, cent markers will be appended for fractional notes.         Eg, `midi_to_note(69.3, cents=True)` == `A4+03`      Returns     -------     notes : str or iterable of str         Strings describing each midi note.      Raises     ------     ParameterError         if `cents` is True and `octave` is False      See Also     --------     midi_to_hz     note_to_midi     hz_to_note
Convert Hz to Mels      Examples     --------     >>> librosa.hz_to_mel(60)     0.9     >>> librosa.hz_to_mel([110, 220, 440])     array([ 1.65,  3.3 ,  6.6 ])      Parameters     ----------     frequencies   : number or np.ndarray [shape=(n,)] , float         scalar or array of frequencies     htk           : bool         use HTK formula instead of Slaney      Returns     -------     mels        : number or np.ndarray [shape=(n,)]         input frequencies in Mels      See Also     --------     mel_to_hz
Convert mel bin numbers to frequencies      Examples     --------     >>> librosa.mel_to_hz(3)     200.      >>> librosa.mel_to_hz([1,2,3,4,5])     array([  66.667,  133.333,  200.   ,  266.667,  333.333])      Parameters     ----------     mels          : np.ndarray [shape=(n,)], float         mel bins to convert     htk           : bool         use HTK formula instead of Slaney      Returns     -------     frequencies   : np.ndarray [shape=(n,)]         input mels in Hz      See Also     --------     hz_to_mel
Alternative implementation of `np.fft.fftfreq`      Parameters     ----------     sr : number > 0 [scalar]         Audio sampling rate      n_fft : int > 0 [scalar]         FFT window size       Returns     -------     freqs : np.ndarray [shape=(1 + n_fft/2,)]         Frequencies `(0, sr/n_fft, 2*sr/n_fft, ..., sr/2)`       Examples     --------     >>> librosa.fft_frequencies(sr=22050, n_fft=16)     array([     0.   ,   1378.125,   2756.25 ,   4134.375,              5512.5  ,   6890.625,   8268.75 ,   9646.875,  11025.   ])
Compute the center frequencies of Constant-Q bins.      Examples     --------     >>> # Get the CQT frequencies for 24 notes, starting at C2     >>> librosa.cqt_frequencies(24, fmin=librosa.note_to_hz('C2'))     array([  65.406,   69.296,   73.416,   77.782,   82.407,   87.307,              92.499,   97.999,  103.826,  110.   ,  116.541,  123.471,             130.813,  138.591,  146.832,  155.563,  164.814,  174.614,             184.997,  195.998,  207.652,  220.   ,  233.082,  246.942])      Parameters     ----------     n_bins  : int > 0 [scalar]         Number of constant-Q bins      fmin    : float > 0 [scalar]         Minimum frequency      bins_per_octave : int > 0 [scalar]         Number of bins per octave      tuning : float in `[-0.5, +0.5)`         Deviation from A440 tuning in fractional bins (cents)      Returns     -------     frequencies : np.ndarray [shape=(n_bins,)]         Center frequency for each CQT bin
Compute an array of acoustic frequencies tuned to the mel scale.      The mel scale is a quasi-logarithmic function of acoustic frequency     designed such that perceptually similar pitch intervals (e.g. octaves)     appear equal in width over the full hearing range.      Because the definition of the mel scale is conditioned by a finite number     of subjective psychoaoustical experiments, several implementations coexist     in the audio signal processing literature [1]_. By default, librosa replicates     the behavior of the well-established MATLAB Auditory Toolbox of Slaney [2]_.     According to this default implementation,  the conversion from Hertz to mel is     linear below 1 kHz and logarithmic above 1 kHz. Another available implementation     replicates the Hidden Markov Toolkit [3]_ (HTK) according to the following formula:      `mel = 2595.0 * np.log10(1.0 + f / 700.0).`      The choice of implementation is determined by the `htk` keyword argument: setting     `htk=False` leads to the Auditory toolbox implementation, whereas setting it `htk=True`     leads to the HTK implementation.      .. [1] Umesh, S., Cohen, L., & Nelson, D. Fitting the mel scale.         In Proc. International Conference on Acoustics, Speech, and Signal Processing         (ICASSP), vol. 1, pp. 217-220, 1998.      .. [2] Slaney, M. Auditory Toolbox: A MATLAB Toolbox for Auditory         Modeling Work. Technical Report, version 2, Interval Research Corporation, 1998.      .. [3] Young, S., Evermann, G., Gales, M., Hain, T., Kershaw, D., Liu, X.,         Moore, G., Odell, J., Ollason, D., Povey, D., Valtchev, V., & Woodland, P.         The HTK book, version 3.4. Cambridge University, March 2009.       See Also     --------     hz_to_mel     mel_to_hz     librosa.feature.melspectrogram     librosa.feature.mfcc       Parameters     ----------     n_mels    : int > 0 [scalar]         Number of mel bins.      fmin      : float >= 0 [scalar]         Minimum frequency (Hz).      fmax      : float >= 0 [scalar]         Maximum frequency (Hz).      htk       : bool         If True, use HTK formula to convert Hz to mel.         Otherwise (False), use Slaney's Auditory Toolbox.      Returns     -------     bin_frequencies : ndarray [shape=(n_mels,)]         Vector of n_mels frequencies in Hz which are uniformly spaced on the Mel         axis.      Examples     --------     >>> librosa.mel_frequencies(n_mels=40)     array([     0.   ,     85.317,    170.635,    255.952,               341.269,    426.586,    511.904,    597.221,               682.538,    767.855,    853.173,    938.49 ,              1024.856,   1119.114,   1222.042,   1334.436,              1457.167,   1591.187,   1737.532,   1897.337,              2071.84 ,   2262.393,   2470.47 ,   2697.686,              2945.799,   3216.731,   3512.582,   3835.643,              4188.417,   4573.636,   4994.285,   5453.621,              5955.205,   6502.92 ,   7101.009,   7754.107,              8467.272,   9246.028,  10096.408,  11025.   ])
Compute the A-weighting of a set of frequencies.      Parameters     ----------     frequencies : scalar or np.ndarray [shape=(n,)]         One or more frequencies (in Hz)      min_db : float [scalar] or None         Clip weights below this threshold.         If `None`, no clipping is performed.      Returns     -------     A_weighting : scalar or np.ndarray [shape=(n,)]         `A_weighting[i]` is the A-weighting of `frequencies[i]`      See Also     --------     perceptual_weighting       Examples     --------      Get the A-weighting for CQT frequencies      >>> import matplotlib.pyplot as plt     >>> freqs = librosa.cqt_frequencies(108, librosa.note_to_hz('C1'))     >>> aw = librosa.A_weighting(freqs)     >>> plt.plot(freqs, aw)     >>> plt.xlabel('Frequency (Hz)')     >>> plt.ylabel('Weighting (log10)')     >>> plt.title('A-Weighting of CQT frequencies')
Return an array of time values to match the time axis from a feature matrix.      Parameters     ----------     X : np.ndarray or scalar         - If ndarray, X is a feature matrix, e.g. STFT, chromagram, or mel spectrogram.         - If scalar, X represents the number of frames.      sr : number > 0 [scalar]         audio sampling rate      hop_length : int > 0 [scalar]         number of samples between successive frames      n_fft : None or int > 0 [scalar]         Optional: length of the FFT window.         If given, time conversion will include an offset of `n_fft / 2`         to counteract windowing effects when using a non-centered STFT.      axis : int [scalar]         The axis representing the time axis of X.         By default, the last axis (-1) is taken.      Returns     -------     times : np.ndarray [shape=(n,)]         ndarray of times (in seconds) corresponding to each frame of X.      See Also     --------     samples_like : Return an array of sample indices to match the time axis from a feature matrix.      Examples     --------     Provide a feature matrix input:      >>> y, sr = librosa.load(librosa.util.example_audio_file())     >>> X = librosa.stft(y)     >>> times = librosa.times_like(X)     >>> times     array([  0.00000000e+00,   2.32199546e-02,   4.64399093e-02, ...,              6.13935601e+01,   6.14167800e+01,   6.14400000e+01])      Provide a scalar input:      >>> n_frames = 2647     >>> times = librosa.times_like(n_frames)     >>> times     array([  0.00000000e+00,   2.32199546e-02,   4.64399093e-02, ...,              6.13935601e+01,   6.14167800e+01,   6.14400000e+01])
Return an array of sample indices to match the time axis from a feature matrix.      Parameters     ----------     X : np.ndarray or scalar         - If ndarray, X is a feature matrix, e.g. STFT, chromagram, or mel spectrogram.         - If scalar, X represents the number of frames.      hop_length : int > 0 [scalar]         number of samples between successive frames      n_fft : None or int > 0 [scalar]         Optional: length of the FFT window.         If given, time conversion will include an offset of `n_fft / 2`         to counteract windowing effects when using a non-centered STFT.      axis : int [scalar]         The axis representing the time axis of X.         By default, the last axis (-1) is taken.      Returns     -------     samples : np.ndarray [shape=(n,)]         ndarray of sample indices corresponding to each frame of X.      See Also     --------     times_like : Return an array of time values to match the time axis from a feature matrix.      Examples     --------     Provide a feature matrix input:      >>> y, sr = librosa.load(librosa.util.example_audio_file())     >>> X = librosa.stft(y)     >>> samples = librosa.samples_like(X)     >>> samples     array([      0,     512,    1024, ..., 1353728, 1354240, 1354752])      Provide a scalar input:      >>> n_frames = 2647     >>> samples = librosa.samples_like(n_frames)     >>> samples     array([      0,     512,    1024, ..., 1353728, 1354240, 1354752])
Compute the hybrid constant-Q transform of an audio signal.      Here, the hybrid CQT uses the pseudo CQT for higher frequencies where     the hop_length is longer than half the filter length and the full CQT     for lower frequencies.      Parameters     ----------     y : np.ndarray [shape=(n,)]         audio time series      sr : number > 0 [scalar]         sampling rate of `y`      hop_length : int > 0 [scalar]         number of samples between successive CQT columns.      fmin : float > 0 [scalar]         Minimum frequency. Defaults to C1 ~= 32.70 Hz      n_bins : int > 0 [scalar]         Number of frequency bins, starting at `fmin`      bins_per_octave : int > 0 [scalar]         Number of bins per octave      tuning : None or float in `[-0.5, 0.5)`         Tuning offset in fractions of a bin (cents).          If `None`, tuning will be automatically estimated from the signal.      filter_scale : float > 0         Filter filter_scale factor. Larger values use longer windows.      sparsity : float in [0, 1)         Sparsify the CQT basis by discarding up to `sparsity`         fraction of the energy in each basis.          Set `sparsity=0` to disable sparsification.      window : str, tuple, number, or function         Window specification for the basis filters.         See `filters.get_window` for details.      pad_mode : string         Padding mode for centered frame analysis.          See also: `librosa.core.stft` and `np.pad`.      res_type : string         Resampling mode.  See `librosa.core.cqt` for details.      Returns     -------     CQT : np.ndarray [shape=(n_bins, t), dtype=np.float]         Constant-Q energy for each frequency at each time.      Raises     ------     ParameterError         If `hop_length` is not an integer multiple of         `2**(n_bins / bins_per_octave)`          Or if `y` is too short to support the frequency range of the CQT.      See Also     --------     cqt     pseudo_cqt      Notes     -----     This function caches at level 20.
Compute the pseudo constant-Q transform of an audio signal.      This uses a single fft size that is the smallest power of 2 that is greater     than or equal to the max of:          1. The longest CQT filter         2. 2x the hop_length      Parameters     ----------     y : np.ndarray [shape=(n,)]         audio time series      sr : number > 0 [scalar]         sampling rate of `y`      hop_length : int > 0 [scalar]         number of samples between successive CQT columns.      fmin : float > 0 [scalar]         Minimum frequency. Defaults to C1 ~= 32.70 Hz      n_bins : int > 0 [scalar]         Number of frequency bins, starting at `fmin`      bins_per_octave : int > 0 [scalar]         Number of bins per octave      tuning : None or float in `[-0.5, 0.5)`         Tuning offset in fractions of a bin (cents).          If `None`, tuning will be automatically estimated from the signal.      filter_scale : float > 0         Filter filter_scale factor. Larger values use longer windows.      sparsity : float in [0, 1)         Sparsify the CQT basis by discarding up to `sparsity`         fraction of the energy in each basis.          Set `sparsity=0` to disable sparsification.      window : str, tuple, number, or function         Window specification for the basis filters.         See `filters.get_window` for details.      pad_mode : string         Padding mode for centered frame analysis.          See also: `librosa.core.stft` and `np.pad`.      Returns     -------     CQT : np.ndarray [shape=(n_bins, t), dtype=np.float]         Pseudo Constant-Q energy for each frequency at each time.      Raises     ------     ParameterError         If `hop_length` is not an integer multiple of         `2**(n_bins / bins_per_octave)`          Or if `y` is too short to support the frequency range of the CQT.      Notes     -----     This function caches at level 20.
Compute the inverse constant-Q transform.      Given a constant-Q transform representation `C` of an audio signal `y`,     this function produces an approximation `y_hat`.       Parameters     ----------     C : np.ndarray, [shape=(n_bins, n_frames)]         Constant-Q representation as produced by `core.cqt`      hop_length : int > 0 [scalar]         number of samples between successive frames      fmin : float > 0 [scalar]         Minimum frequency. Defaults to C1 ~= 32.70 Hz      tuning : float in `[-0.5, 0.5)` [scalar]         Tuning offset in fractions of a bin (cents).      filter_scale : float > 0 [scalar]         Filter scale factor. Small values (<1) use shorter windows         for improved time resolution.      norm : {inf, -inf, 0, float > 0}         Type of norm to use for basis function normalization.         See `librosa.util.normalize`.      sparsity : float in [0, 1)         Sparsify the CQT basis by discarding up to `sparsity`         fraction of the energy in each basis.          Set `sparsity=0` to disable sparsification.      window : str, tuple, number, or function         Window specification for the basis filters.         See `filters.get_window` for details.      scale : bool         If `True`, scale the CQT response by square-root the length         of each channel's filter. This is analogous to `norm='ortho'` in FFT.          If `False`, do not scale the CQT. This is analogous to `norm=None`         in FFT.      length : int > 0, optional         If provided, the output `y` is zero-padded or clipped to exactly         `length` samples.      amin : float or None [DEPRECATED]          .. note:: This parameter is deprecated in 0.7.0 and will be removed in 0.8.0.      res_type : string         Resampling mode.  By default, this uses `fft` mode for high-quality          reconstruction, but this may be slow depending on your signal duration.         See `librosa.resample` for supported modes.      Returns     -------     y : np.ndarray, [shape=(n_samples), dtype=np.float]         Audio time-series reconstructed from the CQT representation.      See Also     --------     cqt     core.resample      Notes     -----     This function caches at level 40.      Examples     --------     Using default parameters      >>> y, sr = librosa.load(librosa.util.example_audio_file(), duration=15)     >>> C = librosa.cqt(y=y, sr=sr)     >>> y_hat = librosa.icqt(C=C, sr=sr)      Or with a different hop length and frequency resolution:      >>> hop_length = 256     >>> bins_per_octave = 12 * 3     >>> C = librosa.cqt(y=y, sr=sr, hop_length=256, n_bins=7*bins_per_octave,     ...                 bins_per_octave=bins_per_octave)     >>> y_hat = librosa.icqt(C=C, sr=sr, hop_length=hop_length,     ...                 bins_per_octave=bins_per_octave)
Generate the frequency domain constant-Q filter basis.
Helper function to trim and stack a collection of CQT responses
Compute the filter response with a target STFT hop.
Compute the number of early downsampling operations
